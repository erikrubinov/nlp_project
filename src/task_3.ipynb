{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:09:41.042997Z",
     "start_time": "2024-06-24T11:09:41.038117Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/stoffregen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/stoffregen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the dataset"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:09:41.880873Z",
     "start_time": "2024-06-24T11:09:41.859315Z"
    }
   },
   "source": "data = json.loads(open(\"../data/processed/Oppositional_thinking_analysis_dataset.json\").read())",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define function to handle imbalance in the training set"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:09:42.759406Z",
     "start_time": "2024-06-24T11:09:42.756395Z"
    }
   },
   "source": [
    "def balance_data(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.pop('id')\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n",
    "\n",
    "    # Handle class imbalance in the training set\n",
    "    train_df_majority = train_df[train_df.category == 'CRITICAL']\n",
    "    train_df_minority = train_df[train_df.category == 'CONSPIRACY']\n",
    "\n",
    "    train_df_minority_upsampled = resample(train_df_minority, \n",
    "                                        replace=True,     \n",
    "                                        n_samples=len(train_df_majority),    \n",
    "                                        random_state=42)\n",
    "\n",
    "    train_df_balanced = pd.concat([train_df_majority, train_df_minority_upsampled])\n",
    "\n",
    "    return train_df_balanced, test_df\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:09:43.341532Z",
     "start_time": "2024-06-24T11:09:43.331835Z"
    }
   },
   "cell_type": "code",
   "source": "train_df_balanced, test_df = balance_data(data)",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define different complex pre-processing functions"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:09:44.221611Z",
     "start_time": "2024-06-24T11:09:44.215902Z"
    }
   },
   "source": [
    "def preprocess_basic(text, lem_tag = True, stem_tag = False):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    if lem_tag:\n",
    "    # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    if stem_tag:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_advanced(text:str, lem_tag = True, stem_tag = False) -> int:\n",
    "\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) # remove decimals  \n",
    "    text = re.sub(r'[\\:\\-\\']', '', text)  # Remove specific punctuation\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove special characters\n",
    "    text = re.sub(r'\\d+\\.\\d+', '', text)  # Matches one or more digits followed by a dot and one or more digits\n",
    "    text = re.sub(r'\\bcom\\b', '', text, flags=re.IGNORECASE)  # Matches \"com\" at word boundaries (whole word)\n",
    "\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    if lem_tag:\n",
    "    # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    if stem_tag:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define train and evaluate function"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:30:30.184251Z",
     "start_time": "2024-06-24T11:30:30.179331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(train_df, test_df, pipeline):\n",
    "    # print trian_df columns\n",
    "    print(f\"Columns in train_df: {train_df.columns}\")\n",
    "    # put all available features into X_train, drop columns category and id if they exist\n",
    "    drop_columns = ['category'] if 'category' in train_df.columns else []\n",
    "    drop_columns += ['id'] if 'id' in train_df.columns else []\n",
    "    X_train = train_df.drop(drop_columns, axis=1)\n",
    "    # If only one column remains, convert it to a Series\n",
    "    if len(X_train.columns) == 1:\n",
    "        X_train = X_train.iloc[:, 0]\n",
    "        \n",
    "    y_train = train_df['category']\n",
    "    \n",
    "    X_test = test_df['text']\n",
    "    y_test = test_df['category']\n",
    "    \n",
    "    # print shapes\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)    \n",
    "    # print f1 score\n",
    "    print(f\"f1 score {pipeline.named_steps['vectorizer'].__class__.__name__}: {report['weighted avg']['f1-score']}\")\n",
    "    \n",
    "    # return the report as a dataframe\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    return report_df"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define function to save the results into a csv file"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:30:30.509869Z",
     "start_time": "2024-06-24T11:30:30.507474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results_to_csv(report, base_path = '../reports/', file_name = 'classification_report.csv'):\n",
    "    # append the data\n",
    "    report.to_csv(base_path + file_name, mode='a', header=False)"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the basic preprocessor class "
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:30:30.897273Z",
     "start_time": "2024-06-24T11:30:30.894167Z"
    }
   },
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, custom_preprocess_function, use_lemmatization=True, use_stemmanization = True):\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.use_stemmanization = use_stemmanization\n",
    "        self.preprocess_function = custom_preprocess_function\n",
    "\n",
    "    # define fit function to make the transformer pipeline compatible with sklearn\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        res = [self.preprocess_function(text, self.use_lemmatization, self.use_stemmanization) for text in X]\n",
    "        return res\n"
   ],
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define function to create pipelines with:\n",
    "* custom pre-processing functions\n",
    "* custom classifier (e.g. naive bayes, FNN)\n",
    "* no lemmatization nor stemming\n",
    "* ngram range  \n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:30:31.127237Z",
     "start_time": "2024-06-24T11:30:31.122904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_pipelines(ngram_lower_bound, ngram_upper_bound, custom_preprocess, classifier):\n",
    "    vectorizers = [\n",
    "        (CountVectorizer, 'CountVectorizer'),\n",
    "        (TfidfVectorizer, 'TfidfVectorizer')\n",
    "    ]\n",
    "    \n",
    "    pipelines = {}\n",
    "    \n",
    "    for vectorizer, vec_name in vectorizers:\n",
    "        pipeline_name = (\n",
    "            f'{vec_name} with preprocessing function {custom_preprocess.__name__}, '\n",
    "            f'classifier {type(classifier).__name__} no Lemmatization and ngram: '\n",
    "            f'{ngram_lower_bound}, {ngram_upper_bound}'\n",
    "        )\n",
    "        \n",
    "        pipelines[pipeline_name] = Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_lemmatization=False, use_stemmanization=False)),\n",
    "            ('vectorizer', vectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound))),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "    \n",
    "    return pipelines\n"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train and evaluate naive bayes and FNN models with different preprocessing\n",
    "* without lemmatization nor stemming\n",
    "* for ngram range 1-3"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:24:11.999227Z",
     "start_time": "2024-06-24T11:24:11.999127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "report_file_name = 'classification_report_1.csv'\n",
    "# empty report\n",
    "report_df = pd.DataFrame()\n",
    "report_df.to_csv('../reports/' + report_file_name)\n",
    "\n",
    "\n",
    "for preprocess_function in [preprocess_advanced, preprocess_basic]:\n",
    "    for classifier in [MultinomialNB(), MLPClassifier(hidden_layer_sizes=(50,))]:\n",
    "        for ngram_lower_bound, ngram_upper_bound in product(range(1, 3), repeat=2):\n",
    "            if ngram_lower_bound <= ngram_upper_bound:\n",
    "                pipelines = create_pipelines(ngram_lower_bound, ngram_upper_bound, preprocess_function, classifier)\n",
    "                for name, pipeline in pipelines.items():\n",
    "                    print(f\"Evaluating {name} with ngram range: {ngram_lower_bound}, {ngram_upper_bound}...\")\n",
    "                    report_df = train_and_evaluate(train_df_balanced, test_df, pipeline)\n",
    "                    # enrich df with the name of the pipeline, ngram range and preprocess function\n",
    "                    report_df['pipeline'] = name\n",
    "                    report_df['classifier'] = pipeline.named_steps['classifier'].__class__.__name__\n",
    "                    report_df['vectorizer'] = pipeline.named_steps['vectorizer'].__class__.__name__\n",
    "                    report_df['ngram_lower_bound'] = ngram_lower_bound\n",
    "                    report_df['ngram_upper_bound'] = ngram_upper_bound\n",
    "                    report_df['preprocess_function'] = preprocess_function.__name__\n",
    "                    save_results_to_csv(report_df, file_name=report_file_name)\n",
    "                    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Enrich data json with uppercase percentage and comment length feature"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:24:16.666406Z",
     "start_time": "2024-06-24T11:24:16.547773Z"
    }
   },
   "source": [
    "## add feature uppercase percentage\n",
    "def calculate_uppercase_percentage(text):\n",
    "    uppercase_count = 0\n",
    "    total_letters = 0\n",
    "    \n",
    "    for char in text:\n",
    "        if char.isalpha():  # Check if the character is a letter\n",
    "            total_letters += 1\n",
    "            if char.isupper():  # Check if the letter is uppercase\n",
    "                uppercase_count += 1\n",
    "    \n",
    "    if total_letters == 0:\n",
    "        return 0\n",
    "    uppercase_percentage = (uppercase_count / total_letters) * 100\n",
    "    \n",
    "    return uppercase_percentage\n",
    "\n",
    "def classify_uppercase_percentage(percentage):\n",
    "    if percentage < 6:\n",
    "        return \"low\"\n",
    "    elif 6 <= percentage <= 12:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "    \n",
    "# add custom feature: comment length\n",
    "def classify_comment_lenght(length):\n",
    "    if length < 190:\n",
    "        return \"short\"\n",
    "    elif 190 <= length <= 560:\n",
    "        return \"average\"\n",
    "    else:\n",
    "        return \"long\"\n",
    "\n",
    "# Update the data with the new key-value pair\n",
    "updated_data = data.copy()\n",
    "for comment in data:\n",
    "    comment_length = classify_comment_lenght(len(comment[\"text\"]))\n",
    "    uppercase_percentage = calculate_uppercase_percentage(comment[\"text\"])\n",
    "    uppercase_amount = classify_uppercase_percentage(uppercase_percentage)\n",
    "    comment[\"comment_length\"] = comment_length\n",
    "    comment[\"uppercase_amount\"] = uppercase_amount\n",
    "\n",
    "\n",
    "# Save the updated data to a new file\n",
    "with open('../data/processed/Oppositional_thinking_analysis_dataset_with_features.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=4)\n"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:31:51.570912Z",
     "start_time": "2024-06-24T11:31:51.552347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = json.loads(open(\"../data/processed/Oppositional_thinking_analysis_dataset_with_features.json\").read())\n",
    "train_df_balanced, test_df = balance_data(data)\n"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Add a preprocessor to handle the new features\n",
    "* it uses a OneHotEncoder to encode the custom features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:31:52.356109Z",
     "start_time": "2024-06-24T11:31:52.350999Z"
    }
   },
   "source": [
    "class CombinedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_transformer, ngram_range = (1,1),  additional_features = []):\n",
    "        self.text_transformer = text_transformer\n",
    "        self.additional_features = additional_features\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the individual transformers\n",
    "        #self.text_transformer.fit(X['text'])\n",
    "        if self.additional_features:\n",
    "            self.encoder.fit(X[self.additional_features]) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        text_features = self.text_transformer.transform(X['text'])\n",
    "     \n",
    "        if self.additional_features:\n",
    "            additional_features = self.encoder.transform(X[self.additional_features]).toarray()\n",
    "            text_features = np.hstack((text_features.toarray(), additional_features))\n",
    "       \n",
    "        return text_features\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## We add some new pipelines \n",
    "* those use the combined features with either CountVectorizer or TfidfVectorizer\n",
    "* they use both lemmatization and stemming\n",
    "* they use a OneHotEncoder for the custom features"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:31:53.135363Z",
     "start_time": "2024-06-24T11:31:53.130420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pre-processing pipelines\n",
    "custom_preprocess = preprocess_advanced\n",
    "\n",
    "def create_advanced_pipelines(ngram_lower_bound, ngram_upper_bound, custom_preprocess, classifier, custom_feautures : list ):\n",
    "    pipelines = {\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization nor stem and ngram: {ngram_lower_bound}, {ngram_upper_bound} with custom features: {custom_feautures}': Pipeline([\n",
    "            ('preprocessor',  TextPreprocessor(custom_preprocess_function=custom_preprocess, use_stemmanization=False, use_lemmatization=False)),\n",
    "            ('features', CombinedFeatures(CountVectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound)), custom_feautures)),\n",
    "            ('classifier', classifier)\n",
    "        ]),\n",
    "        f'TfidfVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization nor stem and ngram: {ngram_lower_bound}, {ngram_upper_bound} with custom features: {custom_feautures}': Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_stemmanization=False, use_lemmatization=False)),\n",
    "            ('features', CombinedFeatures(TfidfVectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound)),custom_feautures)),\n",
    "            ('classifier', classifier)\n",
    "        ]),\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and Lemmatization and stem and ngram: {ngram_lower_bound}, {ngram_upper_bound} with custom features: {custom_feautures}': Pipeline([\n",
    "            ('preprocessor',  TextPreprocessor(custom_preprocess_function=custom_preprocess, use_stemmanization=True, use_lemmatization=True)),\n",
    "            ('features', CombinedFeatures(CountVectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound)), custom_feautures)),\n",
    "            ('classifier', classifier)\n",
    "        ]),\n",
    "        f'TfidfVectorizer with preprocessing function {custom_preprocess.__name__} and Lemmatization and stem and ngram: {ngram_lower_bound}, {ngram_upper_bound} with custom features: {custom_feautures}': Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_stemmanization=True, use_lemmatization=True)),\n",
    "            ('features', CombinedFeatures(TfidfVectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound)), custom_feautures)),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "    }   \n",
    "    return pipelines\n"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run and evaluate the advanced pipelines \n",
    "* with naive bayes as well as FNN\n",
    "* use different feature_list combinations"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T11:31:54.194555Z",
     "start_time": "2024-06-24T11:31:54.129776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "report_file_name = 'classification_report_2.csv'\n",
    "# empty report\n",
    "report_df = pd.DataFrame()\n",
    "report_df.to_csv('../reports/' + report_file_name)\n",
    "\n",
    "# Train and evaluate models with different pipelines\n",
    "for classifier in [MultinomialNB(), MLPClassifier(hidden_layer_sizes=(50,))]:\n",
    "    for feature_list in [['uppercase_amount', 'comment_length'], ['comment_length'], ['uppercase_amount']]:\n",
    "        for preprocess_function in [preprocess_advanced, preprocess_basic]:\n",
    "            for ngram_lower_bound in range(1,3):\n",
    "                for ngram_upper_bound in range(1,3): \n",
    "                    if ngram_lower_bound <= ngram_upper_bound:\n",
    "                        pipelines = create_advanced_pipelines(ngram_lower_bound, ngram_upper_bound, preprocess_function, classifier, feature_list)\n",
    "                        for name, pipeline in pipelines.items():\n",
    "                            print(f\"Evaluating {name}...\")\n",
    "                            train_and_evaluate(train_df_balanced, test_df, pipeline)\n",
    "                            # enrich df with the name of the pipeline, ngram range and preprocess function\n",
    "                            report_df['pipeline'] = name\n",
    "                            report_df['classifier'] = pipeline.named_steps['classifier'].__class__.__name__\n",
    "                            report_df['vectorizer'] = pipeline.named_steps['vectorizer'].text_transformer\n",
    "                            report_df['features'] = pipeline.named_steps['features'].__class__.__name__\n",
    "                            report_df['ngram_lower_bound'] = ngram_lower_bound\n",
    "                            report_df['ngram_upper_bound'] = ngram_upper_bound\n",
    "                            report_df['preprocess_function'] = preprocess_function.__name__\n",
    "                            report_df['preprocess_function_uses_lemmatization'] = pipeline.named_steps['preprocessor'].use_lemmatization\n",
    "                            report_df['preprocess_function_uses_stemmanization'] = pipeline.named_steps['preprocessor'].use_stemmanization\n",
    "                            report_df['feature_list'] = feature_list\n",
    "                            save_results_to_csv(report_df, file_name=report_file_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CountVectorizer with preprocessing function preprocess_advanced and no Lemmatization nor stem and ngram: 1, 1 with custom features: ['uppercase_amount', 'comment_length']...\n",
      "Columns in train_df: Index(['text', 'category', 'comment_length', 'uppercase_amount'], dtype='object')\n",
      "X_train shape: (4194, 3), y_train shape: (4194,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[107], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, pipeline \u001B[38;5;129;01min\u001B[39;00m pipelines\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m     \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df_balanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m# enrich df with the name of the pipeline, ngram range and preprocess function\u001B[39;00m\n\u001B[1;32m     18\u001B[0m     report_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpipeline\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m name\n",
      "Cell \u001B[0;32mIn[96], line 19\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(train_df, test_df, pipeline)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# print shapes\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX_train shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX_train\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, y_train shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my_train\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 19\u001B[0m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m     22\u001B[0m report \u001B[38;5;241m=\u001B[39m classification_report(y_test, y_pred, output_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)    \n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:472\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit the model.\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \n\u001B[1;32m    431\u001B[0m \u001B[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;124;03m    Pipeline with fitted steps.\u001B[39;00m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    471\u001B[0m routed_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_method_params(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, props\u001B[38;5;241m=\u001B[39mparams)\n\u001B[0;32m--> 472\u001B[0m Xt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:409\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, X, y, routed_params)\u001B[0m\n\u001B[1;32m    407\u001B[0m     cloned_transformer \u001B[38;5;241m=\u001B[39m clone(transformer)\n\u001B[1;32m    408\u001B[0m \u001B[38;5;66;03m# Fit or load from cache the current transformer\u001B[39;00m\n\u001B[0;32m--> 409\u001B[0m X, fitted_transformer \u001B[38;5;241m=\u001B[39m \u001B[43mfit_transform_one_cached\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcloned_transformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPipeline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_idx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;66;03m# Replace the transformer of the step with the fitted\u001B[39;00m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;66;03m# transformer. This is necessary when loading the transformer\u001B[39;00m\n\u001B[1;32m    420\u001B[0m \u001B[38;5;66;03m# from the cache.\u001B[39;00m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[step_idx] \u001B[38;5;241m=\u001B[39m (name, fitted_transformer)\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/joblib/memory.py:312\u001B[0m, in \u001B[0;36mNotMemorizedFunc.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:1329\u001B[0m, in \u001B[0;36m_fit_transform_one\u001B[0;34m(transformer, X, y, weight, columns, message_clsname, message, params)\u001B[0m\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[1;32m   1328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(transformer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1329\u001B[0m         res \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfit_transform\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1330\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1331\u001B[0m         res \u001B[38;5;241m=\u001B[39m transformer\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[1;32m   1332\u001B[0m             X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\n\u001B[1;32m   1333\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/base.py:1101\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m-> 1101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "Cell \u001B[0;32mIn[105], line 18\u001B[0m, in \u001B[0;36mCombinedFeatures.transform\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m---> 18\u001B[0m     text_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_transformer\u001B[38;5;241m.\u001B[39mtransform(\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madditional_features:\n\u001B[1;32m     21\u001B[0m         additional_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder\u001B[38;5;241m.\u001B[39mtransform(X[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madditional_features])\u001B[38;5;241m.\u001B[39mtoarray()\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
