{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:36:53.854341Z",
     "start_time": "2024-06-24T14:36:53.629909Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/stoffregen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/stoffregen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/stoffregen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the dataset"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:36:56.495708Z",
     "start_time": "2024-06-24T14:36:56.252981Z"
    }
   },
   "source": "data = json.loads(open(\"../data/processed/Oppositional_thinking_analysis_dataset.json\").read())",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define function to handle imbalance in the training set"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:36:57.071568Z",
     "start_time": "2024-06-24T14:36:57.068771Z"
    }
   },
   "source": [
    "def balance_data(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.pop('id')\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n",
    "\n",
    "    # Handle class imbalance in the training set\n",
    "    train_df_majority = train_df[train_df.category == 'CRITICAL']\n",
    "    train_df_minority = train_df[train_df.category == 'CONSPIRACY']\n",
    "\n",
    "    train_df_minority_upsampled = resample(train_df_minority, \n",
    "                                        replace=True,     \n",
    "                                        n_samples=len(train_df_majority),    \n",
    "                                        random_state=42)\n",
    "\n",
    "    train_df_balanced = pd.concat([train_df_majority, train_df_minority_upsampled])\n",
    "\n",
    "    return train_df_balanced, test_df\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:37:01.277411Z",
     "start_time": "2024-06-24T14:37:01.265329Z"
    }
   },
   "cell_type": "code",
   "source": "train_df_balanced, test_df = balance_data(data)",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define different complex pre-processing functions"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:37:02.063525Z",
     "start_time": "2024-06-24T14:37:02.056371Z"
    }
   },
   "source": [
    "def preprocess_basic(text, lem_tag = True, stem_tag = False):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    if lem_tag:\n",
    "    # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    if stem_tag:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_advanced(text:str, lem_tag = True, stem_tag = False) -> int:\n",
    "\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) # remove decimals  \n",
    "    text = re.sub(r'[\\:\\-\\']', '', text)  # Remove specific punctuation\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove special characters\n",
    "    text = re.sub(r'\\d+\\.\\d+', '', text)  # Matches one or more digits followed by a dot and one or more digits\n",
    "    text = re.sub(r'\\bcom\\b', '', text, flags=re.IGNORECASE)  # Matches \"com\" at word boundaries (whole word)\n",
    "\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    if lem_tag:\n",
    "    # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    if stem_tag:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define train and evaluate function"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:37:03.565822Z",
     "start_time": "2024-06-24T14:37:03.560613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(train_df, test_df, pipeline):\n",
    "    # put all available features into X_train, drop columns category and id if they exist\n",
    "    drop_columns = ['category'] if 'category' in train_df.columns else []\n",
    "    drop_columns += ['id'] if 'id' in train_df.columns else []\n",
    "    X_train = train_df.drop(drop_columns, axis=1)\n",
    "    # If only one column remains, convert it to a Series\n",
    "    if len(X_train.columns) == 1:\n",
    "        X_train = X_train.iloc[:, 0]\n",
    "        \n",
    "    \n",
    "    # if X_train is not a series, make X_test also a dataframe\n",
    "    if isinstance(X_train, pd.Series):\n",
    "        X_test = test_df['text']\n",
    "        y_train = train_df['category']\n",
    "        y_test = test_df['category']\n",
    "    else:\n",
    "        X_test = test_df[['text']]\n",
    "        y_train = train_df[['category']]\n",
    "        y_test = test_df[['category']]\n",
    "        \n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)    \n",
    "    # print f1 score\n",
    "    pipe_name = pipeline.named_steps['vectorizer'].__class__.__name__ if 'vectorizer' in pipeline.named_steps else pipeline.named_steps['features'].text_transformer\n",
    "    print(f\"f1 score {pipe_name}: {report['weighted avg']['f1-score']}\")\n",
    "    \n",
    "    # return the report as a dataframe\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    return report_df"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define function to save the results into a csv file"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:49:35.171756Z",
     "start_time": "2024-06-24T14:49:35.169043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_results_to_csv(report, base_path='../reports/', file_name='classification_report.csv'):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "    \n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    \n",
    "    # Save the report to CSV\n",
    "    if not os.path.isfile(file_path):\n",
    "        report.to_csv(file_path, header=True)\n",
    "    else:\n",
    "        report.to_csv(file_path, mode='a', header=False)"
   ],
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the basic preprocessor class "
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:38:10.133174Z",
     "start_time": "2024-06-24T14:38:10.130144Z"
    }
   },
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, custom_preprocess_function, use_lemmatization=True, use_stemmanization = True):\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.use_stemmanization = use_stemmanization\n",
    "        self.preprocess_function = custom_preprocess_function\n",
    "\n",
    "    # define fit function to make the transformer pipeline compatible with sklearn\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        res = [self.preprocess_function(text, self.use_lemmatization, self.use_stemmanization) for text in X]\n",
    "        return res\n"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define function to create pipelines with:\n",
    "* custom pre-processing functions\n",
    "* custom classifier (e.g. naive bayes, FNN)\n",
    "* no lemmatization nor stemming\n",
    "* ngram range  \n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:38:10.990977Z",
     "start_time": "2024-06-24T14:38:10.987975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_pipelines(ngram_lower_bound, ngram_upper_bound, custom_preprocess, classifier):\n",
    "    vectorizers = [\n",
    "        (CountVectorizer, 'CountVectorizer'),\n",
    "        (TfidfVectorizer, 'TfidfVectorizer')\n",
    "    ]\n",
    "    \n",
    "    pipelines = {}\n",
    "    \n",
    "    for vectorizer, vec_name in vectorizers:\n",
    "        pipeline_name = (\n",
    "            f'{vec_name} with preprocessing function {custom_preprocess.__name__}, '\n",
    "            f'classifier {type(classifier).__name__} no Lemmatization and ngram: '\n",
    "            f'{ngram_lower_bound}, {ngram_upper_bound}'\n",
    "        )\n",
    "        \n",
    "        pipelines[pipeline_name] = Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_lemmatization=False, use_stemmanization=False)),\n",
    "            ('vectorizer', vectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound))),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "    \n",
    "    return pipelines\n"
   ],
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train and evaluate naive bayes and FNN models with different preprocessing\n",
    "* without lemmatization nor stemming\n",
    "* for ngram range 1-3"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:49:55.285201Z",
     "start_time": "2024-06-24T14:49:37.511436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "report_file_name = 'classification_report_1.csv'\n",
    "# remove file\n",
    "if os.path.isfile(\"../reports/\" + report_file_name):\n",
    "    os.remove(\"../reports/\" + report_file_name)\n",
    "\n",
    "\n",
    "for preprocess_function in [preprocess_advanced, preprocess_basic]:\n",
    "    for classifier in [MultinomialNB(), MLPClassifier(hidden_layer_sizes=(50,))]:\n",
    "        for ngram_lower_bound, ngram_upper_bound in product(range(1, 3), repeat=2):\n",
    "            if ngram_lower_bound <= ngram_upper_bound:\n",
    "                pipelines = create_pipelines(ngram_lower_bound, ngram_upper_bound, preprocess_function, classifier)\n",
    "                for name, pipeline in pipelines.items():\n",
    "                    print(f\"Evaluating {name} with ngram range: {ngram_lower_bound}, {ngram_upper_bound}...\")\n",
    "                    report_df = train_and_evaluate(train_df_balanced, test_df, pipeline)\n",
    "                    # enrich df with the name of the pipeline, ngram range and preprocess function\n",
    "                    report_df['pipeline'] = name\n",
    "                    report_df['classifier'] = pipeline.named_steps['classifier'].__class__.__name__\n",
    "                    report_df['vectorizer'] = pipeline.named_steps['vectorizer'].__class__.__name__\n",
    "                    report_df['ngram_lower_bound'] = ngram_lower_bound\n",
    "                    report_df['ngram_upper_bound'] = ngram_upper_bound\n",
    "                    report_df['preprocess_function'] = preprocess_function.__name__\n",
    "                    print(os.path.isfile(\"../reports/classification_report_1.csv\"))\n",
    "                    save_results_to_csv(report_df, file_name=report_file_name)\n",
    "                    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CountVectorizer with preprocessing function preprocess_advanced, classifier MultinomialNB no Lemmatization and ngram: 1, 1 with ngram range: 1, 1...\n",
      "f1 score CountVectorizer: 0.8459123319635202\n",
      "False\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_advanced, classifier MultinomialNB no Lemmatization and ngram: 1, 1 with ngram range: 1, 1...\n",
      "f1 score TfidfVectorizer: 0.8520913770913771\n",
      "True\n",
      "Evaluating CountVectorizer with preprocessing function preprocess_advanced, classifier MultinomialNB no Lemmatization and ngram: 1, 2 with ngram range: 1, 2...\n",
      "f1 score CountVectorizer: 0.8446823604132413\n",
      "True\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_advanced, classifier MultinomialNB no Lemmatization and ngram: 1, 2 with ngram range: 1, 2...\n",
      "f1 score TfidfVectorizer: 0.8276449369477105\n",
      "True\n",
      "Evaluating CountVectorizer with preprocessing function preprocess_advanced, classifier MultinomialNB no Lemmatization and ngram: 2, 2 with ngram range: 2, 2...\n",
      "f1 score CountVectorizer: 0.8179240162822252\n",
      "True\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_advanced, classifier MultinomialNB no Lemmatization and ngram: 2, 2 with ngram range: 2, 2...\n",
      "f1 score TfidfVectorizer: 0.8261081995851075\n",
      "True\n",
      "Evaluating CountVectorizer with preprocessing function preprocess_advanced, classifier MLPClassifier no Lemmatization and ngram: 1, 1 with ngram range: 1, 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score CountVectorizer: 0.8568399835458659\n",
      "True\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_advanced, classifier MLPClassifier no Lemmatization and ngram: 1, 1 with ngram range: 1, 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score TfidfVectorizer: 0.836677815549041\n",
      "True\n",
      "Evaluating CountVectorizer with preprocessing function preprocess_advanced, classifier MLPClassifier no Lemmatization and ngram: 1, 2 with ngram range: 1, 2...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[138], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, pipeline \u001B[38;5;129;01min\u001B[39;00m pipelines\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with ngram range: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mngram_lower_bound\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mngram_upper_bound\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m     report_df \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df_balanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# enrich df with the name of the pipeline, ngram range and preprocess function\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     report_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpipeline\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m name\n",
      "Cell \u001B[0;32mIn[111], line 22\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(train_df, test_df, pipeline)\u001B[0m\n\u001B[1;32m     18\u001B[0m     y_train \u001B[38;5;241m=\u001B[39m train_df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[1;32m     19\u001B[0m     y_test \u001B[38;5;241m=\u001B[39m test_df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[0;32m---> 22\u001B[0m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m     25\u001B[0m report \u001B[38;5;241m=\u001B[39m classification_report(y_test, y_pred, output_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)    \n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:472\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit the model.\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \n\u001B[1;32m    431\u001B[0m \u001B[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;124;03m    Pipeline with fitted steps.\u001B[39;00m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    471\u001B[0m routed_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_method_params(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, props\u001B[38;5;241m=\u001B[39mparams)\n\u001B[0;32m--> 472\u001B[0m Xt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:409\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, X, y, routed_params)\u001B[0m\n\u001B[1;32m    407\u001B[0m     cloned_transformer \u001B[38;5;241m=\u001B[39m clone(transformer)\n\u001B[1;32m    408\u001B[0m \u001B[38;5;66;03m# Fit or load from cache the current transformer\u001B[39;00m\n\u001B[0;32m--> 409\u001B[0m X, fitted_transformer \u001B[38;5;241m=\u001B[39m \u001B[43mfit_transform_one_cached\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcloned_transformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPipeline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_idx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;66;03m# Replace the transformer of the step with the fitted\u001B[39;00m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;66;03m# transformer. This is necessary when loading the transformer\u001B[39;00m\n\u001B[1;32m    420\u001B[0m \u001B[38;5;66;03m# from the cache.\u001B[39;00m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[step_idx] \u001B[38;5;241m=\u001B[39m (name, fitted_transformer)\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/joblib/memory.py:312\u001B[0m, in \u001B[0;36mNotMemorizedFunc.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:1329\u001B[0m, in \u001B[0;36m_fit_transform_one\u001B[0;34m(transformer, X, y, weight, columns, message_clsname, message, params)\u001B[0m\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[1;32m   1328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(transformer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1329\u001B[0m         res \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfit_transform\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1330\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1331\u001B[0m         res \u001B[38;5;241m=\u001B[39m transformer\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[1;32m   1332\u001B[0m             X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\n\u001B[1;32m   1333\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/base.py:1101\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m-> 1101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "Cell \u001B[0;32mIn[120], line 13\u001B[0m, in \u001B[0;36mTextPreprocessor.transform\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 13\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_lemmatization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_stemmanization\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "Cell \u001B[0;32mIn[120], line 13\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 13\u001B[0m     res \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_lemmatization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_stemmanization\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m X]\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "Cell \u001B[0;32mIn[110], line 36\u001B[0m, in \u001B[0;36mpreprocess_advanced\u001B[0;34m(text, lem_tag, stem_tag)\u001B[0m\n\u001B[1;32m     33\u001B[0m tokens \u001B[38;5;241m=\u001B[39m word_tokenize(text)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Removing stop words\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m stop_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[43mstopwords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menglish\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     37\u001B[0m tokens \u001B[38;5;241m=\u001B[39m [word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words]\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Lemmatization\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/corpus/reader/wordlist.py:21\u001B[0m, in \u001B[0;36mWordListCorpusReader.words\u001B[0;34m(self, fileids, ignore_lines_startswith)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwords\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, ignore_lines_startswith\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m     20\u001B[0m         line\n\u001B[0;32m---> 21\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m line_tokenize(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileids\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     22\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m line\u001B[38;5;241m.\u001B[39mstartswith(ignore_lines_startswith)\n\u001B[1;32m     23\u001B[0m     ]\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/corpus/reader/api.py:218\u001B[0m, in \u001B[0;36mCorpusReader.raw\u001B[0;34m(self, fileids)\u001B[0m\n\u001B[1;32m    216\u001B[0m contents \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fileids:\n\u001B[0;32m--> 218\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[1;32m    219\u001B[0m         contents\u001B[38;5;241m.\u001B[39mappend(fp\u001B[38;5;241m.\u001B[39mread())\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concat(contents)\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/corpus/reader/api.py:231\u001B[0m, in \u001B[0;36mCorpusReader.open\u001B[0;34m(self, file)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;124;03mReturn an open stream that can be used to read the given file.\u001B[39;00m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;124;03mIf the file's encoding is not None, then the stream will\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;124;03m:param file: The file identifier of the file to read.\u001B[39;00m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    230\u001B[0m encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding(file)\n\u001B[0;32m--> 231\u001B[0m stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_root\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/data.py:326\u001B[0m, in \u001B[0;36mFileSystemPathPointer.open\u001B[0;34m(self, encoding)\u001B[0m\n\u001B[1;32m    324\u001B[0m stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m encoding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 326\u001B[0m     stream \u001B[38;5;241m=\u001B[39m \u001B[43mSeekableUnicodeStreamReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/compat.py:41\u001B[0m, in \u001B[0;36mpy3_data.<locals>._decorator\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decorator\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     40\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args[\u001B[38;5;241m0\u001B[39m], add_py3_data(args[\u001B[38;5;241m1\u001B[39m])) \u001B[38;5;241m+\u001B[39m args[\u001B[38;5;241m2\u001B[39m:]\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minit_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/data.py:1037\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader.__init__\u001B[0;34m(self, stream, encoding, errors)\u001B[0m\n\u001B[1;32m   1031\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rewind_numchars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1032\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"The number of characters that have been returned since the\u001B[39;00m\n\u001B[1;32m   1033\u001B[0m \u001B[38;5;124;03m   read that started at ``_rewind_checkpoint``.  This is used,\u001B[39;00m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m   together with ``_rewind_checkpoint``, to backtrack to the\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m   beginning of ``linebuffer`` (which is required by ``tell()``).\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1037\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bom \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_bom\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"The length of the byte order marker at the beginning of\u001B[39;00m\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;124;03m   the stream (or None for no byte order marker).\"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/data.py:1403\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader._check_bom\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1401\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_bom\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1402\u001B[0m     \u001B[38;5;66;03m# Normalize our encoding name\u001B[39;00m\n\u001B[0;32m-> 1403\u001B[0m     enc \u001B[38;5;241m=\u001B[39m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m[ -]\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1405\u001B[0m     \u001B[38;5;66;03m# Look up our encoding in the BOM table.\u001B[39;00m\n\u001B[1;32m   1406\u001B[0m     bom_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_BOM_TABLE\u001B[38;5;241m.\u001B[39mget(enc)\n",
      "File \u001B[0;32m/usr/lib/python3.11/re/__init__.py:185\u001B[0m, in \u001B[0;36msub\u001B[0;34m(pattern, repl, string, count, flags)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msub\u001B[39m(pattern, repl, string, count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m    179\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001B[39;00m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001B[39;00m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001B[39;00m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;124;03m    a callable, it's passed the Match object and must return\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;124;03m    a replacement string to be used.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstring\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcount\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Enrich data json with uppercase percentage and comment length feature"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:49:57.764769Z",
     "start_time": "2024-06-24T14:49:57.646910Z"
    }
   },
   "source": [
    "## add feature uppercase percentage\n",
    "def calculate_uppercase_percentage(text):\n",
    "    uppercase_count = 0\n",
    "    total_letters = 0\n",
    "    \n",
    "    for char in text:\n",
    "        if char.isalpha():  # Check if the character is a letter\n",
    "            total_letters += 1\n",
    "            if char.isupper():  # Check if the letter is uppercase\n",
    "                uppercase_count += 1\n",
    "    \n",
    "    if total_letters == 0:\n",
    "        return 0\n",
    "    uppercase_percentage = (uppercase_count / total_letters) * 100\n",
    "    \n",
    "    return uppercase_percentage\n",
    "\n",
    "def classify_uppercase_percentage(percentage):\n",
    "    if percentage < 6:\n",
    "        return \"low\"\n",
    "    elif 6 <= percentage <= 12:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "    \n",
    "# add custom feature: comment length\n",
    "def classify_comment_lenght(length):\n",
    "    if length < 190:\n",
    "        return \"short\"\n",
    "    elif 190 <= length <= 560:\n",
    "        return \"average\"\n",
    "    else:\n",
    "        return \"long\"\n",
    "\n",
    "# Update the data with the new key-value pair\n",
    "updated_data = data.copy()\n",
    "for comment in data:\n",
    "    comment_length = classify_comment_lenght(len(comment[\"text\"]))\n",
    "    uppercase_percentage = calculate_uppercase_percentage(comment[\"text\"])\n",
    "    uppercase_amount = classify_uppercase_percentage(uppercase_percentage)\n",
    "    comment[\"comment_length\"] = comment_length\n",
    "    comment[\"uppercase_amount\"] = uppercase_amount\n",
    "\n",
    "\n",
    "# Save the updated data to a new file\n",
    "with open('../data/processed/Oppositional_thinking_analysis_dataset_with_features.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=4)\n"
   ],
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:49:58.088618Z",
     "start_time": "2024-06-24T14:49:58.066428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = json.loads(open(\"../data/processed/Oppositional_thinking_analysis_dataset_with_features.json\").read())\n",
    "train_df_balanced, test_df = balance_data(data)\n"
   ],
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Add a preprocessor to handle the new features\n",
    "* it uses a OneHotEncoder to encode the custom features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:49:58.760134Z",
     "start_time": "2024-06-24T14:49:58.755524Z"
    }
   },
   "source": [
    "class CombinedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_transformer, ngram_range = (1,1),  additional_features = []):\n",
    "        self.text_transformer = text_transformer\n",
    "        self.additional_features = additional_features\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the individual transformers\n",
    "        self.text_transformer.fit(X['text'])\n",
    "        if self.additional_features:\n",
    "            self.encoder.fit(X[self.additional_features]) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # print type of X\n",
    "        text_features = self.text_transformer.transform(X['text'])\n",
    "     \n",
    "        if self.additional_features:\n",
    "            additional_features = self.encoder.transform(X[self.additional_features]).toarray()\n",
    "            text_features = np.hstack((text_features.toarray(), additional_features))\n",
    "       \n",
    "        return text_features\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## We add some new pipelines \n",
    "* those use the combined features with either CountVectorizer or TfidfVectorizer\n",
    "* they use both lemmatization and stemming\n",
    "* they use a OneHotEncoder for the custom features"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:49:59.606363Z",
     "start_time": "2024-06-24T14:49:59.599913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pre-processing pipelines\n",
    "custom_preprocess = preprocess_advanced\n",
    "\n",
    "def create_advanced_pipelines(ngram_lower_bound, ngram_upper_bound, custom_preprocess, classifier, custom_feautures : list ):\n",
    "    pipelines = {\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization nor stem and ngram: {ngram_lower_bound}, {ngram_upper_bound} with custom features: {custom_feautures} and classifier {classifier}': Pipeline([\n",
    "            ('preprocessor',  TextPreprocessor(custom_preprocess_function=custom_preprocess, use_stemmanization=False, use_lemmatization=False)),\n",
    "            ('features', CombinedFeatures(CountVectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound)), custom_feautures)),\n",
    "            ('classifier', classifier)\n",
    "        ]),\n",
    "        f'TfidfVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization nor stem and ngram: {ngram_lower_bound}, {ngram_upper_bound} with custom features: {custom_feautures} and classifier {classifier}': Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_stemmanization=False, use_lemmatization=False)),\n",
    "            ('features', CombinedFeatures(TfidfVectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound)),custom_feautures)),\n",
    "            ('classifier', classifier)\n",
    "        ]),\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and Lemmatization and stem and ngram: {ngram_lower_bound}, {ngram_upper_bound} with custom features: {custom_feautures} and classifier {classifier}': Pipeline([\n",
    "            ('preprocessor',  TextPreprocessor(custom_preprocess_function=custom_preprocess, use_stemmanization=True, use_lemmatization=True)),\n",
    "            ('features', CombinedFeatures(CountVectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound)), custom_feautures)),\n",
    "            ('classifier', classifier)\n",
    "        ]),\n",
    "        f'TfidfVectorizer with preprocessing function {custom_preprocess.__name__} and Lemmatization and stem and ngram: {ngram_lower_bound}, {ngram_upper_bound} with custom features: {custom_feautures} and classifier {classifier}': Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_stemmanization=True, use_lemmatization=True)),\n",
    "            ('features', CombinedFeatures(TfidfVectorizer(ngram_range=(ngram_lower_bound, ngram_upper_bound)), custom_feautures)),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "    }   \n",
    "    return pipelines\n"
   ],
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## We add a new TextPreprocessor (TODO: WHY?)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:50:00.477633Z",
     "start_time": "2024-06-24T14:50:00.474613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, custom_preprocess_function = preprocess_advanced,  use_lemmatization=True, use_stemmanization=False):\n",
    "        self.custom_preprocess_function = custom_preprocess\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.use_stemmanization = use_stemmanization\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_train_processed = X.copy()\n",
    "        X_train_processed['text'] = X_train_processed['text'].apply(\n",
    "            lambda x: self.custom_preprocess_function(x, lem_tag=self.use_lemmatization, stem_tag=self.use_stemmanization))\n",
    "        return X_train_processed"
   ],
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run and evaluate the advanced pipelines \n",
    "* with naive bayes as well as FNN\n",
    "* use different feature_list combinations"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:50:50.692094Z",
     "start_time": "2024-06-24T14:50:41.797912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "report_file_name = 'classification_report_2.csv'\n",
    "# remove file\n",
    "if os.path.isfile(\"../reports/\" + report_file_name):\n",
    "    os.remove(\"../reports/\" + report_file_name)\n",
    "\n",
    "# Train and evaluate models with different pipelines\n",
    "for classifier in [MultinomialNB(), MLPClassifier(hidden_layer_sizes=(50,))]:\n",
    "    for feature_list in [['uppercase_amount', 'comment_length'], ['comment_length'], ['uppercase_amount']]:\n",
    "        for preprocess_function in [preprocess_advanced, preprocess_basic]:\n",
    "            for ngram_lower_bound in range(1,3):\n",
    "                for ngram_upper_bound in range(1,3): \n",
    "                    if ngram_lower_bound <= ngram_upper_bound:\n",
    "                        pipelines = create_advanced_pipelines(ngram_lower_bound, ngram_upper_bound, preprocess_function, classifier, feature_list)\n",
    "                        for name, pipeline in pipelines.items():\n",
    "                            print(f\"Evaluating {name}...\")\n",
    "                            train_and_evaluate(train_df_balanced, test_df, pipeline)\n",
    "                            # enrich df with the name of the pipeline, ngram range and preprocess function\n",
    "                            report_df['pipeline'] = name\n",
    "                            report_df['classifier'] = pipeline.named_steps['classifier'].__class__.__name__\n",
    "                            report_df['vectorizer'] = pipeline.named_steps['features'].text_transformer\n",
    "                            report_df['features'] = pipeline.named_steps['features'].__class__.__name__\n",
    "                            report_df['ngram_lower_bound'] = ngram_lower_bound\n",
    "                            report_df['ngram_upper_bound'] = ngram_upper_bound\n",
    "                            report_df['preprocess_function'] = preprocess_function.__name__\n",
    "                            report_df['preprocess_function_uses_lemmatization'] = pipeline.named_steps['preprocessor'].use_lemmatization\n",
    "                            report_df['preprocess_function_uses_stemmanization'] = pipeline.named_steps['preprocessor'].use_stemmanization\n",
    "                            report_df['feature_list'] = json.dumps(feature_list)\n",
    "                            save_results_to_csv(report_df, file_name=report_file_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CountVectorizer with preprocessing function preprocess_advanced and no Lemmatization nor stem and ngram: 1, 1 with custom features: ['uppercase_amount', 'comment_length'] and classifier MultinomialNB()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score CountVectorizer(): 0.8459123319635202\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_advanced and no Lemmatization nor stem and ngram: 1, 1 with custom features: ['uppercase_amount', 'comment_length'] and classifier MultinomialNB()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score TfidfVectorizer(): 0.8520913770913771\n",
      "Evaluating CountVectorizer with preprocessing function preprocess_advanced and Lemmatization and stem and ngram: 1, 1 with custom features: ['uppercase_amount', 'comment_length'] and classifier MultinomialNB()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score CountVectorizer(): 0.8289670244407087\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_advanced and Lemmatization and stem and ngram: 1, 1 with custom features: ['uppercase_amount', 'comment_length'] and classifier MultinomialNB()...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[145], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, pipeline \u001B[38;5;129;01min\u001B[39;00m pipelines\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m     \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df_balanced\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m# enrich df with the name of the pipeline, ngram range and preprocess function\u001B[39;00m\n\u001B[1;32m     18\u001B[0m     report_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpipeline\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m name\n",
      "Cell \u001B[0;32mIn[111], line 22\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(train_df, test_df, pipeline)\u001B[0m\n\u001B[1;32m     18\u001B[0m     y_train \u001B[38;5;241m=\u001B[39m train_df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[1;32m     19\u001B[0m     y_test \u001B[38;5;241m=\u001B[39m test_df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[0;32m---> 22\u001B[0m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m     25\u001B[0m report \u001B[38;5;241m=\u001B[39m classification_report(y_test, y_pred, output_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)    \n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:472\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit the model.\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \n\u001B[1;32m    431\u001B[0m \u001B[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;124;03m    Pipeline with fitted steps.\u001B[39;00m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    471\u001B[0m routed_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_method_params(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, props\u001B[38;5;241m=\u001B[39mparams)\n\u001B[0;32m--> 472\u001B[0m Xt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[1;32m    474\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:409\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, X, y, routed_params)\u001B[0m\n\u001B[1;32m    407\u001B[0m     cloned_transformer \u001B[38;5;241m=\u001B[39m clone(transformer)\n\u001B[1;32m    408\u001B[0m \u001B[38;5;66;03m# Fit or load from cache the current transformer\u001B[39;00m\n\u001B[0;32m--> 409\u001B[0m X, fitted_transformer \u001B[38;5;241m=\u001B[39m \u001B[43mfit_transform_one_cached\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcloned_transformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPipeline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_idx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;66;03m# Replace the transformer of the step with the fitted\u001B[39;00m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;66;03m# transformer. This is necessary when loading the transformer\u001B[39;00m\n\u001B[1;32m    420\u001B[0m \u001B[38;5;66;03m# from the cache.\u001B[39;00m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[step_idx] \u001B[38;5;241m=\u001B[39m (name, fitted_transformer)\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/joblib/memory.py:312\u001B[0m, in \u001B[0;36mNotMemorizedFunc.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/pipeline.py:1329\u001B[0m, in \u001B[0;36m_fit_transform_one\u001B[0;34m(transformer, X, y, weight, columns, message_clsname, message, params)\u001B[0m\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[1;32m   1328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(transformer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1329\u001B[0m         res \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfit_transform\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1330\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1331\u001B[0m         res \u001B[38;5;241m=\u001B[39m transformer\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[1;32m   1332\u001B[0m             X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\n\u001B[1;32m   1333\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/base.py:1101\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m-> 1101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "Cell \u001B[0;32mIn[143], line 12\u001B[0m, in \u001B[0;36mTextPreprocessor.transform\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     11\u001B[0m     X_train_processed \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m---> 12\u001B[0m     X_train_processed[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mX_train_processed\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcustom_preprocess_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlem_tag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_lemmatization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstem_tag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_stemmanization\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m X_train_processed\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[143], line 13\u001B[0m, in \u001B[0;36mTextPreprocessor.transform.<locals>.<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     11\u001B[0m     X_train_processed \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m     12\u001B[0m     X_train_processed[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m X_train_processed[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\n\u001B[0;32m---> 13\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcustom_preprocess_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlem_tag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_lemmatization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstem_tag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_stemmanization\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m X_train_processed\n",
      "Cell \u001B[0;32mIn[110], line 43\u001B[0m, in \u001B[0;36mpreprocess_advanced\u001B[0;34m(text, lem_tag, stem_tag)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lem_tag:\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Lemmatization\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     lemmatizer \u001B[38;5;241m=\u001B[39m WordNetLemmatizer()\n\u001B[0;32m---> 43\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43mlemmatizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlemmatize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stem_tag:\n\u001B[1;32m     46\u001B[0m     stemmer \u001B[38;5;241m=\u001B[39m PorterStemmer()\n",
      "Cell \u001B[0;32mIn[110], line 43\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lem_tag:\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Lemmatization\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     lemmatizer \u001B[38;5;241m=\u001B[39m WordNetLemmatizer()\n\u001B[0;32m---> 43\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m [\u001B[43mlemmatizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlemmatize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens]\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stem_tag:\n\u001B[1;32m     46\u001B[0m     stemmer \u001B[38;5;241m=\u001B[39m PorterStemmer()\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/stem/wordnet.py:45\u001B[0m, in \u001B[0;36mWordNetLemmatizer.lemmatize\u001B[0;34m(self, word, pos)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlemmatize\u001B[39m(\u001B[38;5;28mself\u001B[39m, word: \u001B[38;5;28mstr\u001B[39m, pos: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m     34\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 45\u001B[0m     lemmas \u001B[38;5;241m=\u001B[39m \u001B[43mwn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_morphy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmin\u001B[39m(lemmas, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m lemmas \u001B[38;5;28;01melse\u001B[39;00m word\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2100\u001B[0m, in \u001B[0;36mWordNetCorpusReader._morphy\u001B[0;34m(self, form, pos, check_exceptions)\u001B[0m\n\u001B[1;32m   2097\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m filter_forms([form] \u001B[38;5;241m+\u001B[39m exceptions[form])\n\u001B[1;32m   2099\u001B[0m \u001B[38;5;66;03m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001B[39;00m\n\u001B[0;32m-> 2100\u001B[0m forms \u001B[38;5;241m=\u001B[39m \u001B[43mapply_rules\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mform\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2102\u001B[0m \u001B[38;5;66;03m# 2. Return all that are in the database (and check the original too)\u001B[39;00m\n\u001B[1;32m   2103\u001B[0m results \u001B[38;5;241m=\u001B[39m filter_forms([form] \u001B[38;5;241m+\u001B[39m forms)\n",
      "File \u001B[0;32m~/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2075\u001B[0m, in \u001B[0;36mWordNetCorpusReader._morphy.<locals>.apply_rules\u001B[0;34m(forms)\u001B[0m\n\u001B[1;32m   2072\u001B[0m exceptions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception_map[pos]\n\u001B[1;32m   2073\u001B[0m substitutions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMORPHOLOGICAL_SUBSTITUTIONS[pos]\n\u001B[0;32m-> 2075\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_rules\u001B[39m(forms):\n\u001B[1;32m   2076\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m   2077\u001B[0m         form[: \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mlen\u001B[39m(old)] \u001B[38;5;241m+\u001B[39m new\n\u001B[1;32m   2078\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m form \u001B[38;5;129;01min\u001B[39;00m forms\n\u001B[1;32m   2079\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m old, new \u001B[38;5;129;01min\u001B[39;00m substitutions\n\u001B[1;32m   2080\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m form\u001B[38;5;241m.\u001B[39mendswith(old)\n\u001B[1;32m   2081\u001B[0m     ]\n\u001B[1;32m   2083\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfilter_forms\u001B[39m(forms):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
