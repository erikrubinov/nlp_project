{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(open(\"Oppositional_thinking_analysis_dataset.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "CRITICAL      2097\n",
      "CONSPIRACY    2097\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n",
    "\n",
    "# Handle class imbalance in the training set\n",
    "train_df_majority = train_df[train_df.category == 'CRITICAL']\n",
    "train_df_minority = train_df[train_df.category == 'CONSPIRACY']\n",
    "\n",
    "train_df_minority_upsampled = resample(train_df_minority, \n",
    "                                       replace=True,     # sample with replacement\n",
    "                                       n_samples=len(train_df_majority),    # to match majority class\n",
    "                                       random_state=42) # reproducible results\n",
    "\n",
    "train_df_balanced = pd.concat([train_df_majority, train_df_minority_upsampled])\n",
    "\n",
    "print(train_df_balanced['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.76      0.82      0.79       276\n",
      "    CRITICAL       0.90      0.86      0.88       524\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.83      0.84      0.84       800\n",
      "weighted avg       0.85      0.85      0.85       800\n",
      "\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.69      0.87      0.77       276\n",
      "    CRITICAL       0.92      0.79      0.85       524\n",
      "\n",
      "    accuracy                           0.82       800\n",
      "   macro avg       0.81      0.83      0.81       800\n",
      "weighted avg       0.84      0.82      0.82       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define vectorizers\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "# Function to train and evaluate a Naïve Bayes model\n",
    "def train_and_evaluate(train_df, test_df, vectorizer):\n",
    "    X_train = vectorizer.fit_transform(train_df['text'])\n",
    "    y_train = train_df['category']\n",
    "    X_test = vectorizer.transform(test_df['text'])\n",
    "    y_test = test_df['category']\n",
    "    \n",
    "    model = MultinomialNB() #multinomial Naive Bayes\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"Results for {vectorizer.__class__.__name__}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Train and evaluate models with different vectorizers\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    train_and_evaluate(train_df_balanced, test_df, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "CRITICAL      2097\n",
      "CONSPIRACY    2097\n",
      "Name: count, dtype: int64\n",
      "Evaluating CountVectorizer with Stop Words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.78      0.76      0.77       276\n",
      "    CRITICAL       0.88      0.89      0.88       524\n",
      "\n",
      "    accuracy                           0.84       800\n",
      "   macro avg       0.83      0.82      0.82       800\n",
      "weighted avg       0.84      0.84      0.84       800\n",
      "\n",
      "Evaluating TfidfVectorizer with Stop Words...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.76      0.85      0.80       276\n",
      "    CRITICAL       0.92      0.86      0.89       524\n",
      "\n",
      "    accuracy                           0.86       800\n",
      "   macro avg       0.84      0.86      0.85       800\n",
      "weighted avg       0.86      0.86      0.86       800\n",
      "\n",
      "Evaluating Stemmed CountVectorizer...\n",
      "Results for StemmedCountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.72      0.79      0.75       276\n",
      "    CRITICAL       0.88      0.84      0.86       524\n",
      "\n",
      "    accuracy                           0.82       800\n",
      "   macro avg       0.80      0.81      0.81       800\n",
      "weighted avg       0.83      0.82      0.82       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n",
    "\n",
    "# Handle class imbalance in the training set\n",
    "train_df_majority = train_df[train_df.category == 'CRITICAL']\n",
    "train_df_minority = train_df[train_df.category == 'CONSPIRACY']\n",
    "\n",
    "train_df_minority_upsampled = resample(train_df_minority, \n",
    "                                       replace=True,     # sample with replacement\n",
    "                                       n_samples=len(train_df_majority),    # to match majority class\n",
    "                                       random_state=42) # reproducible results\n",
    "\n",
    "train_df_balanced = pd.concat([train_df_majority, train_df_minority_upsampled])\n",
    "\n",
    "print(train_df_balanced['category'].value_counts())\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "def preprocess2(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def preprocess(text:str) -> int:\n",
    "\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) # remove decimals  \n",
    "    text = re.sub(r'[\\:\\-\\']', '', text)  # Remove specific punctuation\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove special characters\n",
    "    text = re.sub(r'\\d+\\.\\d+', '', text)  # Matches one or more digits followed by a dot and one or more digits\n",
    "    text = re.sub(r'\\bcom\\b', '', text, flags=re.IGNORECASE)  # Matches \"com\" at word boundaries (whole word)\n",
    "\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return [preprocess(text) for text in X]\n",
    "    \n",
    "\n",
    "class StemmedCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.vectorizer = CountVectorizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform([' '.join([self.stemmer.stem(word) for word in document.split()]) for document in X])\n",
    "\n",
    "# Pre-processing pipelines\n",
    "pipelines = {\n",
    "    'CountVectorizer with Stop Words': Pipeline([\n",
    "        ('preprocessor', TextPreprocessor()),\n",
    "        ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ]),\n",
    "    'TfidfVectorizer with Stop Words': Pipeline([\n",
    "        ('preprocessor', TextPreprocessor()),\n",
    "        ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ]),\n",
    "    'Stemmed CountVectorizer': Pipeline([\n",
    "        ('preprocessor', TextPreprocessor()),\n",
    "        ('vectorizer', StemmedCountVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Function to train and evaluate a Naïve Bayes model\n",
    "def train_and_evaluate(train_df, test_df, pipeline):\n",
    "    X_train = train_df['text']\n",
    "    y_train = train_df['category']\n",
    "    X_test = test_df['text']\n",
    "    y_test = test_df['category']\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(f\"Results for {pipeline.named_steps['vectorizer'].__class__.__name__}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Train and evaluate models with different pipelines\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    train_and_evaluate(train_df_balanced, test_df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add custom features \n",
    "\n",
    "import json\n",
    "\n",
    "def calculate_uppercase_percentage(text):\n",
    "    uppercase_count = 0\n",
    "    total_letters = 0\n",
    "    \n",
    "    for char in text:\n",
    "        if char.isalpha():  # Check if the character is a letter\n",
    "            total_letters += 1\n",
    "            if char.isupper():  # Check if the letter is uppercase\n",
    "                uppercase_count += 1\n",
    "    \n",
    "    if total_letters == 0:\n",
    "        return 0\n",
    "    uppercase_percentage = (uppercase_count / total_letters) * 100\n",
    "    \n",
    "    return uppercase_percentage\n",
    "\n",
    "def classify_uppercase_percentage(percentage):\n",
    "    if percentage < 6:\n",
    "        return \"low\"\n",
    "    elif 6 <= percentage <= 12:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "# Update the data with the new key-value pair\n",
    "updated_data = []\n",
    "for comment in data:\n",
    "    uppercase_percentage = calculate_uppercase_percentage(comment[\"text\"])\n",
    "    uppercase_amount = classify_uppercase_percentage(uppercase_percentage)\n",
    "    updated_comment = comment.copy()\n",
    "    updated_comment[\"uppercase_amount\"] = uppercase_amount\n",
    "    updated_data.append(updated_comment)\n",
    "\n",
    "# Save the updated data to a new file\n",
    "with open('Oppositional_thinking_analysis_dataset.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "CRITICAL      2097\n",
      "CONSPIRACY    2097\n",
      "Name: count, dtype: int64\n",
      "Evaluating CountVectorizer with Stop Words...\n",
      "Fit 2: Type of X is <class 'pandas.core.frame.DataFrame'>\n",
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.78      0.75      0.77       276\n",
      "    CRITICAL       0.87      0.89      0.88       524\n",
      "\n",
      "    accuracy                           0.84       800\n",
      "   macro avg       0.83      0.82      0.82       800\n",
      "weighted avg       0.84      0.84      0.84       800\n",
      "\n",
      "Evaluating TfidfVectorizer with Stop Words...\n",
      "Fit 2: Type of X is <class 'pandas.core.frame.DataFrame'>\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.77      0.82      0.79       276\n",
      "    CRITICAL       0.90      0.87      0.88       524\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.83      0.84      0.84       800\n",
      "weighted avg       0.85      0.85      0.85       800\n",
      "\n",
      "Evaluating Stemmed CountVectorizer...\n",
      "Fit 2: Type of X is <class 'pandas.core.frame.DataFrame'>\n",
      "Results for StemmedCountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.73      0.78      0.76       276\n",
      "    CRITICAL       0.88      0.85      0.87       524\n",
      "\n",
      "    accuracy                           0.83       800\n",
      "   macro avg       0.81      0.82      0.81       800\n",
      "weighted avg       0.83      0.83      0.83       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Calculate uppercase percentage and classify\n",
    "def calculate_uppercase_percentage(text):\n",
    "    uppercase_count = 0\n",
    "    total_letters = 0\n",
    "    \n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            total_letters += 1\n",
    "            if char.isupper():\n",
    "                uppercase_count += 1\n",
    "    \n",
    "    if total_letters == 0:\n",
    "        return 0\n",
    "    uppercase_percentage = (uppercase_count / total_letters) * 100\n",
    "    \n",
    "    return uppercase_percentage\n",
    "\n",
    "def classify_uppercase_percentage(percentage):\n",
    "    if percentage < 6:\n",
    "        return \"low\"\n",
    "    elif 6 <= percentage <= 12:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "# Update the data with the new key-value pair\n",
    "for comment in data:\n",
    "    uppercase_percentage = calculate_uppercase_percentage(comment[\"text\"])\n",
    "    comment[\"uppercase_amount\"] = classify_uppercase_percentage(uppercase_percentage)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n",
    "\n",
    "# Handle class imbalance in the training set\n",
    "train_df_majority = train_df[train_df.category == 'CRITICAL']\n",
    "train_df_minority = train_df[train_df.category == 'CONSPIRACY']\n",
    "\n",
    "train_df_minority_upsampled = resample(train_df_minority, \n",
    "                                       replace=True,     \n",
    "                                       n_samples=len(train_df_majority),    \n",
    "                                       random_state=42)\n",
    "\n",
    "train_df_balanced = pd.concat([train_df_majority, train_df_minority_upsampled])\n",
    "\n",
    "print(train_df_balanced['category'].value_counts())\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_train_processed = X.copy()\n",
    "        X_train_processed['text'] = X_train_processed['text'].apply(preprocess)\n",
    "        return X_train_processed\n",
    "\n",
    "class StemmedCountVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.vectorizer = CountVectorizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "      \n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform([' '.join([self.stemmer.stem(word) for word in document.split()]) for document in X])\n",
    "\n",
    "class CombinedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_transformer, additional_features):\n",
    "        self.text_transformer = text_transformer\n",
    "        self.additional_features = additional_features\n",
    "        self.encoder = OneHotEncoder()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        # Fit the individual transformers\n",
    "        self.text_transformer.fit(X['text'])\n",
    "        self.encoder.fit(X[[self.additional_features]])  # Note double brackets for DataFrame shape\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        # Transform the data\n",
    "        text_features = self.text_transformer.transform(X['text'])\n",
    "        additional_features = self.encoder.transform(X[[self.additional_features]]).toarray()  # Note double brackets for DataFrame shape\n",
    "        return np.hstack((text_features.toarray(), additional_features))\n",
    "\n",
    "# Pre-processing pipelines\n",
    "pipelines = {\n",
    "    'CountVectorizer with Stop Words': Pipeline([\n",
    "        ('preprocessor', TextPreprocessor()),\n",
    "        ('features', CombinedFeatures(CountVectorizer(stop_words='english'), 'uppercase_amount')),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ]),\n",
    "    'TfidfVectorizer with Stop Words': Pipeline([\n",
    "        ('preprocessor', TextPreprocessor()),\n",
    "        ('features', CombinedFeatures(TfidfVectorizer(stop_words='english'), 'uppercase_amount')),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ]),\n",
    "    'Stemmed CountVectorizer': Pipeline([\n",
    "        ('preprocessor', TextPreprocessor()),\n",
    "        ('features', CombinedFeatures(StemmedCountVectorizer(), 'uppercase_amount')),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Function to train and evaluate a Naïve Bayes model\n",
    "def train_and_evaluate(train_df, test_df, pipeline):\n",
    "    X_train = train_df[['text', 'uppercase_amount']]\n",
    "    #print(\"1:\",type(X_train))\n",
    "    #print(\"1:\",X_train)\n",
    "    y_train = train_df['category']\n",
    "    X_test = test_df[['text', 'uppercase_amount']]\n",
    "    y_test = test_df['category']\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(f\"Results for {pipeline.named_steps['features'].text_transformer.__class__.__name__}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Train and evaluate models with different pipelines\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    train_and_evaluate(train_df_balanced, test_df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                               text    category  \\\n",
      "1281   4912  latest vaers data us deaths reported may occur...    CRITICAL   \n",
      "3392    713  needs stop worldwide immediately every human r...    CRITICAL   \n",
      "3501   3264  united airlines ceo kirby feel bad people gett...    CRITICAL   \n",
      "1718  12399  breaking new icelandic study shows covid reinf...    CRITICAL   \n",
      "2162   3601  carlson would definitely higher vaccination ra...    CRITICAL   \n",
      "...     ...                                                ...         ...   \n",
      "95     1440  father young son refused food nt vaccine passp...    CRITICAL   \n",
      "1898   4859  severe rhabdomyolysis multiorgan failure covid...    CRITICAL   \n",
      "3838   4577  year old brazil suffered facial paralysis weak...    CRITICAL   \n",
      "3355  11142  dr mccullough interview joe rogan got views ru...    CRITICAL   \n",
      "2544   6487  using fake c tests deadiy remdevisir drug kiii...  CONSPIRACY   \n",
      "\n",
      "     uppercase_amount  \n",
      "1281          neutral  \n",
      "3392              low  \n",
      "3501             high  \n",
      "1718              low  \n",
      "2162             high  \n",
      "...               ...  \n",
      "95                low  \n",
      "1898              low  \n",
      "3838              low  \n",
      "3355              low  \n",
      "2544          neutral  \n",
      "\n",
      "[3200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in nltk.corpus.stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "X_train_processed = train_df.copy()\n",
    "X_train_processed['text'] = X_train_processed['text'].apply(preprocess)\n",
    "\n",
    "# Check the result\n",
    "print(X_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united airlines ceo kirby feel bad people getting fired getting vaccinated\n"
     ]
    }
   ],
   "source": [
    "print(res[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.77      0.83      0.80       276\n",
      "    CRITICAL       0.91      0.87      0.89       524\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.84      0.85      0.84       800\n",
      "weighted avg       0.86      0.85      0.86       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Assuming the preprocess and preprocess2 functions are defined as before\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return [preprocess(text) for text in X]\n",
    "\n",
    "# Combine features using ColumnTransformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_vect', TfidfVectorizer(stop_words='english'), 'text'),\n",
    "        ('uppercase_ohe', OneHotEncoder(), ['uppercase_amount'])\n",
    "    ],\n",
    "    remainder='drop'  # This drops the columns that are not specified\n",
    ")\n",
    "\n",
    "# Define the pipeline with the ColumnTransformer\n",
    "pipeline = Pipeline([\n",
    "    ('features', column_transformer),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n",
    "\n",
    "# Handle class imbalance\n",
    "train_df_majority = train_df[train_df.category == 'CRITICAL']\n",
    "train_df_minority = train_df[train_df.category == 'CONSPIRACY']\n",
    "train_df_minority_upsampled = resample(train_df_minority, replace=True, n_samples=len(train_df_majority), random_state=42)\n",
    "train_df_balanced = pd.concat([train_df_majority, train_df_minority_upsampled])\n",
    "\n",
    "def train_and_evaluate(train_df, test_df, pipeline):\n",
    "    X_train = train_df[['text', 'uppercase_amount']]\n",
    "    y_train = train_df['category']\n",
    "    X_test = test_df[['text', 'uppercase_amount']]\n",
    "    y_test = test_df['category']\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate the model\n",
    "train_and_evaluate(train_df_balanced, test_df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
