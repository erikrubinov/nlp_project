{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(open(\"Oppositional_thinking_analysis_dataset.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Dataset\n",
    "\"\"\"\n",
    "\n",
    "def balance_data(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.pop('id')\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n",
    "\n",
    "    # Handle class imbalance in the training set\n",
    "    train_df_majority = train_df[train_df.category == 'CRITICAL']\n",
    "    train_df_minority = train_df[train_df.category == 'CONSPIRACY']\n",
    "\n",
    "    train_df_minority_upsampled = resample(train_df_minority, \n",
    "                                        replace=True,     \n",
    "                                        n_samples=len(train_df_majority),    \n",
    "                                        random_state=42)\n",
    "\n",
    "    train_df_balanced = pd.concat([train_df_majority, train_df_minority_upsampled])\n",
    "\n",
    "    return train_df_balanced, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_easy(text, lem_tag = True, stem_tag = False):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    if lem_tag:\n",
    "    # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    if stem_tag:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text:str, lem_tag = True, stem_tag = False) -> int:\n",
    "\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) # remove decimals  \n",
    "    text = re.sub(r'[\\:\\-\\']', '', text)  # Remove specific punctuation\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove special characters\n",
    "    text = re.sub(r'\\d+\\.\\d+', '', text)  # Matches one or more digits followed by a dot and one or more digits\n",
    "    text = re.sub(r'\\bcom\\b', '', text, flags=re.IGNORECASE)  # Matches \"com\" at word boundaries (whole word)\n",
    "\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    if lem_tag:\n",
    "    # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    if stem_tag:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CountVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 1, 1...\n",
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.78      0.77      0.78       276\n",
      "    CRITICAL       0.88      0.89      0.88       524\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.83      0.83      0.83       800\n",
      "weighted avg       0.85      0.85      0.85       800\n",
      "\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 1, 1...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.75      0.86      0.80       276\n",
      "    CRITICAL       0.92      0.85      0.88       524\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.83      0.85      0.84       800\n",
      "weighted avg       0.86      0.85      0.85       800\n",
      "\n",
      "Evaluating CountVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 1, 2...\n",
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.76      0.80      0.78       276\n",
      "    CRITICAL       0.89      0.87      0.88       524\n",
      "\n",
      "    accuracy                           0.84       800\n",
      "   macro avg       0.83      0.83      0.83       800\n",
      "weighted avg       0.85      0.84      0.84       800\n",
      "\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 1, 2...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.69      0.89      0.78       276\n",
      "    CRITICAL       0.93      0.79      0.85       524\n",
      "\n",
      "    accuracy                           0.82       800\n",
      "   macro avg       0.81      0.84      0.82       800\n",
      "weighted avg       0.85      0.82      0.83       800\n",
      "\n",
      "Evaluating CountVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 2, 2...\n",
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.76      0.70      0.73       276\n",
      "    CRITICAL       0.85      0.89      0.87       524\n",
      "\n",
      "    accuracy                           0.82       800\n",
      "   macro avg       0.80      0.79      0.80       800\n",
      "weighted avg       0.82      0.82      0.82       800\n",
      "\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 2, 2...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.73      0.78      0.75       276\n",
      "    CRITICAL       0.88      0.85      0.86       524\n",
      "\n",
      "    accuracy                           0.82       800\n",
      "   macro avg       0.81      0.81      0.81       800\n",
      "weighted avg       0.83      0.82      0.83       800\n",
      "\n",
      "Evaluating CountVectorizer with preprocessing function preprocess_easy and no Lemmatization and ngram: 1, 1...\n",
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.77      0.78      0.78       276\n",
      "    CRITICAL       0.88      0.88      0.88       524\n",
      "\n",
      "    accuracy                           0.84       800\n",
      "   macro avg       0.83      0.83      0.83       800\n",
      "weighted avg       0.85      0.84      0.85       800\n",
      "\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_easy and no Lemmatization and ngram: 1, 1...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.69      0.87      0.77       276\n",
      "    CRITICAL       0.92      0.79      0.85       524\n",
      "\n",
      "    accuracy                           0.82       800\n",
      "   macro avg       0.81      0.83      0.81       800\n",
      "weighted avg       0.84      0.82      0.82       800\n",
      "\n",
      "Evaluating CountVectorizer with preprocessing function preprocess_easy and no Lemmatization and ngram: 1, 2...\n",
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.75      0.85      0.80       276\n",
      "    CRITICAL       0.92      0.85      0.88       524\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.83      0.85      0.84       800\n",
      "weighted avg       0.86      0.85      0.85       800\n",
      "\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_easy and no Lemmatization and ngram: 1, 2...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.61      0.93      0.74       276\n",
      "    CRITICAL       0.95      0.69      0.80       524\n",
      "\n",
      "    accuracy                           0.78       800\n",
      "   macro avg       0.78      0.81      0.77       800\n",
      "weighted avg       0.84      0.78      0.78       800\n",
      "\n",
      "Evaluating CountVectorizer with preprocessing function preprocess_easy and no Lemmatization and ngram: 2, 2...\n",
      "Results for CountVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.75      0.79      0.77       276\n",
      "    CRITICAL       0.89      0.86      0.87       524\n",
      "\n",
      "    accuracy                           0.84       800\n",
      "   macro avg       0.82      0.83      0.82       800\n",
      "weighted avg       0.84      0.84      0.84       800\n",
      "\n",
      "Evaluating TfidfVectorizer with preprocessing function preprocess_easy and no Lemmatization and ngram: 2, 2...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.66      0.89      0.76       276\n",
      "    CRITICAL       0.93      0.76      0.83       524\n",
      "\n",
      "    accuracy                           0.80       800\n",
      "   macro avg       0.79      0.82      0.80       800\n",
      "weighted avg       0.83      0.80      0.81       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, custom_preprocess_function, use_lemmatization=True, use_stemmanization = True):\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.use_stemmanization = use_stemmanization\n",
    "        self.preprocess_function = custom_preprocess_function\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        res = [self.preprocess_function(text, self.use_lemmatization, self.use_stemmanization) for text in X]\n",
    "        return res\n",
    "    \n",
    "\n",
    "    def __init__(self, text_transformer, ngram_range = (1,1),  additional_features = []):\n",
    "        self.text_transformer = text_transformer\n",
    "        self.additional_features = additional_features\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the individual transformers\n",
    "        self.text_transformer.fit(X['text'])\n",
    "        if self.additional_features:\n",
    "            self.encoder.fit(X[self.additional_features]) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "      \n",
    "        text_features = self.text_transformer.transform(X['text'])\n",
    "     \n",
    "        if self.additional_features:\n",
    "            additional_features = self.encoder.transform(X[self.additional_features]).toarray()\n",
    "            text_features = np.hstack((text_features.toarray(), additional_features))\n",
    "       \n",
    "        return text_features\n",
    "# Pre-processing pipelines\n",
    "def create_pipelines(i,j,custom_preprocess ):\n",
    "    pipelines = {\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization and ngram: {i}, {j}': Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_lemmatization=False, use_stemmanization= False)),\n",
    "            ('vectorizer', CountVectorizer(ngram_range=(i, j))),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ]),\n",
    "        f'TfidfVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization and ngram: {i}, {j}': Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_lemmatization=False, use_stemmanization= False)),\n",
    "            ('vectorizer', TfidfVectorizer(ngram_range=(i, j))),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ])\n",
    "    }\n",
    "    return pipelines\n",
    "\n",
    "# Function to train and evaluate a Naïve Bayes model\n",
    "def train_and_evaluate(train_df, test_df, pipeline):\n",
    "    X_train = train_df['text']\n",
    "    #X_train = test_df[['text', 'uppercase_amount', 'comment_length']]\n",
    "\n",
    "    y_train = train_df['category']\n",
    "    X_test = test_df['text']\n",
    "    y_test = test_df['category']\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(f\"Results for {pipeline.named_steps['vectorizer'].__class__.__name__}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Train and evaluate models with different pipelines\n",
    "train_df_balanced = balance_data(data)[0]\n",
    "test_df = balance_data(data)[1]\n",
    "for preprocess_function in [preprocess, preprocess_easy]:\n",
    "    for i in range(1,3):\n",
    "        for j in range(1,3):\n",
    "            if i<=j:\n",
    "                pipelines = create_pipelines(i,j, preprocess_function)\n",
    "                for name, pipeline in pipelines.items():\n",
    "                    print(f\"Evaluating {name}...\")\n",
    "                    train_and_evaluate(train_df_balanced, test_df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add feature uppercase percentage\n",
    "def calculate_uppercase_percentage(text):\n",
    "    uppercase_count = 0\n",
    "    total_letters = 0\n",
    "    \n",
    "    for char in text:\n",
    "        if char.isalpha():  # Check if the character is a letter\n",
    "            total_letters += 1\n",
    "            if char.isupper():  # Check if the letter is uppercase\n",
    "                uppercase_count += 1\n",
    "    \n",
    "    if total_letters == 0:\n",
    "        return 0\n",
    "    uppercase_percentage = (uppercase_count / total_letters) * 100\n",
    "    \n",
    "    return uppercase_percentage\n",
    "\n",
    "def classify_uppercase_percentage(percentage):\n",
    "    if percentage < 6:\n",
    "        return \"low\"\n",
    "    elif 6 <= percentage <= 12:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "# Update the data with the new key-value pair\n",
    "updated_data = []\n",
    "for comment in data:\n",
    "    uppercase_percentage = calculate_uppercase_percentage(comment[\"text\"])\n",
    "    uppercase_amount = classify_uppercase_percentage(uppercase_percentage)\n",
    "    updated_comment = comment.copy()\n",
    "    updated_comment[\"uppercase_amount\"] = uppercase_amount\n",
    "    updated_data.append(updated_comment)\n",
    "\n",
    "# Save the updated data to a new file\n",
    "with open('Oppositional_thinking_analysis_dataset.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom feature: comment length\n",
    "def classify_comment_lenght(length):\n",
    "    if length < 190:\n",
    "        return \"short\"\n",
    "    elif 190 <= length <= 560:\n",
    "        return \"average\"\n",
    "    else:\n",
    "        return \"long\"\n",
    "\n",
    "# Update the data with the new key-value pair\n",
    "updated_data = []\n",
    "for comment in data:\n",
    "    uppercase_amount = classify_comment_lenght(len(comment[\"text\"]))\n",
    "    updated_comment = comment.copy()\n",
    "    updated_comment[\"comment_length\"] = uppercase_amount\n",
    "    updated_data.append(updated_comment)\n",
    "\n",
    "# Save the updated data to a new file\n",
    "with open('Oppositional_thinking_analysis_dataset.json', 'w') as f:\n",
    "    json.dump(updated_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CountVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 1, 1 with custom features: ['uppercase_amount', 'comment_length']...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.73      0.83      0.78       276\n",
      "    CRITICAL       0.90      0.84      0.87       524\n",
      "\n",
      "    accuracy                           0.83       800\n",
      "   macro avg       0.82      0.83      0.82       800\n",
      "weighted avg       0.84      0.83      0.84       800\n",
      "\n",
      "Evaluating CountVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 1, 2 with custom features: ['uppercase_amount', 'comment_length']...\n",
      "Results for TfidfVectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.70      0.91      0.79       276\n",
      "    CRITICAL       0.95      0.79      0.86       524\n",
      "\n",
      "    accuracy                           0.83       800\n",
      "   macro avg       0.82      0.85      0.83       800\n",
      "weighted avg       0.86      0.83      0.84       800\n",
      "\n",
      "Evaluating CountVectorizer with preprocessing function preprocess and no Lemmatization and ngram: 2, 1 with custom features: ['uppercase_amount', 'comment_length']...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid value for ngram_range=(2, 1) lower boundary larger than the upper boundary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m test_df \u001b[38;5;241m=\u001b[39m balance_data(data)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m train_and_evaluate(train_df_balanced, test_df, pipeline)\n",
      "Cell \u001b[0;32mIn[73], line 83\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_df, test_df, pipeline)\u001b[0m\n\u001b[1;32m     80\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     81\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 83\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     84\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtext_transformer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    406\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    409\u001b[0m     cloned_transformer,\n\u001b[1;32m    410\u001b[0m     X,\n\u001b[1;32m    411\u001b[0m     y,\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    413\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    414\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    415\u001b[0m     params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    416\u001b[0m )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1307\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[73], line 27\u001b[0m, in \u001b[0;36mCombinedFeatures.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Fit the individual transformers\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_transformer\u001b[38;5;241m.\u001b[39mfit(X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_features:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mfit(X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_features]) \n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2108\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2103\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2104\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2105\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2106\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2107\u001b[0m )\n\u001b[0;32m-> 2108\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1371\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m     )\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_vocabulary()\n",
      "File \u001b[0;32m~/anaconda3/envs/metal/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517\u001b[0m, in \u001b[0;36m_VectorizerMixin._validate_ngram_range\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m min_n, max_m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_n \u001b[38;5;241m>\u001b[39m max_m:\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for ngram_range=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower boundary larger than the upper boundary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_range)\n\u001b[1;32m    521\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid value for ngram_range=(2, 1) lower boundary larger than the upper boundary."
     ]
    }
   ],
   "source": [
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, custom_preprocess_function = preprocess,  use_lemmatization=True, use_stem=False):\n",
    "        self.custom_preprocess_function = custom_preprocess\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.use_stem = use_stem\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_train_processed = X.copy()\n",
    "        X_train_processed['text'] = X_train_processed['text'].apply(\n",
    "            lambda x: self.custom_preprocess_function(x, lem_tag=self.use_lemmatization, stem_tag=self.use_stem))\n",
    "        return X_train_processed\n",
    "\n",
    "\n",
    "class CombinedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_transformer, ngram_range = (1,1),  additional_features = []):\n",
    "        self.text_transformer = text_transformer\n",
    "        self.additional_features = additional_features\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the individual transformers\n",
    "        self.text_transformer.fit(X['text'])\n",
    "        if self.additional_features:\n",
    "            self.encoder.fit(X[self.additional_features]) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "      \n",
    "        text_features = self.text_transformer.transform(X['text'])\n",
    "     \n",
    "        if self.additional_features:\n",
    "            additional_features = self.encoder.transform(X[self.additional_features]).toarray()\n",
    "            text_features = np.hstack((text_features.toarray(), additional_features))\n",
    "       \n",
    "        return text_features\n",
    "\n",
    "# Pre-processing pipelines\n",
    "\n",
    "custom_preprocess = preprocess\n",
    "\n",
    "\n",
    "def create_pipelines(i,j,custom_preprocess, custom_feautures : list ):\n",
    "    pipelines = {\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization and ngram: {i}, {j} with custom features: {custom_feautures}': Pipeline([\n",
    "            ('preprocessor',  TextPreprocessor(custom_preprocess_function=custom_preprocess, use_lemmatization=False, use_stem= False)),\n",
    "            ('features', CombinedFeatures(CountVectorizer(ngram_range=(i, j)), custom_feautures)),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ]),\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization and ngram: {i}, {j} with custom features: {custom_feautures}': Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_lemmatization=False, use_stem= False)),\n",
    "            ('features', CombinedFeatures(TfidfVectorizer(ngram_range=(i, j)),custom_feautures)),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ]),\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization and ngram: {i}, {j} with custom features: {custom_feautures}': Pipeline([\n",
    "            ('preprocessor',  TextPreprocessor(custom_preprocess_function=custom_preprocess, use_lemmatization=True, use_stem= True)),\n",
    "            ('features', CombinedFeatures(CountVectorizer(ngram_range=(i, j)), custom_feautures)),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ]),\n",
    "        f'CountVectorizer with preprocessing function {custom_preprocess.__name__} and no Lemmatization and ngram: {i}, {j} with custom features: {custom_feautures}': Pipeline([\n",
    "            ('preprocessor', TextPreprocessor(custom_preprocess_function=custom_preprocess, use_lemmatization=True, use_stem= True)),\n",
    "            ('features', CombinedFeatures(TfidfVectorizer(ngram_range=(i, j)), custom_feautures)),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ])\n",
    "        \n",
    "    }   \n",
    "    return pipelines\n",
    "\n",
    "\n",
    "\n",
    "# Function to train and evaluate a Naïve Bayes model\n",
    "def train_and_evaluate(train_df, test_df, pipeline):\n",
    "    X_train = train_df[['text', 'uppercase_amount', 'comment_length']]\n",
    "    y_train = train_df['category']\n",
    "    X_test = test_df[['text']]\n",
    "    y_test = test_df['category']\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(f\"Results for {pipeline.named_steps['features'].text_transformer.__class__.__name__}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Train and evaluate models with different pipelines\n",
    "for feature_list in [['uppercase_amount', 'comment_length'], ['comment_length'], ['uppercase_amount']]:\n",
    "    for preprocess_function in [preprocess, preprocess_easy]:\n",
    "        for i in range(1,3):\n",
    "            for j in range(1,3): \n",
    "                if i<=j\n",
    "                pipelines = create_pipelines(i,j, preprocess_function, feature_list)\n",
    "                for name, pipeline in pipelines.items():\n",
    "                    train_df_balanced = balance_data(data)[0]\n",
    "                    test_df = balance_data(data)[1]\n",
    "                    print(f\"Evaluating {name}...\")\n",
    "                    train_and_evaluate(train_df_balanced, test_df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  CONSPIRACY       0.77      0.83      0.80       276\n",
      "    CRITICAL       0.91      0.87      0.89       524\n",
      "\n",
      "    accuracy                           0.85       800\n",
      "   macro avg       0.84      0.85      0.84       800\n",
      "weighted avg       0.86      0.85      0.86       800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Assuming the preprocess and preprocess2 functions are defined as before\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return [preprocess(text) for text in X]\n",
    "\n",
    "# Combine features using ColumnTransformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_vect', TfidfVectorizer(stop_words='english'), 'text'),\n",
    "        ('uppercase_ohe', OneHotEncoder(), ['uppercase_amount'])\n",
    "    ],\n",
    "    remainder='drop'  # This drops the columns that are not specified\n",
    ")\n",
    "\n",
    "# Define the pipeline with the ColumnTransformer\n",
    "pipeline = Pipeline([\n",
    "    ('features', column_transformer),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)\n",
    "\n",
    "# Handle class imbalance\n",
    "train_df_majority = train_df[train_df.category == 'CRITICAL']\n",
    "train_df_minority = train_df[train_df.category == 'CONSPIRACY']\n",
    "train_df_minority_upsampled = resample(train_df_minority, replace=True, n_samples=len(train_df_majority), random_state=42)\n",
    "train_df_balanced = pd.concat([train_df_majority, train_df_minority_upsampled])\n",
    "\n",
    "def train_and_evaluate(train_df, test_df, pipeline):\n",
    "    X_train = train_df[['text', 'uppercase_amount']]\n",
    "    y_train = train_df['category']\n",
    "    X_test = test_df[['text', 'uppercase_amount']]\n",
    "    y_test = test_df['category']\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate the model\n",
    "train_and_evaluate(train_df_balanced, test_df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
