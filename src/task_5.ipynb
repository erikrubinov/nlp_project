{
 "cells": [
  {
   "cell_type": "code",
   "id": "8a22b195-2d05-4671-8e90-0c1bc7b6f9c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:42:12.785038Z",
     "start_time": "2024-06-25T14:42:09.616194Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/niclasstoffregen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/niclasstoffregen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/niclasstoffregen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b53618fe-b1be-49ad-9951-590941d10563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:42:15.543129Z",
     "start_time": "2024-06-25T14:42:15.521634Z"
    }
   },
   "source": [
    "data = json.loads(open(\"../data/processed/Oppositional_thinking_analysis_dataset.json\").read())\n",
    "data = pd.DataFrame(data)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3887449b-2e33-4d6d-be27-ee4e06f3f1a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:42:16.017929Z",
     "start_time": "2024-06-25T14:42:16.010969Z"
    }
   },
   "source": [
    "def preprocess_basic(text, lem_tag = True, stem_tag = False):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    if lem_tag:\n",
    "    # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    if stem_tag:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_advanced(text:str, lem_tag = True, stem_tag = False) -> int:\n",
    "\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) # remove decimals  \n",
    "    text = re.sub(r'[\\:\\-\\']', '', text)  # Remove specific punctuation\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove special characters\n",
    "    text = re.sub(r'\\d+\\.\\d+', '', text)  # Matches one or more digits followed by a dot and one or more digits\n",
    "    text = re.sub(r'\\bcom\\b', '', text, flags=re.IGNORECASE)  # Matches \"com\" at word boundaries (whole word)\n",
    "\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    if lem_tag:\n",
    "    # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    if stem_tag:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9a2f8bfd-91f9-444c-9f58-773e6417a4ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:43:45.605806Z",
     "start_time": "2024-06-25T14:43:42.047650Z"
    }
   },
   "source": [
    "data['cleaned_text'] = data['text'].apply(preprocess_advanced)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_inputs = tokenizer(data['cleaned_text'].tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "#encoded_inputs = pd.DataFrame.from_dict(encoded_inputs)\n",
    "\n",
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['category'])\n",
    "labels = torch.tensor(data['label_encoded'].values)\n",
    "\n",
    "\n",
    "#print(\"Shape of encoded_inputs: \", encoded_inputs.shape)\n",
    "print(\"Shape of labels: \", labels.shape)\n",
    "\n",
    "# Split data\n",
    "\n",
    "input_ids, val_input_ids, train_labels, val_labels = train_test_split(encoded_inputs['input_ids'], labels, test_size=0.2, random_state=42)\n",
    "token_type_ids, val_token_type_ids, _, _ = train_test_split(encoded_inputs['token_type_ids'], labels, test_size=0.2, random_state=42)\n",
    "attention_mask, val_attention_mask, _, _ = train_test_split(encoded_inputs['attention_mask'], labels, test_size=0.2, random_state=42)\n",
    "train_inputs = pd.DataFrame(columns=[\"input_ids\", \"token_type_ids\",\"attention_mask\"], data=zip(input_ids, token_type_ids, attention_mask))\n",
    "val_inputs = pd.DataFrame(columns=[\"input_ids\", \"token_type_ids\",\"attention_mask\"], data=zip(val_input_ids, val_token_type_ids, val_attention_mask))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of labels:  torch.Size([4000])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "2b8d3145-c4d3-4c77-bff1-69ec6fe3511e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": "train_inputs['input_ids']",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "235d687d-8789-4d61-8211-15f781abb1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:46:14.098492Z",
     "start_time": "2024-06-25T14:46:14.032700Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_data = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], torch.tensor(train_labels))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "validation_data = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], torch.tensor(val_labels))\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "        batch_input_ids = batch_input_ids.to(device)\n",
    "        batch_input_mask = batch_input_mask.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Average training loss for epoch {epoch}: {avg_train_loss}\")\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_accuracy = 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "        batch_input_ids = batch_input_ids.to(device)\n",
    "        batch_input_mask = batch_input_mask.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        eval_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1).flatten()\n",
    "        eval_accuracy += (predictions == batch_labels).cpu().numpy().mean()\n",
    "\n",
    "    avg_val_loss = eval_loss / len(validation_dataloader)\n",
    "    avg_val_accuracy = eval_accuracy / len(validation_dataloader)\n",
    "    print(f\"Validation loss for epoch {epoch}: {avg_val_loss}\")\n",
    "    print(f\"Validation accuracy for epoch {epoch}: {avg_val_accuracy}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/hr_8w4nd2nn8r21t2r09j11r0000gn/T/ipykernel_28425/3847337491.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], torch.tensor(train_labels))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Prepare DataLoader\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m train_data \u001B[38;5;241m=\u001B[39m \u001B[43mTensorDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_inputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m train_sampler \u001B[38;5;241m=\u001B[39m RandomSampler(train_data)\n\u001B[1;32m      4\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_data, sampler\u001B[38;5;241m=\u001B[39mtrain_sampler, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:203\u001B[0m, in \u001B[0;36mTensorDataset.__init__\u001B[0;34m(self, *tensors)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mtensors: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;43mall\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtensors\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize mismatch between tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:204\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mtensors: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[0;32m--> 204\u001B[0m         \u001B[43mtensors\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m tensors\n\u001B[1;32m    205\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize mismatch between tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not callable"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f129f026e542db5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
