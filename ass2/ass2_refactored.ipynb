{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:32:05.132442Z",
     "start_time": "2024-07-27T08:30:56.054583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from csv import QUOTE_NONE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "import spacy\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!python3 -m spacy download fr_core_news_sm"
   ],
   "id": "3b5d9883f748e844",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/niclasstoffregen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m340.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\r\n",
      "Requirement already satisfied: setuptools in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (70.1.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\r\n",
      "Requirement already satisfied: wrapt in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n",
      "Collecting fr-core-news-sm==3.7.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.3/16.3 MB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from fr-core-news-sm==3.7.0) (3.7.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.4)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.8.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\r\n",
      "Requirement already satisfied: setuptools in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (70.1.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.6.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.7.1)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.18.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\r\n",
      "Requirement already satisfied: wrapt in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.16.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "5120b5ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:32:11.104673Z",
     "start_time": "2024-07-27T08:32:11.102604Z"
    }
   },
   "source": [
    "# Define the paths to your files\n",
    "english_file_path = \"fr-en/europarl-v7.fr-en.en\"\n",
    "french_file_path = \"fr-en/europarl-v7.fr-en.fr\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6f79604e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:32:11.933029Z",
     "start_time": "2024-07-27T08:32:11.931329Z"
    }
   },
   "source": [
    "########### TASK 1###############"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "0d99ea9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:32:12.333155Z",
     "start_time": "2024-07-27T08:32:12.331476Z"
    }
   },
   "source": [
    "########LOAD DATA###############"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "124d7d2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:32:26.487923Z",
     "start_time": "2024-07-27T08:32:17.431599Z"
    }
   },
   "source": [
    "\"\"\" Function to load data into a pandas DataFrame without treating any character as quotes\"\"\"\n",
    "\n",
    "def load_data_to_dataframe(file_path):\n",
    "    # Read the entire file as a single column DataFrame, ignoring any quoting\n",
    "    return pd.read_csv(file_path, header=None, names=['text'], encoding='utf-8', sep='\\t', quoting=QUOTE_NONE, engine='python')\n",
    "\n",
    "# Load the data\n",
    "english_data = load_data_to_dataframe(english_file_path)\n",
    "french_data = load_data_to_dataframe(french_file_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English DataFrame sample:\n",
      "                                                text\n",
      "0                          Resumption of the session\n",
      "1  I declare resumed the session of the European ...\n",
      "2  Although, as you will have seen, the dreaded '...\n",
      "3  You have requested a debate on this subject in...\n",
      "4  In the meantime, I should like to observe a mi...\n",
      "French DataFrame sample:\n",
      "                                                text\n",
      "0                              Reprise de la session\n",
      "1  Je déclare reprise la session du Parlement eur...\n",
      "2  Comme vous avez pu le constater, le grand \"bog...\n",
      "3  Vous avez souhaité un débat à ce sujet dans le...\n",
      "4  En attendant, je souhaiterais, comme un certai...\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "37d7e151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:32:28.797575Z",
     "start_time": "2024-07-27T08:32:28.795477Z"
    }
   },
   "source": [
    "##############DATA ANALYSIS##################"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "bcb90a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:32:29.289641Z",
     "start_time": "2024-07-27T08:32:29.287818Z"
    }
   },
   "source": [
    "## Take 10% fraction of data randomly sampled\n",
    "english_data = english_data.sample(frac=0.1, random_state=42)\n",
    "french_data = french_data.sample(frac=0.1, random_state=42)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "06d518d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:34:53.752171Z",
     "start_time": "2024-07-27T08:34:53.749972Z"
    }
   },
   "source": [
    "########### TASK 2###############"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "f36bdae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:34:54.237382Z",
     "start_time": "2024-07-27T08:34:54.235426Z"
    }
   },
   "source": [
    "##################DATA PREPROCESSING#################################"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "86254f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:34:54.778705Z",
     "start_time": "2024-07-27T08:34:54.652426Z"
    }
   },
   "source": [
    "def preprocess_data(data_en, data_fr):\n",
    "    # Lowercase the text\n",
    "    \"\"\"\n",
    "    Normalizing case helps reduce the complexity of the language model by treating words like “The” and “the” as the same word, which can be particularly helpful in languages like English where capitalization is more stylistic than semantic.\n",
    "    \"\"\"\n",
    "    data_en['text'] = data_en['text'].str.lower()\n",
    "    data_fr['text'] = data_fr['text'].str.lower()\n",
    "\n",
    "    # Remove XML tags\n",
    "    \"\"\"\n",
    "    Lines containing XML-tags are likely not actual conversational or formal text but rather formatting or metadata which is irrelevant for translation purposes\n",
    "    \"\"\"\n",
    "    data_en['text'] = data_en['text'].apply(lambda x: '' if x.strip().startswith('<') else x)\n",
    "    data_fr['text'] = data_fr['text'].apply(lambda x: '' if x.strip().startswith('<') else x)\n",
    "\n",
    "    # Strip empty lines and remove their correspondences\n",
    "    \"\"\"\n",
    "     Empty lines or lines that do not contain any meaningful content should be removed because they do not provide valuable information for training the model. It is also important to remove the corresponding line in the other language to maintain alignment.\n",
    "    \"\"\"\n",
    "    mask = (data_en['text'].str.strip().astype(bool) & data_fr['text'].str.strip().astype(bool))\n",
    "    data_en = data_en[mask]\n",
    "    data_fr = data_fr[mask]\n",
    "\n",
    "    return data_en, data_fr\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Steps Not Chosen and Why:\n",
    "\n",
    "- Removing Numbers or Special Characters: Not chosen because numbers and certain punctuation can carry semantic weight in sentences, which can be important for translations, such as dates, quantities, or formatted text.\n",
    "- Stemming/Lemmatization: Not typically used in machine translation preprocessing because retaining the full form of words is important for accurate translation, especially between languages with different linguistic structures.\n",
    "- Removing Stopwords: Not recommended for translation tasks because stopwords (common words like “and”, “the”, etc.) are crucial for maintaining the grammatical structure of the sentence in both source and target languages.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "preprocessed_en, preprocessed_fr  = preprocess_data(english_data,french_data)\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a5029cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:34:56.323703Z",
     "start_time": "2024-07-27T08:34:56.153347Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Remove all characters that are defined as noise from both translations\n",
    "\"\"\"\n",
    "def remove_noisy_characters(data):\n",
    "    # Define the characters to remove\n",
    "    noisy_characters = re.escape('@#$%^&*~<>|\\\\{}[]+=_/')\n",
    "    \n",
    "    # Regex to match any noisy character\n",
    "    regex_pattern = f'[{noisy_characters}]'\n",
    "    \n",
    "    # Remove noisy characters using regex substitution\n",
    "    data['text'] = data['text'].apply(lambda x: re.sub(regex_pattern, '', x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "preprocessed_en = remove_noisy_characters(english_data)\n",
    "preprocessed_fr = remove_noisy_characters(french_data)\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "1675930d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:34:58.684807Z",
     "start_time": "2024-07-27T08:34:58.681342Z"
    }
   },
   "source": [
    "# TODO FIX BUG\n",
    "\n",
    "\"\"\"\n",
    "remove special characterers only if a special character appears in one translation but not in the other and vice versa\n",
    "\"\"\"\n",
    "\n",
    "def synchronize_special_characters(data_en, data_fr):\n",
    "    counter = 0\n",
    "    # Define the characters to synchronize\n",
    "    special_characters = re.escape('@#$%^&*~<>|\\\\{}[]+=_/')\n",
    "    \n",
    "    # Regex to match any special character\n",
    "    regex_pattern = f'[{special_characters}]'\n",
    "\n",
    "    # Process each sentence pair\n",
    "    for idx in range(len(data_en)):\n",
    "        if idx >= len(data_fr):  # Ensure index is within the bounds for both dataframes\n",
    "            break\n",
    "        \n",
    "        # Extract texts from both dataframes\n",
    "        text_en = data_en.loc[idx, 'text']\n",
    "        text_fr = data_fr.loc[idx, 'text']\n",
    "\n",
    "        # Find special characters in both texts\n",
    "        found_chars_en = set(re.findall(regex_pattern, text_en))\n",
    "        found_chars_fr = set(re.findall(regex_pattern, text_fr))\n",
    "\n",
    "        # Determine characters to remove (those not in both)\n",
    "        chars_to_remove = found_chars_en.symmetric_difference(found_chars_fr)\n",
    "\n",
    "        # Remove the special characters that do not appear in both translations\n",
    "        if chars_to_remove:\n",
    "            counter += 1\n",
    "            remove_regex = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "            data_en.loc[idx, 'text'] = re.sub(remove_regex, '', text_en)\n",
    "            data_fr.loc[idx, 'text'] = re.sub(remove_regex, '', text_fr)\n",
    "    print(counter)\n",
    "    return data_en, data_fr\n",
    "\n",
    "#data_en_sync, data_fr_sync = synchronize_special_characters(data_en, data_fr)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "835596ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:01.536609Z",
     "start_time": "2024-07-27T08:35:01.534677Z"
    }
   },
   "source": [
    "#######TRAINING without pretrained Embeddings############# "
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "be12ee0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:02.471646Z",
     "start_time": "2024-07-27T08:35:02.467171Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Model Architecture:\n",
    "Encoder: An RNN (LSTM or GRU) that processes the input sentence and encodes it into a context vector.\n",
    "Decoder: Another RNN that takes the context vector and generates the output sentence in the target language.\n",
    "\"\"\"\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "941d48f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:03.148882Z",
     "start_time": "2024-07-27T08:35:03.146397Z"
    }
   },
   "source": [
    "#Create Embeddings manually \n",
    "# Tokenization function\n",
    "def tokenize(sentences):\n",
    "    return [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Building vocabulary\n",
    "def build_vocab_1(tokenized_texts):\n",
    "    vocab = defaultdict()\n",
    "    vocab.default_factory = lambda: len(vocab)\n",
    "    for tokens in tokenized_texts:\n",
    "        for token in tokens:\n",
    "            vocab[token]\n",
    "    return vocab\n",
    "\n",
    "# Convert texts to numerical indices\n",
    "def numericalize(texts, vocab):\n",
    "    return [[vocab[token] for token in tokens] for tokens in tokenize(texts)]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "f30c0ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:03.861156Z",
     "start_time": "2024-07-27T08:35:03.858345Z"
    }
   },
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    # Decoder initialization without an SOS token\n",
    "    # The first input to the decoder could be an assumed blank token, often just zeros\n",
    "    decoder_input = torch.zeros((1, 1), dtype=torch.long)  # Assuming your vocab is zero-indexed\n",
    "    decoder_hidden = encoder_hidden  # Use the last hidden state from the encoder to start the decoder\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # Use model's own prediction as next input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        # Check if decoder has generated the end of sequence, often by target length or a special condition\n",
    "        if di == target_length - 1:  # Simple condition assuming reaching the end of target tensor\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "9014b084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:04.446546Z",
     "start_time": "2024-07-27T08:35:04.445279Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "174364b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:04.985019Z",
     "start_time": "2024-07-27T08:35:04.981813Z"
    }
   },
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f'Iteration: {iter} Average Loss: {print_loss_avg:.4f}')"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "adbb89e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:05.720783Z",
     "start_time": "2024-07-27T08:35:05.719485Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "052693ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:19.583954Z",
     "start_time": "2024-07-27T08:35:06.215074Z"
    }
   },
   "source": [
    "#TODO takes to much time..\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_en, data_fr, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "# init decoder and encoder and loss function \n",
    "\n",
    "input_size = vocab_size_en   # size of the English vocabulary was calculated in task 1\n",
    "output_size = vocab_size_fr  # size of the French vocabulary was calculated in task 1\n",
    "hidden_size = 256            # typically a power of 2 like 256 or 512\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "data = {\n",
    "    'X_train': X_train.tolist(),\n",
    "    'y_train': y_train.tolist()\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "#create Tensors\n",
    "\n",
    "# Tokenize and build vocabularies\n",
    "tokenized_x = tokenize(df['X_train'])\n",
    "tokenized_y = tokenize(df['y_train'])\n",
    "\n",
    "#{token(in this case a word):number} ex. 'watts': 4239\n",
    "vocab_x = build_vocab_1(tokenized_x)\n",
    "vocab_y = build_vocab_1(tokenized_y)\n",
    "\n",
    "# Numericalize the data\n",
    "numericalized_x = numericalize(df['X_train'], vocab_x)\n",
    "numericalized_y = numericalize(df['y_train'], vocab_y)\n",
    "\n",
    "# Convert lists to PyTorch tensors and pad sequences\n",
    "input_tensor = pad_sequence([torch.tensor(seq) for seq in numericalized_x], batch_first=True, padding_value=0)\n",
    "target_tensor = pad_sequence([torch.tensor(seq) for seq in numericalized_y], batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "num_iterations = 10000\n",
    "print_every = 1000\n",
    "\n",
    "\"\"\"for iter in range(1, num_iterations + 1):\n",
    "    # Randomly selecting an example each time (for simplicity in this example)\n",
    "    # Ideally, use a more systematic way to create batches and cycle through data\n",
    "    input_example = random.choice(input_tensor)\n",
    "    target_example = random.choice(target_tensor)\n",
    "    loss = train(input_example, target_example, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, 1000)\n",
    "    if iter % print_every == 0:\n",
    "        print(f'Iter: {iter}, Loss: {loss:.4f}')\"\"\"\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for iter in range(1, num_iterations + 1):\\n    # Randomly selecting an example each time (for simplicity in this example)\\n    # Ideally, use a more systematic way to create batches and cycle through data\\n    input_example = random.choice(input_tensor)\\n    target_example = random.choice(target_tensor)\\n    loss = train(input_example, target_example, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, 1000)\\n    if iter % print_every == 0:\\n        print(f'Iter: {iter}, Loss: {loss:.4f}')\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "159d91f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:21.662605Z",
     "start_time": "2024-07-27T08:35:21.660730Z"
    }
   },
   "source": [
    "####### TODO TRAIN MODEL WITH SOS AND EOS TOKEN #########"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "4fe5e804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:21.960602Z",
     "start_time": "2024-07-27T08:35:21.958254Z"
    }
   },
   "source": [
    "#TODO add SOS and EOS tokens \n",
    "\n",
    "SOS_token = 0  # Assuming 0 is the index for SOS token in your vocabulary\n",
    "EOS_token = 1  # Assuming 1 is the index for EOS token in your vocabulary\n",
    "\n",
    "def add_special_tokens(sentences, sos_token, eos_token=None):\n",
    "    # Add an SOS token at the beginning of each sentence.\n",
    "    # Optionally, add an EOS token at the end.\n",
    "    updated_sentences = []\n",
    "    for sentence in sentences:\n",
    "        modified_sentence = [sos_token] + sentence\n",
    "        if eos_token is not None:\n",
    "            modified_sentence += [eos_token]\n",
    "        updated_sentences.append(modified_sentence)\n",
    "    return updated_sentences\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "1bf21853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:22.687661Z",
     "start_time": "2024-07-27T08:35:22.685983Z"
    }
   },
   "source": [
    "########################## USE DIFFERENT EMBEDDING MODELS #################\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "929a107a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:23.842044Z",
     "start_time": "2024-07-27T08:35:23.835782Z"
    }
   },
   "source": [
    "\n",
    "def load_embeddings_and_create_index(path):\n",
    "    word_to_idx = {}\n",
    "    idx = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1\n",
    "    return word_to_idx\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Vocabulary mapping\n",
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.token_to_idx:\n",
    "            idx = len(self.token_to_idx)\n",
    "            self.token_to_idx[word] = idx\n",
    "            self.idx_to_token[idx] = word\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        return self.token_to_idx.get(word, self.token_to_idx['<unk>'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "# Build vocabularies for both languages\n",
    "def build_vocab(sentences, existing_embeddings_word2idx):\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')  # Assuming you handle padding explicitly\n",
    "    vocab.add_word('<unk>')  # Handle unknown words\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token in existing_embeddings_word2idx:\n",
    "                vocab.add_word(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Dataset preparation\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences, source_vocab, target_vocab):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source_sentence = [self.source_vocab(token) if token in self.source_vocab.token_to_idx else self.source_vocab('<unk>') for token in tokenize(self.source_sentences.iloc[index])]\n",
    "        target_sentence = [self.target_vocab(token) if token in self.target_vocab.token_to_idx else self.target_vocab('<unk>') for token in tokenize(self.target_sentences.iloc[index])]\n",
    "        return torch.tensor(source_sentence, dtype=torch.long), torch.tensor(target_sentence, dtype=torch.long)\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    source_batch, target_batch = zip(*batch)\n",
    "    source_batch_padded = pad_sequence(source_batch, padding_value=vocab_en('<pad>'), batch_first=True)\n",
    "    target_batch_padded = pad_sequence(target_batch, padding_value=vocab_fr('<pad>'), batch_first=True)\n",
    "    return source_batch_padded, target_batch_padded\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "cafce509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:24.867024Z",
     "start_time": "2024-07-27T08:35:24.863684Z"
    }
   },
   "source": [
    "# Example GloVe embedding file path and embedding dimension\n",
    "\n",
    "def load_glove_embeddings(path: str, word2idx: Dict[str, int], embedding_dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a specified file and align them with the given word index dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The file path to the GloVe embeddings file.\n",
    "    - word2idx (Dict[str, int]): A dictionary mapping words to their corresponding indices. This dictionary defines\n",
    "      the position each word’s vector should occupy in the resulting embedding matrix.\n",
    "    - embedding_dim (int): The dimensionality of the GloVe vectors (e.g., 50, 100, 200, 300).\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor of shape (len(word2idx), embedding_dim) containing the GloVe vectors aligned according to word2idx.\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # Initialize the embedding matrix with zeros\n",
    "        embeddings = np.zeros((len(word2idx), embedding_dim))\n",
    "        \n",
    "        # Process each line in the GloVe file\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            \n",
    "            # If the word is in the provided dictionary, update the corresponding row in embeddings\n",
    "            if word in word2idx:\n",
    "                # Convert embedding values from strings to float32\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                # Place the vector in the correct index as per word2idx\n",
    "                embeddings[word2idx[word]] = vector\n",
    "    \n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    return torch.from_numpy(embeddings)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "c09042f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:25.503191Z",
     "start_time": "2024-07-27T08:35:25.498923Z"
    }
   },
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the Encoder with pre-trained embeddings and a GRU layer.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "            pretrained_embeddings (torch.Tensor): A tensor containing the pre-trained word embeddings.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # Ensure that the pretrained embeddings are of type float32\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        embed_size = pretrained_embeddings.shape[1]  # Embedding size is the second dimension of the embeddings tensor\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True).float()  # Ensure GRU is initialized as float32\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder which processes the input sequence.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input sequence tensor, which should be indexed by batch.\n",
    "\n",
    "        Returns:\n",
    "            hidden (torch.Tensor): The hidden state of the GRU, representing the encoded information of the input.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x).float()  # Ensure embedding outputs float32\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, pretrained_embeddings):\n",
    "        \"\"\"\n",
    "        Initialize the Decoder with pre-trained embeddings, a GRU layer, and a linear output layer.\n",
    "\n",
    "        Parameters:\n",
    "            embed_size (int): The size of each embedding vector.\n",
    "            hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            pretrained_embeddings (torch.Tensor): A tensor containing the pre-trained word embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        # Ensure that the pretrained embeddings are of type float32\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True).float()  # Ensure GRU is initialized as float32\n",
    "        self.fc = nn.Linear(hidden_size, output_size).float()  # Ensure Linear is initialized as float32\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder that processes one timestep of the sequence.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input tensor for the current timestep.\n",
    "            hidden (torch.Tensor): The hidden state from the last timestep.\n",
    "\n",
    "        Returns:\n",
    "            predicted (torch.Tensor): The output logits for the next word in the sequence.\n",
    "            hidden (torch.Tensor): The updated hidden state.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x).float()  # Ensure embedding outputs float32\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        predicted = self.fc(output)\n",
    "        return predicted, hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "        \n",
    "    def __init__(self, encoder, decoder):\n",
    "    \n",
    "        \"\"\"\n",
    "        Initialize the sequence-to-sequence model which contains an encoder and a decoder.\n",
    "\n",
    "        Parameters:\n",
    "            encoder (Encoder): The encoder part of the Seq2Seq model.\n",
    "            decoder (Decoder): The decoder part of the Seq2Seq model.\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        \"\"\"\n",
    "        Forward pass of the Seq2Seq model which processes the entire input and target sequence.\n",
    "\n",
    "        Parameters:\n",
    "            src (torch.Tensor): The input sequence tensor.\n",
    "            trg (torch.Tensor): The target sequence tensor used during training.\n",
    "\n",
    "        Returns:\n",
    "            outputs (torch.Tensor): The output from the decoder for each step in the sequence.\n",
    "        \"\"\"\n",
    "        hidden = self.encoder(src)\n",
    "        outputs, _ = self.decoder(trg, hidden)\n",
    "        return outputs\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "32199614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:26.047093Z",
     "start_time": "2024-07-27T08:35:26.044107Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, criterion, epochs=10, device=device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for src, trg in loader:\n",
    "            # Move tensors to the correct device and ensure they are long type for indexing operations\n",
    "            src = src.to(device).long()  # Correct type for embedding layer\n",
    "            trg = trg.to(device).long()  # Correct type for embedding layer\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: The decoder's input is all except the last word\n",
    "            output = model(src, trg[:, :-1])  \n",
    "            \n",
    "            # Since output will be in float (from linear layers, and GRU output), ensure it's float32 if not already\n",
    "            output = output.float()\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)  # Target doesn't include the first <sos> token\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}')\n",
    "        \n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "40c6f773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:39:12.641783Z",
     "start_time": "2024-07-27T08:37:38.022975Z"
    }
   },
   "source": [
    "data_en = english_data[\"text\"]\n",
    "data_fr = french_data[\"text\"]\n",
    "\n",
    "word_to_idx_en = load_embeddings_and_create_index('glove.6B/glove.6B.100d.txt')\n",
    "word_to_idx_fr = load_embeddings_and_create_index('glove.6B/glove.6B.100d.txt')\n",
    "vocab_en = build_vocab(data_en, word_to_idx_en)\n",
    "vocab_fr = build_vocab(data_fr, word_to_idx_fr)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_en, data_fr, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "data_en = pd.DataFrame({'sentence': X_train.tolist()}) \n",
    "data_fr = pd.DataFrame({'sentence': y_train.tolist()})\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "embedding_dim =100\n",
    "\n",
    "\n",
    "data = {\n",
    "    'X_train': X_train.tolist(),\n",
    "    'y_train': y_train.tolist()\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['tokenized_x'] = df['X_train'].apply(tokenize)\n",
    "df['tokenized_y'] = df['y_train'].apply(tokenize)\n",
    "\n",
    "#{token(in this case a word):number} ex. 'watts': 4239\n",
    "vocab_x = build_vocab_1(tokenized_x)\n",
    "vocab_y = build_vocab_1(tokenized_y)\n",
    "\n",
    "\n",
    "\n",
    "# Load embeddings\n",
    "glove_embeddings_en = load_glove_embeddings('glove.6B/glove.6B.100d.txt', vocab_x, embedding_dim)\n",
    "glove_embeddings_fr = load_glove_embeddings('glove.6B/glove.6B.100d.txt', vocab_x, embedding_dim)\n",
    "\n",
    "# Model instantiation\n",
    "encoder = Encoder(hidden_size=hidden_size, pretrained_embeddings=glove_embeddings_en)\n",
    "decoder = Decoder(embed_size=embedding_dim, hidden_size=hidden_size, output_size=len(vocab_fr), pretrained_embeddings=glove_embeddings_fr)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss(ignore_index=vocab_fr.token_to_idx['<pad>']).to(device)  # Move the loss function to the device\n",
    "\n",
    "\n",
    "\n",
    "word_to_idx_en = load_embeddings_and_create_index('glove.6B/glove.6B.100d.txt')\n",
    "word_to_idx_fr = load_embeddings_and_create_index('glove.6B/glove.6B.100d.txt')\n",
    "\n",
    "vocab_en = build_vocab(data_en['sentence'], word_to_idx_en)\n",
    "vocab_fr = build_vocab(data_fr['sentence'], word_to_idx_fr)\n",
    "\n",
    "dataset = TranslationDataset(data_en['sentence'], data_fr['sentence'], vocab_en, vocab_fr)\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "train(model, loader, optimizer, criterion, epochs=10, device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 63\u001B[0m\n\u001B[1;32m     60\u001B[0m dataset \u001B[38;5;241m=\u001B[39m TranslationDataset(data_en[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m], data_fr[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m], vocab_en, vocab_fr)\n\u001B[1;32m     61\u001B[0m loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, collate_fn\u001B[38;5;241m=\u001B[39mcollate_fn)\n\u001B[0;32m---> 63\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[32], line 27\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, loader, optimizer, criterion, epochs, device)\u001B[0m\n\u001B[1;32m     24\u001B[0m trg \u001B[38;5;241m=\u001B[39m trg[:, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Target doesn't include the first <sos> token\u001B[39;00m\n\u001B[1;32m     26\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, trg)\n\u001B[0;32m---> 27\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     30\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b13af837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocabulary at 0x350c2a910>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936c567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
