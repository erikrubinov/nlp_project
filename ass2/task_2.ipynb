{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-27T08:56:56.849050Z",
     "start_time": "2024-07-27T08:56:56.615711Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from csv import QUOTE_NONE\n",
    "import re\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:56:57.822406Z",
     "start_time": "2024-07-27T08:56:57.820044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the paths to your files\n",
    "english_file_path = \"fr-en/europarl-v7.fr-en.en\"\n",
    "french_file_path = \"fr-en/europarl-v7.fr-en.fr\""
   ],
   "id": "d751376ae07924dc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:57:06.426818Z",
     "start_time": "2024-07-27T08:56:58.540830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\" Function to load data into a pandas DataFrame without treating any character as quotes\"\"\"\n",
    "\n",
    "def load_data_to_dataframe(file_path):\n",
    "    # Read the entire file as a single column DataFrame, ignoring any quoting\n",
    "    return pd.read_csv(file_path, header=None, names=['text'], encoding='utf-8', sep='\\t', quoting=QUOTE_NONE, engine='python')\n",
    "\n",
    "# Load the data\n",
    "english_data = load_data_to_dataframe(english_file_path)\n",
    "french_data = load_data_to_dataframe(french_file_path)"
   ],
   "id": "b8d80593b953309f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:57:08.164858Z",
     "start_time": "2024-07-27T08:57:08.009648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Take 10% fraction of data randomly sampled\n",
    "english_data = english_data.sample(frac=0.1, random_state=42)\n",
    "french_data = french_data.sample(frac=0.1, random_state=42)\n"
   ],
   "id": "52ba97bbe4d4085",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:57:09.267663Z",
     "start_time": "2024-07-27T08:57:08.788889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(data_en, data_fr):\n",
    "    # Lowercase the text\n",
    "    \"\"\"\n",
    "    Normalizing case helps reduce the complexity of the language model by treating words like “The” and “the” as the same word, which can be particularly helpful in languages like English where capitalization is more stylistic than semantic.\n",
    "    \"\"\"\n",
    "    data_en['text'] = data_en['text'].str.lower()\n",
    "    data_fr['text'] = data_fr['text'].str.lower()\n",
    "\n",
    "    # Remove XML tags\n",
    "    \"\"\"\n",
    "    Lines containing XML-tags are likely not actual conversational or formal text but rather formatting or metadata which is irrelevant for translation purposes\n",
    "    \"\"\"\n",
    "    data_en['text'] = data_en['text'].apply(lambda x: '' if x.strip().startswith('<') else x)\n",
    "    data_fr['text'] = data_fr['text'].apply(lambda x: '' if x.strip().startswith('<') else x)\n",
    "\n",
    "    # Strip empty lines and remove their correspondences\n",
    "    \"\"\"\n",
    "     Empty lines or lines that do not contain any meaningful content should be removed because they do not provide valuable information for training the model. It is also important to remove the corresponding line in the other language to maintain alignment.\n",
    "    \"\"\"\n",
    "    mask = (data_en['text'].str.strip().astype(bool) & data_fr['text'].str.strip().astype(bool))\n",
    "    data_en = data_en[mask]\n",
    "    data_fr = data_fr[mask]\n",
    "\n",
    "    return data_en, data_fr\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Steps Not Chosen and Why:\n",
    "\n",
    "- Removing Numbers or Special Characters: Not chosen because numbers and certain punctuation can carry semantic weight in sentences, which can be important for translations, such as dates, quantities, or formatted text.\n",
    "- Stemming/Lemmatization: Not typically used in machine translation preprocessing because retaining the full form of words is important for accurate translation, especially between languages with different linguistic structures.\n",
    "- Removing Stopwords: Not recommended for translation tasks because stopwords (common words like “and”, “the”, etc.) are crucial for maintaining the grammatical structure of the sentence in both source and target languages.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "preprocessed_en, preprocessed_fr  = preprocess_data(english_data,french_data)\n"
   ],
   "id": "9a711754df6f6458",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/hr_8w4nd2nn8r21t2r09j11r0000gn/T/ipykernel_3714/986401010.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  data_en = data_en[mask]\n",
      "/var/folders/73/hr_8w4nd2nn8r21t2r09j11r0000gn/T/ipykernel_3714/986401010.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  data_fr = data_fr[mask]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:57:16.579106Z",
     "start_time": "2024-07-27T08:57:16.244614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Remove all characters that are defined as noise from both translations\n",
    "\"\"\"\n",
    "def remove_noisy_characters(data):\n",
    "    # Define the characters to remove\n",
    "    noisy_characters = re.escape('@#$%^&*~<>|\\\\{}[]+=_/')\n",
    "    \n",
    "    # Regex to match any noisy character\n",
    "    regex_pattern = f'[{noisy_characters}]'\n",
    "    \n",
    "    # Remove noisy characters using regex substitution\n",
    "    data['text'] = data['text'].apply(lambda x: re.sub(regex_pattern, '', x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "preprocessed_en = remove_noisy_characters(english_data)\n",
    "preprocessed_fr = remove_noisy_characters(french_data)\n"
   ],
   "id": "5c465f1f7e8acd36",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:57:19.075904Z",
     "start_time": "2024-07-27T08:57:19.071304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO FIX BUG\n",
    "\n",
    "\"\"\"\n",
    "remove special characterers only if a special character appears in one translation but not in the other and vice versa\n",
    "\"\"\"\n",
    "\n",
    "def synchronize_special_characters(data_en, data_fr):\n",
    "    counter = 0\n",
    "    # Define the characters to synchronize\n",
    "    special_characters = re.escape('@#$%^&*~<>|\\\\{}[]+=_/')\n",
    "    \n",
    "    # Regex to match any special character\n",
    "    regex_pattern = f'[{special_characters}]'\n",
    "\n",
    "    # Process each sentence pair\n",
    "    for idx in range(len(data_en)):\n",
    "        if idx >= len(data_fr):  # Ensure index is within the bounds for both dataframes\n",
    "            break\n",
    "        \n",
    "        # Extract texts from both dataframes\n",
    "        text_en = data_en.loc[idx, 'text']\n",
    "        text_fr = data_fr.loc[idx, 'text']\n",
    "\n",
    "        # Find special characters in both texts\n",
    "        found_chars_en = set(re.findall(regex_pattern, text_en))\n",
    "        found_chars_fr = set(re.findall(regex_pattern, text_fr))\n",
    "\n",
    "        # Determine characters to remove (those not in both)\n",
    "        chars_to_remove = found_chars_en.symmetric_difference(found_chars_fr)\n",
    "\n",
    "        # Remove the special characters that do not appear in both translations\n",
    "        if chars_to_remove:\n",
    "            counter += 1\n",
    "            remove_regex = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "            data_en.loc[idx, 'text'] = re.sub(remove_regex, '', text_en)\n",
    "            data_fr.loc[idx, 'text'] = re.sub(remove_regex, '', text_fr)\n",
    "    print(counter)\n",
    "    return data_en, data_fr\n",
    "\n",
    "#data_en_sync, data_fr_sync = synchronize_special_characters(data_en, data_fr)"
   ],
   "id": "30dc200b2a497791",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e5ae7cf53c260d9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
