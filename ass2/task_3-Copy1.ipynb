{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3b5d9883f748e844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:31:49.602734Z",
     "start_time": "2024-07-27T19:31:47.986253Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from preprocessing_task_2 import prepare_data\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c99c4a6b38254c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:31:59.611798Z",
     "start_time": "2024-07-27T19:31:49.603784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erikrubinov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/erikrubinov/anaconda3/bin/python3: No module named spacy\n",
      "/Users/erikrubinov/anaconda3/bin/python3: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!python3 -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "222dc7acd4ceed5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:32:08.026496Z",
     "start_time": "2024-07-27T19:31:59.613185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English DataFrame sample:\n",
      "                                                text\n",
      "0                          Resumption of the session\n",
      "1  I declare resumed the session of the European ...\n",
      "2  Although, as you will have seen, the dreaded '...\n",
      "3  You have requested a debate on this subject in...\n",
      "4  In the meantime, I should like to observe a mi...\n",
      "French DataFrame sample:\n",
      "                                                text\n",
      "0                              Reprise de la session\n",
      "1  Je déclare reprise la session du Parlement eur...\n",
      "2  Comme vous avez pu le constater, le grand \"bog...\n",
      "3  Vous avez souhaité un débat à ce sujet dans le...\n",
      "4  En attendant, je souhaiterais, comme un certai...\n"
     ]
    }
   ],
   "source": [
    "####### LOADING AND PREPROCESSING #############\n",
    "#english_data, french_data = prepare_data()\n",
    "\n",
    "\n",
    "\"\"\" Function to load data into a pandas DataFrame without treating any character as quotes\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from csv import QUOTE_NONE\n",
    "\n",
    "# Define the paths to your files\n",
    "english_file_path = \"fr-en/europarl-v7.fr-en.en\"\n",
    "french_file_path = \"fr-en/europarl-v7.fr-en.fr\"\n",
    "\n",
    "\n",
    "def load_data_to_dataframe(file_path):\n",
    "    # Read the entire file as a single column DataFrame, ignoring any quoting\n",
    "    return pd.read_csv(file_path, header=None, names=['text'], encoding='utf-8', sep='\\t', quoting=QUOTE_NONE, engine='python')\n",
    "\n",
    "# Load the data\n",
    "english_data = load_data_to_dataframe(english_file_path)\n",
    "french_data = load_data_to_dataframe(french_file_path)\n",
    "\n",
    "# Show the first few entries of the DataFrame\n",
    "print(\"English DataFrame sample:\")\n",
    "print(english_data.head())\n",
    "print(\"French DataFrame sample:\")\n",
    "print(french_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d025a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take fraction of data due to hardware constraints\n",
    "\n",
    "english_data = english_data[:100000]\n",
    "french_data = french_data[:100000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "61a7669965faa00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:32:10.099989Z",
     "start_time": "2024-07-27T19:32:08.027910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions to tokenize data and build vocab\n",
    "spacy_fr = spacy.load('fr_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def build_vocab(sentences, vocab_size, word_tokenize):\n",
    "    all_words = [word for sentence in sentences for word in word_tokenize(sentence)]\n",
    "    word_counts = Counter(all_words)\n",
    "    vocab = [word for word, _ in word_counts.most_common(vocab_size)]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab, start=4)}\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    word2idx['<sos>'] = 2\n",
    "    word2idx['<eos>'] = 3\n",
    "    return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb94199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4fe5e804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:32:10.105594Z",
     "start_time": "2024-07-27T19:32:10.103988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    source_batch, target_batch = zip(*batch)\n",
    "    source_batch_padded = pad_sequence(source_batch, padding_value=vocab_en['<pad>'], batch_first=True)\n",
    "    target_batch_padded = pad_sequence(target_batch, padding_value=vocab_fr['<pad>'], batch_first=True)\n",
    "    return source_batch_padded, target_batch_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14a700f37aee5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:32:10.107251Z",
     "start_time": "2024-07-27T19:32:10.106142Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a8faf5e32ad685b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:32:10.109022Z",
     "start_time": "2024-07-27T19:32:10.107819Z"
    }
   },
   "outputs": [],
   "source": [
    "########################## TODO: USE DIFFERENT EMBEDDING MODELS #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "929a107a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:32:10.114856Z",
     "start_time": "2024-07-27T19:32:10.112676Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dataset preparation\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences, source_vocab, target_vocab, tokenizer_source, tokenizer_target):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.tokenizer_source = tokenizer_source\n",
    "        self.tokenizer_target = tokenizer_target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source_sentence = [self.source_vocab[token] if token in self.source_vocab else self.source_vocab['<unk>'] for token in self.tokenizer_source(self.source_sentences.iloc[index])]\n",
    "        target_sentence = [self.target_vocab[token] if token in self.target_vocab else self.target_vocab['<unk>'] for token in self.tokenizer_target(self.target_sentences.iloc[index])]\n",
    "        return torch.tensor(source_sentence, dtype=torch.long), torch.tensor(target_sentence, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cafce509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T19:32:10.117597Z",
     "start_time": "2024-07-27T19:32:10.115422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example GloVe embedding file path and embedding dimension\n",
    "\n",
    "def load_glove_embeddings(path: str, word2idx: Dict[str, int], embedding_dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a specified file and align them with the given word index dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The file path to the GloVe embeddings file.\n",
    "    - word2idx (Dict[str, int]): A dictionary mapping words to their corresponding indices. This dictionary defines\n",
    "      the position each word’s vector should occupy in the resulting embedding matrix.\n",
    "    - embedding_dim (int): The dimensionality of the GloVe vectors (e.g., 50, 100, 200, 300).\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor of shape (len(word2idx), embedding_dim) containing the GloVe vectors aligned according to word2idx.\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # Initialize the embedding matrix with zeros\n",
    "        #embeddings = np.zeros((len(word2idx), embedding_dim))\n",
    "        #better approach: init with random \n",
    "        embeddings = np.random.uniform(-0.1, 0.1, (len(word2idx), embedding_dim))\n",
    "        # Process each line in the GloVe file\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            \n",
    "            # If the word is in the provided dictionary, update the corresponding row in embeddings\n",
    "            if word in word2idx.keys():\n",
    "                # Convert embedding values from strings to float32\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                # Place the vector in the correct index as per word2idx\n",
    "                embeddings[word2idx[word]] = vector\n",
    "            else:\n",
    "                pass\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    return torch.from_numpy(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def load_word2vec_embeddings(path, word2idx, embedding_dim):\n",
    "    embeddings = np.random.uniform(-0.1, 0.1, (len(word2idx), embedding_dim))\n",
    "    with open(path, 'r', encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            if word in word2idx:\n",
    "                try:\n",
    "                    vector = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings[word2idx[word]] = vector\n",
    "                except ValueError:\n",
    "                    print(f\"Error converting values for word: {word}\")\n",
    "                    continue\n",
    "    return torch.from_numpy(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c09042f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T20:44:48.576241Z",
     "start_time": "2024-07-27T20:44:48.569744Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the Encoder with pre-trained embeddings and a GRU layer.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "            pretrained_embeddings (torch.Tensor): A tensor containing the pre-trained word embeddings.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # Ensure that the pretrained embeddings are of type float32\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        embed_size = pretrained_embeddings.shape[1]  # Embedding size is the second dimension of the embeddings tensor\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True).float()  # Ensure GRU is initialized as float32\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder which processes the input sequence.\n",
    "\n",
    "        Parameters:\n",
    "            input (torch.Tensor): The input sequence tensor, which should be indexed by batch.\n",
    "\n",
    "        Returns:\n",
    "            hidden (torch.Tensor): The hidden state of the GRU, representing the encoded information of the input.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input).float()  # Ensure embedding outputs float32\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, pretrained_embeddings):\n",
    "        \"\"\"\n",
    "        Initialize the Decoder with pre-trained embeddings, a GRU layer, and a linear output layer.\n",
    "\n",
    "        Parameters:\n",
    "            embed_size (int): The size of each embedding vector.\n",
    "            hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            pretrained_embeddings (torch.Tensor): A tensor containing the pre-trained word embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        # Ensure that the pretrained embeddings are of type float32\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True).float()  # Ensure GRU is initialized as float32\n",
    "        self.fc = nn.Linear(hidden_size, output_size).float()  # Ensure Linear is initialized as float32\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder that processes one timestep of the sequence.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input tensor for the current timestep.\n",
    "            hidden (torch.Tensor): The hidden state from the last timestep.\n",
    "\n",
    "        Returns:\n",
    "            predicted (torch.Tensor): The output logits for the next word in the sequence.\n",
    "            hidden (torch.Tensor): The updated hidden state.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x).float()  # Ensure embedding outputs float32\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        predicted = self.fc(output)\n",
    "        return predicted, hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "\n",
    "        \"\"\"\n",
    "        Forward pass of the Seq2Seq model which processes the entire input and target sequence.\n",
    "\n",
    "        Parameters:\n",
    "            source (torch.Tensor): The input sequence tensor.\n",
    "            target (torch.Tensor): The target sequence tensor used during training.\n",
    "\n",
    "        Returns:\n",
    "            outputs (torch.Tensor): The output from the decoder for each step in the sequence.\n",
    "        \"\"\"\n",
    "        hidden = self.encoder(source)\n",
    "        outputs, _ = self.decoder(target, hidden)\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f94672db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings, num_layers=2, bidirectional=True):\n",
    "        super(Encoder2, self).__init__()  # Ensure the class name here matches the class being defined\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        embed_size = pretrained_embeddings.shape[1]\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers=num_layers, \n",
    "                          batch_first=True, bidirectional=bidirectional).float()\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input).float()\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = hidden.view(hidden.size(0)//2, 2, hidden.size(1), hidden.size(2))\n",
    "            hidden = torch.sum(hidden, dim=1)\n",
    "        return hidden\n",
    "\n",
    "class Decoder2(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, pretrained_embeddings, num_layers=2):\n",
    "        super(Decoder2, self).__init__()  # Correct use of super()\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers=num_layers, batch_first=True).float()\n",
    "        self.fc = nn.Linear(hidden_size, output_size).float()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x).float()\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        predicted = self.fc(output)\n",
    "        return predicted, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3f1cdd4a53b9ac02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T20:51:43.188623Z",
     "start_time": "2024-07-27T20:51:43.183695Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_1(model, sentence, source_vocab, target_vocab, tokenizer_source, tokenizer_target, max_length=50, device=\"cpu\"):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    tokens = tokenizer_source(sentence.lower())\n",
    "    # Convert tokens to indices\n",
    "    indices = [source_vocab.get(token, source_vocab['<unk>']) for token in tokens]\n",
    "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    # Pass through the encoder\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden = model.encoder(input_tensor)\n",
    "    # Initialize the decoder input and hidden state\n",
    "    decoder_input = torch.tensor([[target_vocab['<sos>']]], dtype=torch.long).to(device)\n",
    "    decoder_hidden = encoder_hidden#.unsqueeze(0)  # Add batch dimension back\n",
    "\n",
    "    # Generate the output sequence\n",
    "    output_tokens = []\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        next_token = topi.squeeze().item()\n",
    "\n",
    "        if next_token == target_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "        output_tokens.append(next_token)\n",
    "        decoder_input = topi.squeeze(0)#.detach()#.unsqueeze(0)\n",
    "\n",
    "    # Convert indices to tokens\n",
    "    output_sentence = [list(target_vocab.keys())[list(target_vocab.values()).index(idx)] for idx in output_tokens]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0e8b88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate_bleu(model, data_loader, source_vocab, target_vocab, tokenizer_source, tokenizer_target, device):\n",
    "    \"\"\"\n",
    "    Evaluate the BLEU score for a translation model.\n",
    "\n",
    "    Args:\n",
    "        model: The translation model (Seq2Seq).\n",
    "        data_loader: DataLoader for the dataset to evaluate.\n",
    "        source_vocab: Vocabulary for the source language.\n",
    "        target_vocab: Vocabulary for the target language.\n",
    "        tokenizer_source: Tokenization function for the source language.\n",
    "        tokenizer_target: Tokenization function for the target language.\n",
    "        device: Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        float: The BLEU score for the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for source, target in data_loader:\n",
    "            # Process each sentence in the batch\n",
    "            for i in range(source.size(0)):\n",
    "                source_sentence = source[i]\n",
    "                target_sentence = target[i]\n",
    "\n",
    "                # Convert source tensor to sentence\n",
    "                src_sent = ' '.join([list(source_vocab.keys())[list(source_vocab.values()).index(idx)] for idx in source_sentence if idx not in [source_vocab['<pad>'], source_vocab['<unk>'], source_vocab['<sos>'], source_vocab['<eos>']]])\n",
    "                # Generate translation\n",
    "                translation = predict_1(model, src_sent, source_vocab, target_vocab, tokenizer_source, tokenizer_target, device=device)\n",
    "                \n",
    "                # Convert target tensor to actual sentence\n",
    "                ref_sent = [list(target_vocab.keys())[list(target_vocab.values()).index(idx)] for idx in target_sentence if idx not in [target_vocab['<pad>'], target_vocab['<unk>'], target_vocab['<sos>'], target_vocab['<eos>']]]\n",
    "                \n",
    "                # Append to lists\n",
    "                hypotheses.append(translation.split())\n",
    "                references.append([ref_sent])\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    return corpus_bleu(references, hypotheses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "de671c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_rouge(model, data_loader, source_vocab, target_vocab, tokenizer_source, tokenizer_target, device):\n",
    "    \"\"\"\n",
    "    Evaluate the ROUGE score for a translation model.\n",
    "\n",
    "    Args:\n",
    "        model: The translation model (Seq2Seq).\n",
    "        data_loader: DataLoader for the dataset to evaluate.\n",
    "        source_vocab: Vocabulary for the source language.\n",
    "        target_vocab: Vocabulary for the target language.\n",
    "        tokenizer_source: Tokenization function for the source language.\n",
    "        tokenizer_target: Tokenization function for the target language.\n",
    "        device: Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rouge = Rouge()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for source, target in data_loader:\n",
    "            for i in range(source.size(0)):\n",
    "                source_sentence = source[i]\n",
    "                target_sentence = target[i]\n",
    "                # More readable version\n",
    "                src_sent = ' '.join(str(source_vocab.get(idx.item(), source_vocab['<unk>'])) for idx in source_sentence if idx not in [source_vocab['<pad>'], source_vocab['<unk>'], source_vocab['<sos>'], source_vocab['<eos>']])\n",
    "                translation = predict_1(model, src_sent, source_vocab, target_vocab, tokenizer_source, tokenizer_target, device=device)\n",
    "                ref_sent = ' '.join(str(target_vocab.get(idx.item(), target_vocab['<unk>'])) for idx in target_sentence if idx not in [target_vocab['<pad>'], target_vocab['<unk>'], target_vocab['<sos>'], target_vocab['<eos>']])                \n",
    "                hypotheses.append(translation)\n",
    "                references.append(ref_sent)\n",
    "\n",
    "    return rouge.get_scores(hypotheses, references, avg=True)\n",
    "                                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1dfd80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_4(model, train_loader,val_loader, optimizer, criterion, epochs=10, device=\"cpu\"):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for src, trg in train_loader:\n",
    "            src = src.to(device).long()\n",
    "            trg = trg.to(device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Ensure the first token for the decoder is <sos> which is trg[:,0]\n",
    "            # and the input to the decoder includes this <sos> token at the start\n",
    "            # up to before the <eos> token at the end.\n",
    "            decoder_input = trg[:, :-1]  # Excludes <eos> at the end\n",
    "            target_output = trg[:, 1:]  # Excludes <sos> at the start\n",
    "\n",
    "            # Pass the source and modified target to the model\n",
    "            output = model(src, decoder_input)\n",
    "            \n",
    "            output = output.float()  # Ensure the output is in float format\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # Flatten the output for computing the loss\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            target_output = target_output.contiguous().view(-1)\n",
    "\n",
    "            # Compute the loss; we don't need to handle <eos> explicitly here as it's managed by the target sequence setup\n",
    "            loss = criterion(output, target_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        bleu_score = evaluate_bleu(model, val_loader, vocab_en, vocab_fr, tokenize_en, tokenize_fr, device)\n",
    "        print(\"bleu_score:\",bleu_score)\n",
    "        rouge_score = evaluate_rouge(model, val_loader, vocab_en, vocab_fr, tokenize_en, tokenize_fr, device)\n",
    "        print(\"rouge_1_score:\",rouge_score[\"rouge-1\"][\"r\"])\n",
    "        print(predict_1(model, \"Hello, this is a test\", vocab_en, vocab_fr, tokenize_en, tokenize_fr, device=device))\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7b75f4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting values for word: communauté\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn import CrossEntropyLoss, Module\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Define your device based on the availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Define the vocabulary size and embedding dimension\n",
    "vocab_size = 10000\n",
    "\n",
    "\n",
    "# Build vocabularies based on the training set only\n",
    "X_train, X_test, y_train, y_test = train_test_split(english_data, french_data, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Building vocabularies using only the training data to prevent information leakage\n",
    "vocab_en = build_vocab(X_train[\"text\"], vocab_size, tokenize_en)\n",
    "vocab_fr = build_vocab(y_train[\"text\"], vocab_size, tokenize_fr)\n",
    "\n",
    "#Load embeddings glove\n",
    "#embedding_dim = 300\n",
    "#vocab_embeddings_en = load_glove_embeddings('glove.6B/glove.6B.300d.txt', vocab_en, embedding_dim)\n",
    "#vocab_embeddings_fr = load_glove_embeddings('fasttext/cc.fr.300.vec', vocab_fr, embedding_dim)\n",
    "\n",
    "#Load embeddings word2vec\n",
    "embedding_dim = 100\n",
    "vocab_embeddings_en = load_word2vec_embeddings('word2vec/english.txt', vocab_en, embedding_dim)\n",
    "vocab_embeddings_fr = load_word2vec_embeddings('word2vec/france.txt', vocab_fr, embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model components\n",
    "hidden_size = 1024\n",
    "#encoder = Encoder(hidden_size=hidden_size, pretrained_embeddings=vocab_embeddings_en)\n",
    "#decoder = Decoder(embed_size=embedding_dim, hidden_size=hidden_size, output_size=len(vocab_fr), pretrained_embeddings=vocab_embeddings_fr)\n",
    "encoder = Encoder2(hidden_size=hidden_size, pretrained_embeddings=vocab_embeddings_en)\n",
    "decoder = Decoder2(embed_size=embedding_dim, hidden_size=hidden_size, output_size=len(vocab_fr), pretrained_embeddings=vocab_embeddings_fr)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss(ignore_index=vocab_fr['<pad>']).to(device)\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = TranslationDataset(X_train['text'], y_train['text'], vocab_en, vocab_fr, tokenize_en, tokenize_fr)\n",
    "val_dataset = TranslationDataset(X_val['text'], y_val['text'], vocab_en, vocab_fr, tokenize_en, tokenize_fr)\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "train_4(model, train_loader, val_loader, optimizer, criterion, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936c567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T20:51:47.492197Z",
     "start_time": "2024-07-27T20:51:47.232176Z"
    }
   },
   "outputs": [],
   "source": [
    "print(predict_1(model, \"I am a student\", vocab_en, vocab_fr, tokenize_en, tokenize_fr, device=device))\n",
    "print(predict_2(model, \"I am a book\", vocab_en, vocab_fr, tokenize_en, tokenize_fr, device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6826af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bf900c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10004\n",
      "6906\n",
      "10004\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "# Example GloVe embedding file path and embedding dimension\n",
    "\n",
    "def check_missing_words_in_embeddings(path: str, word2idx: Dict[str, int], embedding_dim: int) -> torch.Tensor:\n",
    "    print(len(word2idx))\n",
    "    missing_word_counter = 0\n",
    "    \n",
    "    \n",
    "        \n",
    "    glove_words = set()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # Process each line in the GloVe file to collect all GloVe words\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            glove_words.add(word)\n",
    "    \n",
    "    for word in word2idx:\n",
    "        if word not in glove_words:\n",
    "            missing_word_counter+=1\n",
    "    \n",
    "\n",
    "    print(missing_word_counter)\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "\n",
    "check_missing_words_in_embeddings('glove.6B/glove.6B.100d.txt', vocab_fr, embedding_dim)\n",
    "\n",
    "check_missing_words_in_embeddings('fasttext/cc.fr.300.vec', vocab_fr, 300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1338acce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRelevant notes:\\nIf a word in your vocabulary (word2idx) doesn’t exist in the GloVe or FastText embeddings file, \\nits embedding vector would not be updated and would remain as initially set. \\nGiven that you initialize the embeddings matrix with zeros, \\nany word not found in the pre-trained dataset would be represented by a zero vector. \\n\\nupdate not found in vocubulary words are initialized with random vector\\n\\n\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Relevant notes:\n",
    "If a word in  the vocabulary (word2idx) doesn’t exist in the GloVe or FastText embeddings file, \n",
    "its embedding vector would not be updated and would remain as initially set by a zero vector. \n",
    "\n",
    "update:\n",
    "not found in vocubulary words are initialized with random vector\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c763cc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 687,  124,    7,  149,    4,  377, 1420,  195,    5,    7,  149,   13,\n",
       "          926,  688,   66,  689,    9,   13,   50,   11,  452,   30,    4,  322,\n",
       "            8]),\n",
       " tensor([1479,   53,  265,   14,    7,  943,   46,  944,    4,   35,   37,   76,\n",
       "          197,    9,  714,    4,   35,   21,  579,  103, 1480,   93,   11,  266,\n",
       "            6]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4338b0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:\n",
      " They need to see the political dimension working , to see that officials accept their responsibilities and that there is communication with the citizens .\n",
      "french:\n",
      " Ils ont besoin que la dimension politique fonctionne , qu' il y ait des responsabilités , qu' une communication soit établie avec les citoyens .\n"
     ]
    }
   ],
   "source": [
    "def get_key_by_value(dictionary, search_value):\n",
    "    for key, value in dictionary.items():\n",
    "        if value == search_value:\n",
    "            return key\n",
    "    return None  # If the value is not found, return None or raise an exception\n",
    "\n",
    "\n",
    "list_en= [ 687,  124,    7,  149,    4,  377, 1420,  195,    5,    7,  149,   13,\n",
    "          926,  688,   66,  689,    9,   13,   50,   11,  452,   30,    4,  322,\n",
    "            8]\n",
    "\n",
    "print(\"english:\")\n",
    "\n",
    "res= \"\"\n",
    "for i in list_en:\n",
    "    res= res + \" \"+  get_key_by_value(vocab_en, i)\n",
    "print(res)\n",
    "    \n",
    "\n",
    "##########\n",
    "\n",
    "\n",
    "list_fr= [1479,   53,  265,   14,    7,  943,   46,  944,    4,   35,   37,   76,\n",
    "          197,    9,  714,    4,   35,   21,  579,  103, 1480,   93,   11,  266,\n",
    "            6]\n",
    "\n",
    "print(\"french:\")\n",
    "\n",
    "res= \"\"\n",
    "for i in list_fr:\n",
    "    res= res + \" \"+  get_key_by_value(vocab_fr, i)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "75b107fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>There is still a need to tighten up in the are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>This is especially necessary in relation to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>As Liberals and Greens, we clearly have differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Mr President, Commissioner, there are just two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Are state aid to business or inter-company agr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0                            Resumption of the session\n",
       "1    I declare resumed the session of the European ...\n",
       "2    Although, as you will have seen, the dreaded '...\n",
       "3    You have requested a debate on this subject in...\n",
       "4    In the meantime, I should like to observe a mi...\n",
       "..                                                 ...\n",
       "995  There is still a need to tighten up in the are...\n",
       "996  This is especially necessary in relation to th...\n",
       "997  As Liberals and Greens, we clearly have differ...\n",
       "998  Mr President, Commissioner, there are just two...\n",
       "999  Are state aid to business or inter-company agr...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b702f6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reprise de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>En attendant, je souhaiterais, comme un certai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Cela se trouvait dans la communication du mois...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>N'allez pas croire qu'il y ait eu une quelconq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>Je pensais que ce dossier était bien plus avancé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>Je suis désolée.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>Ne croyez pas que la date a été choisie en fon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0                                  Reprise de la session\n",
       "1      Je déclare reprise la session du Parlement eur...\n",
       "2      Comme vous avez pu le constater, le grand \"bog...\n",
       "3      Vous avez souhaité un débat à ce sujet dans le...\n",
       "4      En attendant, je souhaiterais, comme un certai...\n",
       "...                                                  ...\n",
       "99995  Cela se trouvait dans la communication du mois...\n",
       "99996  N'allez pas croire qu'il y ait eu une quelconq...\n",
       "99997  Je pensais que ce dossier était bien plus avancé.\n",
       "99998                                   Je suis désolée.\n",
       "99999  Ne croyez pas que la date a été choisie en fon...\n",
       "\n",
       "[100000 rows x 1 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ab91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a31a9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(english_data, french_data, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25)  # 0.25 x 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "09aec1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Please rise, then, for this minute' s silence.\""
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"text\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2e5898fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je vous invite à vous lever pour cette minute de silence.'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[\"text\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e60273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47eeeca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Metal)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
