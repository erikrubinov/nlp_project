{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#TODO: delete this file",
   "id": "afc413aa066279c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T19:02:03.787536Z",
     "start_time": "2024-07-26T19:02:03.784554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, bigrams, trigrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ],
   "id": "b952e6edecb913dd",
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "9cdc1b92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:28:41.114572Z",
     "start_time": "2024-07-26T18:28:41.109748Z"
    }
   },
   "source": [
    "# Define the paths to your files\n",
    "english_file_path = \"fr-en/europarl-v7.fr-en.en\"\n",
    "french_file_path = \"fr-en/europarl-v7.fr-en.fr\"\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6f247c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:28:54.068865Z",
     "start_time": "2024-07-26T18:28:54.066746Z"
    }
   },
   "source": [
    "########### TASK 1###############"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f1b02805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:28:55.283035Z",
     "start_time": "2024-07-26T18:28:55.280985Z"
    }
   },
   "source": [
    "########LOAD DATA###############"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "45c377cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:29:31.793184Z",
     "start_time": "2024-07-26T18:29:30.832937Z"
    }
   },
   "source": [
    "\"\"\"Function to read a file and return its content. Saves line by line in a list\"\"\"\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.readlines()  # reads all lines into a list\n",
    "    return content\n",
    "\n",
    "# Load the data\n",
    "english_data = read_file(english_file_path)\n",
    "french_data = read_file(french_file_path)\n",
    "\n",
    "# Optionally, print the first few lines to check\n",
    "print(\"Sample English data:\", english_data[:5])\n",
    "print(\"Sample French data:\", french_data[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample English data: ['Resumption of the session\\n', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\n', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\n\", 'You have requested a debate on this subject in the course of the next few days, during this part-session.\\n', \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\n\"]\n",
      "Sample French data: ['Reprise de la session\\n', 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\\n', 'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\\n', 'Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\\n', \"En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\\n\"]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "0cc4625b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:29:40.526690Z",
     "start_time": "2024-07-26T18:29:31.794261Z"
    }
   },
   "source": [
    "\"\"\" Function to load data into a pandas DataFrame without treating any character as quotes\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from csv import QUOTE_NONE\n",
    "\n",
    "\n",
    "def load_data_to_dataframe(file_path):\n",
    "    # Read the entire file as a single column DataFrame, ignoring any quoting\n",
    "    return pd.read_csv(file_path, header=None, names=['text'], encoding='utf-8', sep='\\t', quoting=QUOTE_NONE, engine='python')\n",
    "\n",
    "# Load the data\n",
    "english_data = load_data_to_dataframe(english_file_path)\n",
    "french_data = load_data_to_dataframe(french_file_path)\n",
    "\n",
    "# Show the first few entries of the DataFrame\n",
    "print(\"English DataFrame sample:\")\n",
    "print(english_data.head())\n",
    "print(\"French DataFrame sample:\")\n",
    "print(french_data.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English DataFrame sample:\n",
      "                                                text\n",
      "0                          Resumption of the session\n",
      "1  I declare resumed the session of the European ...\n",
      "2  Although, as you will have seen, the dreaded '...\n",
      "3  You have requested a debate on this subject in...\n",
      "4  In the meantime, I should like to observe a mi...\n",
      "French DataFrame sample:\n",
      "                                                text\n",
      "0                              Reprise de la session\n",
      "1  Je déclare reprise la session du Parlement eur...\n",
      "2  Comme vous avez pu le constater, le grand \"bog...\n",
      "3  Vous avez souhaité un débat à ce sujet dans le...\n",
      "4  En attendant, je souhaiterais, comme un certai...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "f5afd61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:29:41.979790Z",
     "start_time": "2024-07-26T18:29:41.978015Z"
    }
   },
   "source": [
    "##############DATA ANALYSIS##################"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ca0cfa7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:29:42.762840Z",
     "start_time": "2024-07-26T18:29:42.761063Z"
    }
   },
   "source": [
    "## Take fraction of data due to hardware constraints\n",
    "\n",
    "english_data = english_data[:100000]\n",
    "french_data = french_data[:100000]\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "a225d41c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:29:56.337963Z",
     "start_time": "2024-07-26T18:29:43.692828Z"
    }
   },
   "source": [
    "# Vocabulary size\n",
    "\n",
    "\"\"\"\n",
    "comment: \n",
    "Instead of nltk use also spicy tokenizer:\n",
    "import spacy\n",
    "\n",
    "# Load the English tokenizer from spaCy\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example text\n",
    "text_en = \"Apple's looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# Process the text\n",
    "doc_en = nlp_en(text_en)\n",
    "\n",
    "# Extract tokens\n",
    "tokens_en = [token.text for token in doc_en]\n",
    "\n",
    "print(tokens_en)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_vocabulary(text_data):\n",
    "    # Tokenize the text\n",
    "    tokens = [word_tokenize(text.lower()) for text in text_data]\n",
    "    # Flatten the list of token lists into a single list of tokens\n",
    "    all_tokens = [token for sublist in tokens for token in sublist]\n",
    "    # Create a Counter object to count unique tokens\n",
    "    vocabulary = Counter(all_tokens)\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "data_en = english_data[\"text\"]\n",
    "data_fr = french_data[\"text\"]\n",
    "\n",
    "# Build vocabularies\n",
    "vocab_en = build_vocabulary(data_en)\n",
    "vocab_fr = build_vocabulary(data_fr)\n",
    "\n",
    "# Get the size of each vocabulary\n",
    "vocab_size_en = len(vocab_en)\n",
    "vocab_size_fr = len(vocab_fr)\n",
    "\n",
    "print(f\"English Vocabulary Size: {vocab_size_en}\")\n",
    "print(f\"French Vocabulary Size: {vocab_size_fr}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 32097\n",
      "French Vocabulary Size: 46435\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "21f1ee92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:30:47.731720Z",
     "start_time": "2024-07-26T18:30:47.493464Z"
    }
   },
   "source": [
    "# TODO average lenght difference between sentences in source and target language in words and letters\n",
    "\n",
    "def get_average_lenght_difference(source,target):\n",
    "    word_lenght_dif=[]\n",
    "    letter_lenght_dif= []\n",
    "    for sen_source, sen_target in zip(source[\"text\"],target[\"text\"]):\n",
    "        #print(sen_source)\n",
    "        letter_lenght_dif.append(abs(len(sen_source)- len(sen_target)))\n",
    "        word_lenght_dif.append(abs(len(sen_source.split())-len(sen_target.split())))\n",
    "    print(np.mean(letter_lenght_dif))\n",
    "    print(np.mean(word_lenght_dif))\n",
    "    #print(len(sen_source.split()))\n",
    "\n",
    "get_average_lenght_difference(english_data,french_data)\n",
    "\n",
    "#print(english_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.44385\n",
      "16.84479\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "23aa726c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:30:49.709213Z",
     "start_time": "2024-07-26T18:30:49.348935Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "def overlay_noisy_character_counts(data_en, data_fr):\n",
    "    # Define the characters to remove\n",
    "    noisy_characters = re.escape('@#$%^&*~<>|\\\\{}[]+=_/')\n",
    "    \n",
    "    # Regex to match any noisy character\n",
    "    regex_pattern = f'[{noisy_characters}]'\n",
    "    \n",
    "    # Function to count characters in a dataframe\n",
    "    def count_characters(data):\n",
    "        character_counts = Counter()\n",
    "        for text in data['text']:\n",
    "            found_chars = re.findall(regex_pattern, text)\n",
    "            character_counts.update(found_chars)\n",
    "        return character_counts\n",
    "\n",
    "    # Count characters for both datasets\n",
    "    en_counts = count_characters(data_en)\n",
    "    fr_counts = count_characters(data_fr)\n",
    "\n",
    "    # Plotting the occurrences for both English and French data in one overlaid histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    labels, en_values = zip(*en_counts.items())  # Unpack keys and values for English\n",
    "    _, fr_values = zip(*fr_counts.items())      # Unpack values for French\n",
    "    \n",
    "    # Ensure both languages have entries for each label\n",
    "    labels = list(set(labels + tuple(fr_counts.keys())))\n",
    "    en_values = [en_counts.get(label, 0) for label in labels]\n",
    "    fr_values = [fr_counts.get(label, 0) for label in labels]\n",
    "\n",
    "    # Plot\n",
    "    indices = np.arange(len(labels))  # the x locations for the groups\n",
    "    width = 0.35                     # the width of the bars\n",
    "    \n",
    "    plt.bar(indices - width/2, en_values, width, label='English', alpha=0.5)\n",
    "    plt.bar(indices + width/2, fr_values, width, label='French', alpha=0.5)\n",
    "\n",
    "    plt.ylabel('Occurrences')\n",
    "    plt.title('Comparison of Noisy Characters in English and French Text Data')\n",
    "    plt.xticks(indices, labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return en_counts, fr_counts\n",
    "\n",
    "en_noisy_character_counts, fr_noisy_character_counts = overlay_noisy_character_counts(english_data, french_data)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "c7bec1ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:31:11.592966Z",
     "start_time": "2024-07-26T18:30:51.875779Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Adding word count and sentence length columns\n",
    "english_data['word_count'] = english_data['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "french_data['word_count'] = french_data['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "english_data['sentence_length_in_letters'] = english_data['text'].apply(len)\n",
    "french_data['sentence_length_in_letters'] = french_data['text'].apply(len)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"English average word count:\", english_data['word_count'].mean())\n",
    "print(\"French average word count:\", french_data['word_count'].mean())\n",
    "print(\"English average sentence length in letters:\", english_data['sentence_length_in_letters'].mean())\n",
    "print(\"French average sentence length in letters:\", french_data['sentence_length_in_letters'].mean())\n",
    "\n",
    "# Histogram of sentence lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(english_data['sentence_length_in_letters'], bins=50, alpha=0.5, label='English')\n",
    "plt.hist(french_data['sentence_length_in_letters'], bins=50, alpha=0.5, label='French')\n",
    "plt.title('Distribution of Sentence Lengths in letters')\n",
    "plt.xlabel('Sentence Length in letters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Most common words\n",
    "top_n = 10\n",
    "english_words = Counter(word.lower() for sentence in english_data['text'] for word in word_tokenize(sentence))\n",
    "french_words = Counter(word.lower() for sentence in french_data['text'] for word in word_tokenize(sentence))\n",
    "\n",
    "print(\"Most common words in English:\")\n",
    "print(english_words.most_common(top_n))\n",
    "\n",
    "print(\"Most common words in French:\")\n",
    "print(french_words.most_common(top_n))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/niclasstoffregen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English average word count: 28.07578\n",
      "French average word count: 29.68897\n",
      "English average sentence length in letters: 150.65034\n",
      "French average sentence length in letters: 166.21101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/8AAAIjCAYAAABViau2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABt/0lEQVR4nO3deVwW5f7/8fcNCggKuLElKrmbuyWSuaNoZll2csstSvNg5prRYmadNM2tU2mdSjyVJ7XMOpoL7qloLqFluYZyTEDTAFdAmN8f/Zivd+AC3njD+Ho+Hvcj5prrvuYzM9zY+57NZhiGIQAAAAAAYFkuzi4AAAAAAAAULcI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/ANzmJk6cKJvNdkuW1a5dO7Vr186c3rBhg2w2m7744otbsvxBgwapevXqt2RZhXXu3Dk9+eSTCggIkM1m08iRI51dEoqxo0ePymaz6a233iqyZTj6b0RuzTExMQ4bEwBwfYR/ALCQmJgY2Ww28+Xh4aGgoCBFRETo7bff1tmzZx2ynBMnTmjixImKj493yHiOVJxruxFvvPGGYmJiNGzYMH3yySfq37//VftmZmZq9uzZatq0qby9veXr66u77rpLQ4YM0f79+4u0zgULFmjWrFlFuoxbqV27dmrQoIGzy7iqb7/9VhMnTnR2GU73888/a+LEiTp69Gieee+99x5fKADANRD+AcCCJk2apE8++URz5szRM888I0kaOXKkGjZsqL1799r1femll3Tx4sUCjX/ixAm9+uqrBQ7Yq1ev1urVqwv0noK6Vm3/+te/dODAgSJd/s1at26dWrZsqVdeeUWPP/64mjdvftW+PXv21JgxY9SgQQNNmTJFr776qtq0aaMVK1Zo27ZtRVqn1cJ/cfftt9/q1VdfdcqyC/M3oqj8/PPPevXVVwn/AFAIpZxdAADA8bp27aq7777bnI6Ojta6dev0wAMP6MEHH9Qvv/yiMmXKSJJKlSqlUqWK9p+DCxcuyNPTU25ubkW6nOspXbq0U5d/I06ePKn69etft9+OHTu0bNky/eMf/9ALL7xgN++dd95RampqEVWI282t+BtRXF2+fFk5OTlO/9sFAI7AkX8AuE106NBBL7/8so4dO6ZPP/3UbM/vet7Y2Fjdd9998vX1VdmyZVWnTh0zYG7YsEH33HOPJGnw4MHmJQa5R9xyT5/etWuX2rRpI09PT/O9f73mP1d2drZeeOEFBQQEyMvLSw8++KD+97//2fWpXr26Bg0alOe9V455vdryu+b//PnzGjNmjIKDg+Xu7q46derorbfekmEYdv1sNpuGDx+upUuXqkGDBnJ3d9ddd92llStX5r/B/+LkyZOKjIyUv7+/PDw81LhxY82fP9+cn3v/g4SEBC1fvtysPb8jnJJ05MgRSVKrVq3yzHN1dVXFihXt2n777Tc98cQT8vf3N2v/+OOP7frk1rBo0SL94x//UJUqVeTh4aGOHTvq8OHDZr927dpp+fLlOnbsmFnnlds1IyNDr7zyimrWrCl3d3cFBwfrueeeU0ZGRqG36W+//abIyEgFBQXJ3d1dISEhGjZsmDIzM80+qampGjlypLkva9asqTfffFM5OTn5bsPCWLFihVq3bi0vLy+VK1dO3bp10759++z6DBo0SGXLltVvv/2mHj16qGzZsqpcubLGjh2r7Oxsu76nT59W//79zcs2Bg4cqD179uT5vX333XfNbZb7+qsPPvhANWrUkLu7u+655x7t2LHDbn5ycrIGDx6sKlWqyN3dXYGBgXrooYeu+juWK7+/ETf7ecjP/v379eijj6pChQry8PDQ3XffrW+++cacHxMTo7/97W+SpPbt25vbYcOGDapevbr27dunjRs3mu1X/q25kd+NK++fMGvWLHNb/vzzz5Kkf/7zn7rrrrvk6emp8uXL6+6779aCBQsKvb4AcKvdnl/jAsBtqn///nrhhRe0evVqPfXUU/n22bdvnx544AE1atRIkyZNkru7uw4fPqwtW7ZIkurVq6dJkyZpwoQJGjJkiFq3bi1Juvfee80xTp8+ra5du6p37956/PHH5e/vf826/vGPf8hms2n8+PE6efKkZs2apfDwcMXHx5tnKNyIG6ntSoZh6MEHH9T69esVGRmpJk2aaNWqVRo3bpx+++03zZw5067/5s2btWTJEv39739XuXLl9Pbbb6tnz55KTEzME7avdPHiRbVr106HDx/W8OHDFRISosWLF2vQoEFKTU3Vs88+q3r16umTTz7RqFGjVKVKFY0ZM0aSVLly5XzHrFatmiTps88+U6tWra55ZDYlJUUtW7Y0A1vlypW1YsUKRUZGKj09Pc9NBadMmSIXFxeNHTtWaWlpmjp1qvr166ft27dLkl588UWlpaXp+PHj5jYqW7asJCknJ0cPPvigNm/erCFDhqhevXr68ccfNXPmTB08eFBLly4t8DY9ceKEWrRoodTUVA0ZMkR169bVb7/9pi+++EIXLlyQm5ubLly4oLZt2+q3337T0KFDVbVqVW3dulXR0dFKSkpyyCUKn3zyiQYOHKiIiAi9+eabunDhgubMmaP77rtPP/zwg90XINnZ2YqIiFBoaKjeeustrVmzRtOnT1eNGjU0bNgwc1t1795d33//vYYNG6a6devq66+/1sCBA+2WO3ToUJ04cUKxsbH65JNP8q1twYIFOnv2rIYOHSqbzaapU6fqkUce0a+//mqe8dKzZ0/t27dPzzzzjKpXr66TJ08qNjZWiYmJhboRZmE/D/nZt2+fWrVqpTvuuEPPP/+8vLy8tGjRIvXo0UNffvmlHn74YbVp00YjRozQ22+/rRdeeEH16tWT9OfnftasWXrmmWdUtmxZvfjii5Jk/t0p6O/GvHnzdOnSJQ0ZMkTu7u6qUKGC/vWvf2nEiBF69NFH9eyzz+rSpUvau3evtm/frr59+xZ42wGAUxgAAMuYN2+eIcnYsWPHVfv4+PgYTZs2NadfeeUV48p/DmbOnGlIMk6dOnXVMXbs2GFIMubNm5dnXtu2bQ1Jxty5c/Od17ZtW3N6/fr1hiTjjjvuMNLT0832RYsWGZKM2bNnm23VqlUzBg4ceN0xr1XbwIEDjWrVqpnTS5cuNSQZr7/+ul2/Rx991LDZbMbhw4fNNkmGm5ubXduePXsMScY///nPPMu60qxZswxJxqeffmq2ZWZmGmFhYUbZsmXt1r1atWpGt27drjmeYRhGTk6Oua39/f2NPn36GO+++65x7NixPH0jIyONwMBA4/fff7dr7927t+Hj42NcuHDBMIz/2x/16tUzMjIyzH6zZ882JBk//vij2datWze7bZnrk08+MVxcXIzvvvvOrn3u3LmGJGPLli1m241u0wEDBhguLi75/l7n5OQYhmEYr732muHl5WUcPHjQbv7zzz9vuLq6GomJiXnee6W2bdsad91111Xnnz171vD19TWeeuopu/bk5GTDx8fHrn3gwIGGJGPSpEl2fZs2bWo0b97cnP7yyy8NScasWbPMtuzsbKNDhw55foejoqKM/P63LSEhwZBkVKxY0Thz5ozZ/vXXXxuSjP/+97+GYRjGH3/8YUgypk2bds3tkJ+//o0wjJv7POTWfOX6dezY0WjYsKFx6dIlsy0nJ8e49957jVq1apltixcvNiQZ69evzzPuXXfdZfe3INeN/m7k1uXt7W2cPHnSru9DDz10zd8PACgJOO0fAG4zZcuWveZd/319fSVJX3/9daFPl3Z3d9fgwYNvuP+AAQNUrlw5c/rRRx9VYGCgvv3220It/0Z9++23cnV11YgRI+zax4wZI8MwtGLFCrv28PBw1ahRw5xu1KiRvL299euvv153OQEBAerTp4/ZVrp0aY0YMULnzp3Txo0bC1y7zWbTqlWr9Prrr6t8+fL6z3/+o6ioKFWrVk29evUyr/k3DENffvmlunfvLsMw9Pvvv5uviIgIpaWlaffu3XZjDx482O4a59wzKK63npK0ePFi1atXT3Xr1rVbVocOHSRJ69evt+t/vW2ak5OjpUuXqnv37nb3sbhyO+Qut3Xr1ipfvrzdcsPDw5Wdna1NmzZdt/ZriY2NVWpqqvr06WM3vqurq0JDQ/OslyQ9/fTTdtOtW7e224YrV65U6dKl7c7CcXFxUVRUVIHr69Wrl8qXL2+3LOn/9lmZMmXk5uamDRs26I8//ijw+Pkp7Ofhr86cOaN169bpscce09mzZ81te/r0aUVEROjQoUP67bffCl1nQX83evbsmeeMG19fXx0/fjzPpRQAUJJw2j8A3GbOnTsnPz+/q87v1auXPvzwQz355JN6/vnn1bFjRz3yyCN69NFH5eJyY98Z33HHHQW6QVatWrXspm02m2rWrHnda5Fv1rFjxxQUFGT3xYMk83TiY8eO2bVXrVo1zxjly5e/bpg6duyYatWqlWf7XW05N8rd3V0vvviiXnzxRSUlJWnjxo2aPXu2Fi1apNKlS+vTTz/VqVOnlJqaqg8++EAffPBBvuOcPHnSbvqv65kbKm8kNB46dEi//PLLVS9XuN6ycpeXu6xTp04pPT39uo/hO3TokPbu3XvDyy2oQ4cOSZL5JcZfeXt72017eHjkqeWvvyvHjh1TYGCgPD097frVrFmzwPVdb5+5u7vrzTff1JgxY+Tv76+WLVvqgQce0IABAxQQEFDg5eW3zNzlFvTLhcOHD8swDL388st6+eWX8+1z8uRJ3XHHHYWqs6C/GyEhIXn6jB8/XmvWrFGLFi1Us2ZNde7cWX379s33nhsAUFwR/gHgNnL8+HGlpaVdM1yUKVNGmzZt0vr167V8+XKtXLlSCxcuVIcOHbR69Wq5urpedzkFuU7/RuV3gzPpz2urb6QmR7jacoy/3BzQGQIDA9W7d2/17NlTd911lxYtWqSYmBjz7I3HH388z7XkuRo1amQ3fTPrmZOTo4YNG2rGjBn5zg8ODnbYsv663E6dOum5557Ld37t2rULNF5+40t/XvefX1j+6z0XbtXv5PWWd+V2HDlypLp3766lS5dq1apVevnllzV58mStW7dOTZs2LZJl3ojcbTt27FhFRETk26cwX4hcOX5Bfjfy+/tVr149HThwQMuWLdPKlSv15Zdf6r333tOECROc9ghGACgowj8A3EZybxZ2tf/BzuXi4qKOHTuqY8eOmjFjht544w29+OKLWr9+vcLDw68axAsr96hqLsMwdPjwYbtQWr58+XwfX3fs2DHdeeed5nRBaqtWrZrWrFmjs2fP2h39379/vznfEapVq6a9e/cqJyfH7ui/o5cj/Xk5QaNGjXTo0CH9/vvvqly5ssqVK6fs7GyFh4c7bDlX2841atTQnj171LFjR4f8nlSuXFne3t766aefrtmvRo0aOnfunEPX8a/jS5Kfn5/DllGtWjWtX7/efBRmriufrJDLUZ+5GjVqaMyYMRozZowOHTqkJk2aaPr06XZPALnVcj+/pUuXvu62vdZ2uNbvpCN+N7y8vNSrVy/16tVLmZmZeuSRR/SPf/xD0dHR8vDwuKmxAeBW4Jp/ALhNrFu3Tq+99ppCQkLUr1+/q/Y7c+ZMnrYmTZpIkvmoNi8vL0ly2LPk//3vf9vdh+CLL75QUlKSunbtarbVqFFD27Zts3u027Jly/I8ErAgtd1///3Kzs7WO++8Y9c+c+ZM2Ww2u+XfjPvvv1/JyclauHCh2Xb58mX985//VNmyZdW2bdsCj3no0CElJibmaU9NTVVcXJzKly+vypUry9XVVT179tSXX36Zb4A+depUgZct/bmd09LS8rQ/9thj+u233/Svf/0rz7yLFy/q/PnzBVqOi4uLevToof/+97/auXNnnvm5R5kfe+wxxcXFadWqVXn6pKam6vLlywVa7l9FRETI29tbb7zxhrKysvLML8x2jIiIUFZWlt22ysnJMR/rd6Wb/cxduHBBly5dsmurUaOGypUrl+cRjLean5+f2rVrp/fff19JSUl55l+5ba+1Hby8vPJtd8TvxunTp+2m3dzcVL9+fRmGke/vAwAURxz5BwALWrFihfbv36/Lly8rJSVF69atU2xsrKpVq6ZvvvnmmkepJk2apE2bNqlbt26qVq2aTp48qffee09VqlTRfffdJ+nP0ODr66u5c+eqXLly8vLyUmhoaL7Xyt6IChUq6L777tPgwYOVkpKiWbNmqWbNmnY3QnvyySf1xRdfqEuXLnrsscd05MgRffrpp3Y3HCtobd27d1f79u314osv6ujRo2rcuLFWr16tr7/+WiNHjswzdmENGTJE77//vgYNGqRdu3apevXq+uKLL7RlyxbNmjUrzz0HbsSePXvUt29fde3aVa1bt1aFChX022+/af78+Tpx4oRmzZplnpY9ZcoUrV+/XqGhoXrqqadUv359nTlzRrt379aaNWvy/cLnepo3b66FCxdq9OjRuueee1S2bFl1795d/fv316JFi/T0009r/fr1atWqlbKzs7V//34tWrRIq1atyvfGfdfyxhtvaPXq1Wrbtq35+MCkpCQtXrxYmzdvlq+vr8aNG6dvvvlGDzzwgAYNGqTmzZvr/Pnz+vHHH/XFF1/o6NGjqlSp0jWXc+rUKb3++ut52nO/MJszZ4769++vZs2aqXfv3qpcubISExO1fPlytWrVKs+XSNfTo0cPtWjRQmPGjNHhw4dVt25dffPNN+b+uPJIdvPmzSVJI0aMUEREhFxdXdW7d+8bXtbBgwfVsWNHPfbYY6pfv75KlSqlr776SikpKQUap6i8++67uu+++9SwYUM99dRTuvPOO5WSkqK4uDgdP35ce/bskfTnF5Gurq568803lZaWJnd3d3Xo0EF+fn5q3ry55syZo9dff101a9aUn5+fOnTo4JDfjc6dOysgIECtWrWSv7+/fvnlF73zzjvq1q1boT6/AOAUznnIAACgKOQ+6i/35ebmZgQEBBidOnUyZs+ebfdIuVx/fYzX2rVrjYceesgICgoy3NzcjKCgIKNPnz55HpP19ddfG/Xr1zdKlSpl99iuaz0y7WqP+vvPf/5jREdHG35+fkaZMmWMbt265fvIuunTpxt33HGH4e7ubrRq1crYuXNnnjGvVdtfH/VnGH8+wm3UqFFGUFCQUbp0aaNWrVrGtGnTzEfI5ZJkREVF5anpao8g/KuUlBRj8ODBRqVKlQw3NzejYcOG+T6O8EYf9ZeSkmJMmTLFaNu2rREYGGiUKlXKKF++vNGhQwfjiy++yLd/VFSUERwcbJQuXdoICAgwOnbsaHzwwQdmn9z9sXjxYrv35vdotnPnzhl9+/Y1fH19DUl22zUzM9N48803jbvuustwd3c3ypcvbzRv3tx49dVXjbS0NLNfQbbpsWPHjAEDBhiVK1c23N3djTvvvNOIioqyeyTh2bNnjejoaKNmzZqGm5ubUalSJePee+813nrrLSMzM/Oa2zP3sYn5vTp27Gi3jSIiIgwfHx/Dw8PDqFGjhjFo0CBj586dZp+BAwcaXl5eeZaR3yPzTp06ZfTt29coV66c4ePjYwwaNMjYsmWLIcn4/PPPzX6XL182nnnmGaNy5cqGzWYzx8ndN/k9wk+S8corrxiGYRi///67ERUVZdStW9fw8vIyfHx8jNDQUGPRokXX3C5Xq/tmPg/5/T4ZhmEcOXLEGDBggBEQEGCULl3auOOOO4wHHnggz+/zv/71L+POO+80XF1d7R77l5ycbHTr1s0oV66cIcnu78KN/G5ca1u+//77Rps2bYyKFSsa7u7uRo0aNYxx48bZ/T4DQHFnM4xicJciAAAASJKWLl2qhx9+WJs3b+Zu8gAAhyH8AwAAOMnFixft7i6fnZ2tzp07a+fOnUpOTi6SJ2cAAG5PXPMPAADgJM8884wuXryosLAwZWRkaMmSJdq6daveeOMNgj8AwKE48g8AAOAkCxYs0PTp03X48GFdunRJNWvW1LBhwzR8+HBnlwYAsBjCPwAAAAAAFufi7AIAAAAAAEDRIvwDAAAAAGBx3PDPQXJycnTixAmVK1dONpvN2eUAAAAAACzOMAydPXtWQUFBcnG59rF9wr+DnDhxQsHBwc4uAwAAAABwm/nf//6nKlWqXLMP4d9BypUrJ+nPje7t7e3kagAAAAAAVpeenq7g4GAzj14L4d9Bck/19/b2JvwDAAAAAG6ZG7n0nBv+AQAAAABgcYR/AAAAAAAsjvAPAAAAAIDFOTX8T548Wffcc4/KlSsnPz8/9ejRQwcOHLDrc+nSJUVFRalixYoqW7asevbsqZSUFLs+iYmJ6tatmzw9PeXn56dx48bp8uXLdn02bNigZs2ayd3dXTVr1lRMTEyeet59911Vr15dHh4eCg0N1ffff+/wdQYAAACAksAwDGVlZenSpUu8nPTKysqSYRgO2Z9OveHfxo0bFRUVpXvuuUeXL1/WCy+8oM6dO+vnn3+Wl5eXJGnUqFFavny5Fi9eLB8fHw0fPlyPPPKItmzZIknKzs5Wt27dFBAQoK1btyopKUkDBgxQ6dKl9cYbb0iSEhIS1K1bNz399NP67LPPtHbtWj355JMKDAxURESEJGnhwoUaPXq05s6dq9DQUM2aNUsRERE6cOCA/Pz8nLOBAAAAAMAJMjMzlZSUpAsXLji7lNuep6enAgMD5ebmdlPj2AxHfY3gAKdOnZKfn582btyoNm3aKC0tTZUrV9aCBQv06KOPSpL279+vevXqKS4uTi1bttSKFSv0wAMP6MSJE/L395ckzZ07V+PHj9epU6fk5uam8ePHa/ny5frpp5/MZfXu3VupqalauXKlJCk0NFT33HOP3nnnHUlSTk6OgoOD9cwzz+j555+/bu3p6eny8fFRWload/sHAAAAUGLl5OTo0KFDcnV1VeXKleXm5nZDd5OHYxmGoczMTJ06dUrZ2dmqVauWXFzsT94vSA4tVo/6S0tLkyRVqFBBkrRr1y5lZWUpPDzc7FO3bl1VrVrVDP9xcXFq2LChGfwlKSIiQsOGDdO+ffvUtGlTxcXF2Y2R22fkyJGS/vxWa9euXYqOjjbnu7i4KDw8XHFxcfnWmpGRoYyMDHM6PT395lYeAAAAAIqBzMxM82Cop6ens8u5rZUpU0alS5fWsWPHlJmZKQ8Pj0KPVWxu+JeTk6ORI0eqVatWatCggSQpOTlZbm5u8vX1tevr7++v5ORks8+VwT93fu68a/VJT0/XxYsX9fvvvys7OzvfPrlj/NXkyZPl4+NjvoKDgwu34gAAAABQDP31KDOcw1H7odjszaioKP3000/6/PPPnV3KDYmOjlZaWpr5+t///ufskgAAAAAAyFexOO1/+PDhWrZsmTZt2qQqVaqY7QEBAcrMzFRqaqrd0f+UlBQFBASYff56V/7cpwFc2eevTwhISUmRt7e3ypQpI1dXV7m6uubbJ3eMv3J3d5e7u3vhVhgAAAAAgFvIqeHfMAw988wz+uqrr7RhwwaFhITYzW/evLlKly6ttWvXqmfPnpKkAwcOKDExUWFhYZKksLAw/eMf/9DJkyfNu/LHxsbK29tb9evXN/t8++23dmPHxsaaY7i5ual58+Zau3atevToIenPyxDWrl2r4cOHF9n6AwAAAEBJMjP24C1d3qhOtW/p8m5ETEyMRo4cqdTUVEnSxIkTtXTpUsXHx1/3vQXp62hOPe0/KipKn376qRYsWKBy5copOTlZycnJunjxoiTJx8dHkZGRGj16tNavX69du3Zp8ODBCgsLU8uWLSVJnTt3Vv369dW/f3/t2bNHq1at0ksvvaSoqCjzyPzTTz+tX3/9Vc8995z279+v9957T4sWLdKoUaPMWkaPHq1//etfmj9/vn755RcNGzZM58+f1+DBg2/9hgEAAAAAFNigQYNks9nyvLp06VJkyxw7dqzWrl1bZOM7ilOP/M+ZM0eS1K5dO7v2efPmadCgQZKkmTNnysXFRT179lRGRoYiIiL03nvvmX1dXV21bNkyDRs2TGFhYfLy8tLAgQM1adIks09ISIiWL1+uUaNGafbs2apSpYo+/PBDRUREmH169eqlU6dOacKECUpOTlaTJk20cuXKPDcBBAAAAAAUX126dNG8efPs2oryku2yZcuqbNmyRTa+ozj1yL9hGPm+coO/JHl4eOjdd9/VmTNndP78eS1ZsiTPdfjVqlXTt99+qwsXLujUqVN66623VKqU/fca7dq10w8//KCMjAwdOXLEbhm5hg8frmPHjikjI0Pbt29XaGhoUaw2AAAAAKCIuLu7KyAgwO5Vvnx5SZLNZtOHH36ohx9+WJ6enqpVq5a++eYbu/d/8803qlWrljw8PNS+fXvNnz9fNpvNPM3/ryZOnKgmTZqY0xs2bFCLFi3k5eUlX19ftWrVSseOHbN7zyeffKLq1avLx8dHvXv31tmzZx26DfJTbO72DwAAAABAUXv11Vf12GOPae/evbr//vvVr18/nTlzRpKUkJCgRx99VD169NCePXs0dOhQvfjiizc89uXLl9WjRw+1bdtWe/fuVVxcnIYMGSKbzWb2OXLkiJYuXaply5Zp2bJl2rhxo6ZMmeLw9fwrwj8AAAAAwDKWLVtmnoqf+3rjjTfM+YMGDVKfPn1Us2ZNvfHGGzp37pz5BLn3339fderU0bRp01SnTh317t0737PGryY9PV1paWl64IEHVKNGDdWrV08DBw5U1apVzT45OTmKiYlRgwYN1Lp1a/Xv3/+W3DOgWDzqDwAAAAAAR2jfvr15f7lcFSpUMH9u1KiR+bOXl5e8vb118uRJSX8+Xe6ee+6xe2+LFi1ueNkVKlTQoEGDFBERoU6dOik8PFyPPfaYAgMDzT7Vq1dXuXLlzOnAwEBz+UWJI/8AAAAAAMvw8vJSzZo17V5Xhv/SpUvb9bfZbMrJyXHY8ufNm6e4uDjde++9WrhwoWrXrq1t27bdsuVfDeEfAAAAAABJderU0c6dO+3aduzYUeBxmjZtqujoaG3dulUNGjTQggULHFVioXHaP5xmZuzBmx5jVKfaDqgEAAAAgFVkZGQoOTnZrq1UqVKqVKnSdd87dOhQzZgxQ+PHj1dkZKTi4+MVExMjSXY37buahIQEffDBB3rwwQcVFBSkAwcO6NChQxowYECh1sWRCP8AAAAAgBtSEg6+rVy50u4ae+nPI/r79++/7ntDQkL0xRdfaMyYMZo9e7bCwsL04osvatiwYXJ3d7/u+z09PbV//37Nnz9fp0+fVmBgoKKiojR06NBCr4+j2AzDMJxdhBWkp6fLx8dHaWlp8vb2dnY5JQJH/gEAAIDi59KlS0pISFBISIg8PDycXY7T/eMf/9DcuXP1v//9zynLv9b+KEgO5cg/AAAAAAD/33vvvad77rlHFStW1JYtWzRt2jQNHz7c2WXdNMI/AAAAAAD/36FDh/T666/rzJkzqlq1qsaMGaPo6Ghnl3XTCP8AAAAAAPx/M2fO1MyZM51dhsPxqD8AAAAAACyO8A8AAAAAgMUR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4gj/AAAAAABYHI/6AwAAAADcmPWTb+3y2kff2uU5WExMjEaOHKnU1FRnl8KRfwAAAACANQwaNEg2my3P6/Dhw84uzek48g8AAAAAsIwuXbpo3rx5dm2VK1e2m87MzJSbm9utLMvpOPIPAAAAALAMd3d3BQQE2L06duyo4cOHa+TIkapUqZIiIiIkST/99JO6du2qsmXLyt/fX/3799fvv/9ujtWuXTuNGDFCzz33nCpUqKCAgABNnDjRbnmpqakaOnSo/P395eHhoQYNGmjZsmV2fVatWqV69eqpbNmy6tKli5KSkop8O/wV4R8AAAAAYHnz58+Xm5ubtmzZorlz5yo1NVUdOnRQ06ZNtXPnTq1cuVIpKSl67LHH8rzPy8tL27dv19SpUzVp0iTFxsZKknJyctS1a1dt2bJFn376qX7++WdNmTJFrq6u5vsvXLigt956S5988ok2bdqkxMREjR079pauu8Rp/wAAAAAAC1m2bJnKli1rTnft2lWSVKtWLU2dOtVsf/3119W0aVO98cYbZtvHH3+s4OBgHTx4ULVr15YkNWrUSK+88oo5xjvvvKO1a9eqU6dOWrNmjb7//nv98ssvZv8777zTrp6srCzNnTtXNWrUkCQNHz5ckyZNKoI1vzbCPwAAAADAMtq3b685c+aY015eXurTp4+aN29u12/Pnj1av3693RcFuY4cOWIX/q8UGBiokydPSpLi4+NVpUoVs29+PD09zeD/1/ffSoR/AAAAAIBleHl5qWbNmvm2X+ncuXPq3r273nzzzTx9AwMDzZ9Lly5tN89msyknJ0eSVKZMmevWk9/7DcO47vscjfAPAAAAALjtNGvWTF9++aWqV6+uUqUKF40bNWqk48eP210mUFxxwz8AAAAAwG0nKipKZ86cUZ8+fbRjxw4dOXJEq1at0uDBg5WdnX1DY7Rt21Zt2rRRz549FRsbq4SEBK1YsUIrV64s4uoLjiP/AAAAAIAb0z7a2RU4TFBQkLZs2aLx48erc+fOysjIULVq1dSlSxe5uNz4cfIvv/xSY8eOVZ8+fXT+/HnVrFlTU6ZMKcLKC8dmOONiAwtKT0+Xj4+P0tLS5O3t7exySoSZsQdveoxRnYr3qTUAAABASXPp0iUlJCQoJCREHh4ezi7ntnet/VGQHMpp/wAAAAAAWBzhHwAAAAAAiyP8AwAAAABgcYR/AAAAAAAsjvAPAAAAAMiDe8MXD47aD4R/AAAAAICpdOnSkqQLFy44uRJI/7cfcvdLYZVyRDEAAAAAAGtwdXWVr6+vTp48KUny9PSUzWZzclW3H8MwdOHCBZ08eVK+vr5ydXW9qfEI/wAAAAAAOwEBAZJkfgEA5/H19TX3x80g/AMAAAAA7NhsNgUGBsrPz09ZWVnOLue2Vbp06Zs+4p+L8A8AAAAAyJerq6vDwiecixv+AQAAAABgcYR/AAAAAAAsjvAPAAAAAIDFEf4BAAAAALA4wj8AAAAAABZH+AcAAAAAwOKcGv43bdqk7t27KygoSDabTUuXLrWbb7PZ8n1NmzbN7FO9evU886dMmWI3zt69e9W6dWt5eHgoODhYU6dOzVPL4sWLVbduXXl4eKhhw4b69ttvi2SdAQAAAAC41Uo5c+Hnz59X48aN9cQTT+iRRx7JMz8pKcluesWKFYqMjFTPnj3t2idNmqSnnnrKnC5Xrpz5c3p6ujp37qzw8HDNnTtXP/74o5544gn5+vpqyJAhkqStW7eqT58+mjx5sh544AEtWLBAPXr00O7du9WgQQNHrjIcbGbswZseY1Sn2g6oBAAAAACKL6eG/65du6pr165XnR8QEGA3/fXXX6t9+/a688477drLlSuXp2+uzz77TJmZmfr444/l5uamu+66S/Hx8ZoxY4YZ/mfPnq0uXbpo3LhxkqTXXntNsbGxeueddzR37tybWUUAAAAAAJyuxFzzn5KSouXLlysyMjLPvClTpqhixYpq2rSppk2bpsuXL5vz4uLi1KZNG7m5uZltEREROnDggP744w+zT3h4uN2YERERiouLu2o9GRkZSk9Pt3sBAAAAAFAcOfXIf0HMnz9f5cqVy3N5wIgRI9SsWTNVqFBBW7duVXR0tJKSkjRjxgxJUnJyskJCQuze4+/vb84rX768kpOTzbYr+yQnJ1+1nsmTJ+vVV191xKoBAAAAAFCkSkz4//jjj9WvXz95eHjYtY8ePdr8uVGjRnJzc9PQoUM1efJkubu7F1k90dHRdstOT09XcHBwkS0PAAAAAIDCKhHh/7vvvtOBAwe0cOHC6/YNDQ3V5cuXdfToUdWpU0cBAQFKSUmx65M7nXufgKv1udp9BCTJ3d29SL9cAAAAAADAUUrENf8fffSRmjdvrsaNG1+3b3x8vFxcXOTn5ydJCgsL06ZNm5SVlWX2iY2NVZ06dVS+fHmzz9q1a+3GiY2NVVhYmAPXAgAAAAAA53Bq+D937pzi4+MVHx8vSUpISFB8fLwSExPNPunp6Vq8eLGefPLJPO+Pi4vTrFmztGfPHv3666/67LPPNGrUKD3++ONmsO/bt6/c3NwUGRmpffv2aeHChZo9e7bdKfvPPvusVq5cqenTp2v//v2aOHGidu7cqeHDhxftBgAAAAAA4BZw6mn/O3fuVPv27c3p3EA+cOBAxcTESJI+//xzGYahPn365Hm/u7u7Pv/8c02cOFEZGRkKCQnRqFGj7IK9j4+PVq9eraioKDVv3lyVKlXShAkTzMf8SdK9996rBQsW6KWXXtILL7ygWrVqaenSpWrQoEERrTkAAAAAALeOzTAMw9lFWEF6erp8fHyUlpYmb29vZ5dTIsyMPejsEiRJozrVdnYJAAAAAFBgBcmhJeKafwAAAAAAUHiEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDinBr+N23apO7duysoKEg2m01Lly61mz9o0CDZbDa7V5cuXez6nDlzRv369ZO3t7d8fX0VGRmpc+fO2fXZu3evWrduLQ8PDwUHB2vq1Kl5alm8eLHq1q0rDw8PNWzYUN9++63D1xcAAAAAAGco5cyFnz9/Xo0bN9YTTzyhRx55JN8+Xbp00bx588xpd3d3u/n9+vVTUlKSYmNjlZWVpcGDB2vIkCFasGCBJCk9PV2dO3dWeHi45s6dqx9//FFPPPGEfH19NWTIEEnS1q1b1adPH02ePFkPPPCAFixYoB49emj37t1q0KBBEa19yTYz9qCzSwAAAAAA3CCbYRiGs4uQJJvNpq+++ko9evQw2wYNGqTU1NQ8ZwTk+uWXX1S/fn3t2LFDd999tyRp5cqVuv/++3X8+HEFBQVpzpw5evHFF5WcnCw3NzdJ0vPPP6+lS5dq//79kqRevXrp/PnzWrZsmTl2y5Yt1aRJE82dO/eG6k9PT5ePj4/S0tLk7e1diC1Qslgp/I/qVNvZJQAAAABAgRUkhxb7a/43bNggPz8/1alTR8OGDdPp06fNeXFxcfL19TWDvySFh4fLxcVF27dvN/u0adPGDP6SFBERoQMHDuiPP/4w+4SHh9stNyIiQnFxcVetKyMjQ+np6XYvAAAAAACKo2Id/rt06aJ///vfWrt2rd58801t3LhRXbt2VXZ2tiQpOTlZfn5+du8pVaqUKlSooOTkZLOPv7+/XZ/c6ev1yZ2fn8mTJ8vHx8d8BQcH39zKAgAAAABQRJx6zf/19O7d2/y5YcOGatSokWrUqKENGzaoY8eOTqxMio6O1ujRo83p9PR0vgAAAAAAABRLxfrI/1/deeedqlSpkg4fPixJCggI0MmTJ+36XL58WWfOnFFAQIDZJyUlxa5P7vT1+uTOz4+7u7u8vb3tXgAAAAAAFEclKvwfP35cp0+fVmBgoCQpLCxMqamp2rVrl9ln3bp1ysnJUWhoqNln06ZNysrKMvvExsaqTp06Kl++vNln7dq1dsuKjY1VWFhYUa8SAAAAAABFzqnh/9y5c4qPj1d8fLwkKSEhQfHx8UpMTNS5c+c0btw4bdu2TUePHtXatWv10EMPqWbNmoqIiJAk1atXT126dNFTTz2l77//Xlu2bNHw4cPVu3dvBQUFSZL69u0rNzc3RUZGat++fVq4cKFmz55td8r+s88+q5UrV2r69Onav3+/Jk6cqJ07d2r48OG3fJsAAAAAAOBoTg3/O3fuVNOmTdW0aVNJ0ujRo9W0aVNNmDBBrq6u2rt3rx588EHVrl1bkZGRat68ub777ju5u7ubY3z22WeqW7euOnbsqPvvv1/33XefPvjgA3O+j4+PVq9erYSEBDVv3lxjxozRhAkTNGTIELPPvffeqwULFuiDDz5Q48aN9cUXX2jp0qVq0KDBrdsYAAAAAAAUEZthGIazi7CCgjxf0Qpmxh50dgkOM6pTbWeXAAAAAAAFVpAcWqKu+QcAAAAAAAVH+AcAAAAAwOII/wAAAAAAWBzhHwAAAAAAiyP8AwAAAABgcYR/AAAAAAAsjvAPAAAAAIDFEf4BAAAAALA4wj8AAAAAABZH+AcAAAAAwOII/wAAAAAAWBzhHwAAAAAAiyP8AwAAAABgcYR/AAAAAAAsjvAPAAAAAIDFEf4BAAAAALA4wj8AAAAAABZH+AcAAAAAwOII/wAAAAAAWBzhHwAAAAAAiyP8AwAAAABgcYR/AAAAAAAsrpSzCwCcbWbswZseY1Sn2g6oBAAAAACKBkf+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHFODf+bNm1S9+7dFRQUJJvNpqVLl5rzsrKyNH78eDVs2FBeXl4KCgrSgAEDdOLECbsxqlevLpvNZveaMmWKXZ+9e/eqdevW8vDwUHBwsKZOnZqnlsWLF6tu3bry8PBQw4YN9e233xbJOgMAAAAAcKs5NfyfP39ejRs31rvvvptn3oULF7R79269/PLL2r17t5YsWaIDBw7owQcfzNN30qRJSkpKMl/PPPOMOS89PV2dO3dWtWrVtGvXLk2bNk0TJ07UBx98YPbZunWr+vTpo8jISP3www/q0aOHevTooZ9++qloVhwAAAAAgFuolDMX3rVrV3Xt2jXfeT4+PoqNjbVre+edd9SiRQslJiaqatWqZnu5cuUUEBCQ7zifffaZMjMz9fHHH8vNzU133XWX4uPjNWPGDA0ZMkSSNHv2bHXp0kXjxo2TJL322muKjY3VO++8o7lz5zpiVQEAAAAAcJoSdc1/WlqabDabfH197dqnTJmiihUrqmnTppo2bZouX75szouLi1ObNm3k5uZmtkVEROjAgQP6448/zD7h4eF2Y0ZERCguLu6qtWRkZCg9Pd3uBQAAAABAceTUI/8FcenSJY0fP159+vSRt7e32T5ixAg1a9ZMFSpU0NatWxUdHa2kpCTNmDFDkpScnKyQkBC7sfz9/c155cuXV3Jystl2ZZ/k5OSr1jN58mS9+uqrjlo9AAAAAACKTIkI/1lZWXrsscdkGIbmzJljN2/06NHmz40aNZKbm5uGDh2qyZMny93dvchqio6Otlt2enq6goODi2x5AAAAAAAUVrEP/7nB/9ixY1q3bp3dUf/8hIaG6vLlyzp69Kjq1KmjgIAApaSk2PXJnc69T8DV+lztPgKS5O7uXqRfLgAAAAAA4CjF+pr/3OB/6NAhrVmzRhUrVrzue+Lj4+Xi4iI/Pz9JUlhYmDZt2qSsrCyzT2xsrOrUqaPy5cubfdauXWs3TmxsrMLCwhy4NgAAAAAAOIdTj/yfO3dOhw8fNqcTEhIUHx+vChUqKDAwUI8++qh2796tZcuWKTs727wGv0KFCnJzc1NcXJy2b9+u9u3bq1y5coqLi9OoUaP0+OOPm8G+b9++evXVVxUZGanx48frp59+0uzZszVz5kxzuc8++6zatm2r6dOnq1u3bvr888+1c+dOu8cBAgAAAABQUtkMwzCctfANGzaoffv2edoHDhyoiRMn5rlRX67169erXbt22r17t/7+979r//79ysjIUEhIiPr376/Ro0fbnZK/d+9eRUVFaceOHapUqZKeeeYZjR8/3m7MxYsX66WXXtLRo0dVq1YtTZ06Vffff/8Nr0t6erp8fHyUlpZ23UsTrGBm7EFnl1CsjOpU29klAAAAALjNFCSHOjX8Wwnh//ZG+AcAAABwqxUkhxbra/4BAAAAAMDNI/wDAAAAAGBxhH8AAAAAACyO8A8AAAAAgMUR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4gj/AAAAAABYHOEfAAAAAACLI/wDAAAAAGBxhH8AAAAAACyulLMLAAqiZeIHN9x3W9UhRVgJAAAAAJQchTry/+uvvzq6DgAAAAAAUEQKFf5r1qyp9u3b69NPP9WlS5ccXRMAAAAAAHCgQoX/3bt3q1GjRho9erQCAgI0dOhQff/9946uDQAAAAAAOEChrvlv0qSJZs+erenTp+ubb75RTEyM7rvvPtWuXVtPPPGE+vfvr8qVKzu6VlhQQa7hBwAAAAAUjs0wDONmB8nIyNB7772n6OhoZWZmys3NTY899pjefPNNBQYGOqLOYi89PV0+Pj5KS0uTt7e3s8spcjNjDzpknOIU/m/mBoGjOtV2YCUAAAAAcH0FyaE39ai/nTt36u9//7sCAwM1Y8YMjR07VkeOHFFsbKxOnDihhx566GaGBwAAAAAADlCo0/5nzJihefPm6cCBA7r//vv173//W/fff79cXP78LiEkJEQxMTGqXr26I2sFAAAAAACFUKjwP2fOHD3xxBMaNGjQVU/r9/Pz00cffXRTxQEAAAAAgJtXqPB/6NCh6/Zxc3PTwIEDCzM8AAAAAABwoEJd8z9v3jwtXrw4T/vixYs1f/78my4KAAAAAAA4TqHC/+TJk1WpUqU87X5+fnrjjTduuigAAAAAAOA4hQr/iYmJCgkJydNerVo1JSYm3nRRAAAAAADAcQoV/v38/LR379487Xv27FHFihVvuigAAAAAAOA4hQr/ffr00YgRI7R+/XplZ2crOztb69at07PPPqvevXs7ukYAAAAAAHATCnW3/9dee01Hjx5Vx44dVarUn0Pk5ORowIABXPMPAAAAAEAxU6jw7+bmpoULF+q1117Tnj17VKZMGTVs2FDVqlVzdH0AAAAAAOAmFSr856pdu7Zq167tqFoAAAAAAEARKFT4z87OVkxMjNauXauTJ08qJyfHbv66descUhwAAAAAALh5hQr/zz77rGJiYtStWzc1aNBANpvN0XUBAAAAAAAHKVT4//zzz7Vo0SLdf//9jq4HAAAAAAA4WKEe9efm5qaaNWs6uhYAAAAAAFAEChX+x4wZo9mzZ8swDEfXAwAAAAAAHKxQp/1v3rxZ69ev14oVK3TXXXepdOnSdvOXLFnikOIAAAAAAMDNK1T49/X11cMPP+zoWgAAAAAAQBEoVPifN2+eo+sAAAAAAABFpFDX/EvS5cuXtWbNGr3//vs6e/asJOnEiRM6d+6cw4oDAAAAAAA3r1BH/o8dO6YuXbooMTFRGRkZ6tSpk8qVK6c333xTGRkZmjt3rqPrBAAAAAAAhVSoI//PPvus7r77bv3xxx8qU6aM2f7www9r7dq1DisOAAAAAADcvEId+f/uu++0detWubm52bVXr15dv/32m0MKAwAAAAAAjlGoI/85OTnKzs7O0378+HGVK1fuposCAAAAAACOU6jw37lzZ82aNcucttlsOnfunF555RXdf//9jqoNAAAAAAA4QKFO+58+fboiIiJUv359Xbp0SX379tWhQ4dUqVIl/ec//3F0jQAAAAAA4CYUKvxXqVJFe/bs0eeff669e/fq3LlzioyMVL9+/exuAAgAAAAAAJyvUOFfkkqVKqXHH3/ckbUAAAAAAIAiUKhr/v/9739f83WjNm3apO7duysoKEg2m01Lly61m28YhiZMmKDAwECVKVNG4eHhOnTokF2fM2fOqF+/fvL29pavr68iIyN17tw5uz579+5V69at5eHhoeDgYE2dOjVPLYsXL1bdunXl4eGhhg0b6ttvv73xDQIAAAAAQDFWqCP/zz77rN10VlaWLly4IDc3N3l6emrAgAE3NM758+fVuHFjPfHEE3rkkUfyzJ86darefvttzZ8/XyEhIXr55ZcVERGhn3/+WR4eHpKkfv36KSkpSbGxscrKytLgwYM1ZMgQLViwQJKUnp6uzp07Kzw8XHPnztWPP/6oJ554Qr6+vhoyZIgkaevWrerTp48mT56sBx54QAsWLFCPHj20e/duNWjQoDCbCLeZmbEHb3qMUZ1qO6ASAAAAAMjLZhiG4YiBDh06pGHDhmncuHGKiIgoeCE2m7766iv16NFD0p9H/YOCgjRmzBiNHTtWkpSWliZ/f3/FxMSod+/e+uWXX1S/fn3t2LFDd999tyRp5cqVuv/++3X8+HEFBQVpzpw5evHFF5WcnCw3NzdJ0vPPP6+lS5dq//79kqRevXrp/PnzWrZsmVlPy5Yt1aRJE82dO/eG6k9PT5ePj4/S0tLk7e1d4PUvaRwRdiWpZeIHDhnHEbZVHeLU5RP+AQAAABREQXJooU77z0+tWrU0ZcqUPGcFFFZCQoKSk5MVHh5utvn4+Cg0NFRxcXGSpLi4OPn6+prBX5LCw8Pl4uKi7du3m33atGljBn9JioiI0IEDB/THH3+Yfa5cTm6f3OXkJyMjQ+np6XYvAAAAAACKI4eFf+nPmwCeOHHCIWMlJydLkvz9/e3a/f39zXnJycny8/PLU0OFChXs+uQ3xpXLuFqf3Pn5mTx5snx8fMxXcHBwQVcRAAAAAIBbolDX/H/zzTd204ZhKCkpSe+8845atWrlkMKKu+joaI0ePdqcTk9P5wsAAAAAAECxVKjwn3tdfi6bzabKlSurQ4cOmj59uiPqUkBAgCQpJSVFgYGBZntKSoqaNGli9jl58qTd+y5fvqwzZ86Y7w8ICFBKSopdn9zp6/XJnZ8fd3d3ubu7F2LNAAAAAAC4tQp12n9OTo7dKzs7W8nJyVqwYIFdUL8ZISEhCggI0Nq1a8229PR0bd++XWFhYZKksLAwpaamateuXWafdevWKScnR6GhoWafTZs2KSsry+wTGxurOnXqqHz58mafK5eT2yd3OQAAAAAAlGQOvea/oM6dO6f4+HjFx8dL+vMmf/Hx8UpMTJTNZtPIkSP1+uuv65tvvtGPP/6oAQMGKCgoyDzzoF69eurSpYueeuopff/999qyZYuGDx+u3r17KygoSJLUt29fubm5KTIyUvv27dPChQs1e/Zsu1P2n332Wa1cuVLTp0/X/v37NXHiRO3cuVPDhw+/1ZsEAAAAAACHK9Rp/1cG5+uZMWPGVeft3LlT7du3zzPuwIEDFRMTo+eee07nz5/XkCFDlJqaqvvuu08rV66Uh4eH+Z7PPvtMw4cPV8eOHeXi4qKePXvq7bffNuf7+Pho9erVioqKUvPmzVWpUiVNmDBBQ4b832Pd7r33Xi1YsEAvvfSSXnjhBdWqVUtLly5VgwYNbng9AQAAAAAormyGYRgFfVP79u31ww8/KCsrS3Xq1JEkHTx4UK6urmrWrNn/DW6zad26dY6rthgryPMVrWBm7EGHjNMy8QOHjOMI26oOuX6nIjSqU22nLh8AAABAyVKQHFqoI//du3dXuXLlNH/+fPO6+T/++EODBw9W69atNWbMmMIMCwAAAAAAikChrvmfPn26Jk+ebAZ/SSpfvrxef/11h93tHwAAAAAAOEahjvynp6fr1KlTedpPnTqls2fP3nRRgDMU5BIEZ18iAAAAAAAFUagj/w8//LAGDx6sJUuW6Pjx4zp+/Li+/PJLRUZG6pFHHnF0jQAAAAAA4CYU6sj/3LlzNXbsWPXt21dZWVl/DlSqlCIjIzVt2jSHFggAAAAAAG5OocK/p6en3nvvPU2bNk1HjhyRJNWoUUNeXl4OLQ4AAAAAANy8QoX/XElJSUpKSlKbNm1UpkwZGYYhm83mqNpQQhWnx/cBAAAAAAp5zf/p06fVsWNH1a5dW/fff7+SkpIkSZGRkTzmDwAAAACAYqZQ4X/UqFEqXbq0EhMT5enpabb36tVLK1eudFhxAAAAAADg5hXqtP/Vq1dr1apVqlKlil17rVq1dOzYMYcUBgAAAAAAHKNQR/7Pnz9vd8Q/15kzZ+Tu7n7TRQEAAAAAAMcpVPhv3bq1/v3vf5vTNptNOTk5mjp1qtq3b++w4gAAAAAAwM0r1Gn/U6dOVceOHbVz505lZmbqueee0759+3TmzBlt2bLF0TUCAAAAAICbUKgj/w0aNNDBgwd133336aGHHtL58+f1yCOP6IcfflCNGjUcXSMAAAAAALgJBT7yn5WVpS5dumju3Ll68cUXi6ImAAAAAADgQAU+8l+6dGnt3bu3KGoBAAAAAABFoFCn/T/++OP66KOPHF0LAAAAAAAoAoW64d/ly5f18ccfa82aNWrevLm8vLzs5s+YMcMhxQEAAAAAgJtXoPD/66+/qnr16vrpp5/UrFkzSdLBgwft+thsNsdVBwAAAAAAblqBwn+tWrWUlJSk9evXS5J69eqlt99+W/7+/kVSHAAAAAAAuHkFuubfMAy76RUrVuj8+fMOLQgAAAAAADhWoW74l+uvXwYAAAAAAIDip0Dh32az5bmmn2v8AQAAAAAo3gp0zb9hGBo0aJDc3d0lSZcuXdLTTz+d527/S5YscVyFAAAAAADgphQo/A8cONBu+vHHH3doMQAAAAAAwPEKFP7nzZtXVHUAAAAAAIAiUqDwD+BPLRM/KFD/bVWHFFElAAAAAHB9N3W3fwAAAAAAUPwR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4gj/AAAAAABYHOEfAAAAAACLI/wDAAAAAGBxhH8AAAAAACyO8A8AAAAAgMUR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4gj/AAAAAABYHOEfAAAAAACLI/wDAAAAAGBxhH8AAAAAACyO8A8AAAAAgMUR/gEAAAAAsLhiH/6rV68um82W5xUVFSVJateuXZ55Tz/9tN0YiYmJ6tatmzw9PeXn56dx48bp8uXLdn02bNigZs2ayd3dXTVr1lRMTMytWkUAAAAAAIpUKWcXcD07duxQdna2Of3TTz+pU6dO+tvf/ma2PfXUU5o0aZI57enpaf6cnZ2tbt26KSAgQFu3blVSUpIGDBig0qVL64033pAkJSQkqFu3bnr66af12Wefae3atXryyScVGBioiIiIW7CWAAAAAAAUnWIf/itXrmw3PWXKFNWoUUNt27Y12zw9PRUQEJDv+1evXq2ff/5Za9askb+/v5o0aaLXXntN48eP18SJE+Xm5qa5c+cqJCRE06dPlyTVq1dPmzdv1syZMwn/AAAAAIASr9if9n+lzMxMffrpp3riiSdks9nM9s8++0yVKlVSgwYNFB0drQsXLpjz4uLi1LBhQ/n7+5ttERERSk9P1759+8w+4eHhdsuKiIhQXFzcVWvJyMhQenq63QsAAAAAgOKo2B/5v9LSpUuVmpqqQYMGmW19+/ZVtWrVFBQUpL1792r8+PE6cOCAlixZIklKTk62C/6SzOnk5ORr9klPT9fFixdVpkyZPLVMnjxZr776qiNXDwAAAACAIlGiwv9HH32krl27KigoyGwbMmSI+XPDhg0VGBiojh076siRI6pRo0aR1RIdHa3Ro0eb0+np6QoODi6y5QEAAAAAUFglJvwfO3ZMa9asMY/oX01oaKgk6fDhw6pRo4YCAgL0/fff2/VJSUmRJPM+AQEBAWbblX28vb3zPeovSe7u7nJ3dy/UugAAAAAAcCuVmGv+582bJz8/P3Xr1u2a/eLj4yVJgYGBkqSwsDD9+OOPOnnypNknNjZW3t7eql+/vtln7dq1duPExsYqLCzMgWsAAAAAAIBzlIjwn5OTo3nz5mngwIEqVer/TlY4cuSIXnvtNe3atUtHjx7VN998owEDBqhNmzZq1KiRJKlz586qX7+++vfvrz179mjVqlV66aWXFBUVZR65f/rpp/Xrr7/queee0/79+/Xee+9p0aJFGjVqlFPWFwAAAAAARyoRp/2vWbNGiYmJeuKJJ+za3dzctGbNGs2aNUvnz59XcHCwevbsqZdeesns4+rqqmXLlmnYsGEKCwuTl5eXBg4cqEmTJpl9QkJCtHz5co0aNUqzZ89WlSpV9OGHH/KYv1zrJ+dpapl42gmFAAAAAAAKw2YYhuHsIqwgPT1dPj4+SktLk7e3t7PLcax8wn/cr4T/gthWdch1+4zqVPsWVAIAAADAKgqSQ0vEaf8AAAAAAKDwCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxZVydgHA7aBl4gfX77S+4p//bR9dtMUAAAAAuO1w5B8AAAAAAIsj/AMAAAAAYHGc9g8UE3G/npYkbbt8sNBjjOpU21HlAAAAALAQjvwDAAAAAGBxhH8AAAAAACyO8A8AAAAAgMUR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4gj/AAAAAABYHOEfAAAAAACLI/wDAAAAAGBxpZxdAG69mbEHC9S/ZeLpIqoEAAAAAHArcOQfAAAAAACLI/wDAAAAAGBxhH8AAAAAACyO8A8AAAAAgMUR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4gj/AAAAAABYHOEfAAAAAACLI/wDAAAAAGBxpZxdAAB7LRM/KFD/bVWHFFElAAAAAKyCI/8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWV6zD/8SJE2Wz2exedevWNedfunRJUVFRqlixosqWLauePXsqJSXFbozExER169ZNnp6e8vPz07hx43T58mW7Phs2bFCzZs3k7u6umjVrKiYm5lasHgAAAAAAt0SxDv+SdNdddykpKcl8bd682Zw3atQo/fe//9XixYu1ceNGnThxQo888og5Pzs7W926dVNmZqa2bt2q+fPnKyYmRhMmTDD7JCQkqFu3bmrfvr3i4+M1cuRIPfnkk1q1atUtXU8AAAAAAIpKKWcXcD2lSpVSQEBAnva0tDR99NFHWrBggTp06CBJmjdvnurVq6dt27apZcuWWr16tX7++WetWbNG/v7+atKkiV577TWNHz9eEydOlJubm+bOnauQkBBNnz5dklSvXj1t3rxZM2fOVERExC1dVwAAAAAAikKxP/J/6NAhBQUF6c4771S/fv2UmJgoSdq1a5eysrIUHh5u9q1bt66qVq2quLg4SVJcXJwaNmwof39/s09ERITS09O1b98+s8+VY+T2yR3jajIyMpSenm73AgAAAACgOCrW4T80NFQxMTFauXKl5syZo4SEBLVu3Vpnz55VcnKy3Nzc5Ovra/cef39/JScnS5KSk5Ptgn/u/Nx51+qTnp6uixcvXrW2yZMny8fHx3wFBwff7OoCAAAAAFAkivVp/127djV/btSokUJDQ1WtWjUtWrRIZcqUcWJlUnR0tEaPHm1Op6en8wUAAAAAAKBYKtZH/v/K19dXtWvX1uHDhxUQEKDMzEylpqba9UlJSTHvERAQEJDn7v+509fr4+3tfc0vGNzd3eXt7W33AgAAAACgOCpR4f/cuXM6cuSIAgMD1bx5c5UuXVpr16415x84cECJiYkKCwuTJIWFhenHH3/UyZMnzT6xsbHy9vZW/fr1zT5XjpHbJ3cMAAAAAABKumId/seOHauNGzfq6NGj2rp1qx5++GG5urqqT58+8vHxUWRkpEaPHq3169dr165dGjx4sMLCwtSyZUtJUufOnVW/fn31799fe/bs0apVq/TSSy8pKipK7u7ukqSnn35av/76q5577jnt379f7733nhYtWqRRo0Y5c9UBAAAAAHCYYn3N//Hjx9WnTx+dPn1alStX1n333adt27apcuXKkqSZM2fKxcVFPXv2VEZGhiIiIvTee++Z73d1ddWyZcs0bNgwhYWFycvLSwMHDtSkSZPMPiEhIVq+fLlGjRql2bNnq0qVKvrwww95zB8AAAAAwDJshmEYzi7CCtLT0+Xj46O0tLRif/3/zNiDBerfMvGDIqoEjrCt6hDz51GdajuxEgAAAAC3UkFyaLE+7R8AAAAAANw8wj8AAAAAABZH+AcAAAAAwOII/wAAAAAAWBzhHwAAAAAAiyP8AwAAAABgcYR/AAAAAAAsjvAPAAAAAIDFlXJ2AQAcZ2bswZseY1Sn2g6oBAAAAEBxwpF/AAAAAAAsjvAPAAAAAIDFEf4BAAAAALA4wj8AAAAAABZH+AcAAAAAwOII/wAAAAAAWByP+gNKuJaJH9xw321VhxRhJQAAAACKK478AwAAAABgcYR/AAAAAAAsjvAPAAAAAIDFEf4BAAAAALA4wj8AAAAAABZH+AcAAAAAwOII/wAAAAAAWBzhHwAAAAAAiyP8AwAAAABgcYR/AAAAAAAsjvAPAAAAAIDFEf4BAAAAALA4wj8AAAAAABZH+AcAAAAAwOII/wAAAAAAWBzhHwAAAAAAiyP8AwAAAABgcYR/AAAAAAAsrpSzCwBw67RM/OD6ndZX/L+f20cXXTEAAAAAbhmO/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALK6Uswu4lsmTJ2vJkiXav3+/ypQpo3vvvVdvvvmm6tSpY/Zp166dNm7caPe+oUOHau7cueZ0YmKihg0bpvXr16ts2bIaOHCgJk+erFKl/m/1N2zYoNGjR2vfvn0KDg7WSy+9pEGDBhX5OgLFTdyvp82ft10+WKgxRnWq7ahyAAAAADhAsT7yv3HjRkVFRWnbtm2KjY1VVlaWOnfurPPnz9v1e+qpp5SUlGS+pk6das7Lzs5Wt27dlJmZqa1bt2r+/PmKiYnRhAkTzD4JCQnq1q2b2rdvr/j4eI0cOVJPPvmkVq1adcvWFQAAAACAolKsj/yvXLnSbjomJkZ+fn7atWuX2rRpY7Z7enoqICAg3zFWr16tn3/+WWvWrJG/v7+aNGmi1157TePHj9fEiRPl5uamuXPnKiQkRNOnT5ck1atXT5s3b9bMmTMVERFRdCsIAAAAAMAtUKyP/P9VWlqaJKlChQp27Z999pkqVaqkBg0aKDo6WhcuXDDnxcXFqWHDhvL39zfbIiIilJ6ern379pl9wsPD7caMiIhQXFzcVWvJyMhQenq63QsAAAAAgOKoWB/5v1JOTo5GjhypVq1aqUGDBmZ73759Va1aNQUFBWnv3r0aP368Dhw4oCVLlkiSkpOT7YK/JHM6OTn5mn3S09N18eJFlSlTJk89kydP1quvvurQdQQAAAAAoCiUmPAfFRWln376SZs3b7ZrHzJkiPlzw4YNFRgYqI4dO+rIkSOqUaNGkdUTHR2t0aNHm9Pp6ekKDg4usuUBAAAAAFBYJeK0/+HDh2vZsmVav369qlSpcs2+oaGhkqTDhw9LkgICApSSkmLXJ3c69z4BV+vj7e2d71F/SXJ3d5e3t7fdCwAAAACA4qhYh3/DMDR8+HB99dVXWrdunUJCQq77nvj4eElSYGCgJCksLEw//vijTp48afaJjY2Vt7e36tevb/ZZu3at3TixsbEKCwtz0JoAAAAAAOA8xfq0/6ioKC1YsEBff/21ypUrZ16j7+PjozJlyujIkSNasGCB7r//flWsWFF79+7VqFGj1KZNGzVq1EiS1LlzZ9WvX1/9+/fX1KlTlZycrJdeeklRUVFyd3eXJD399NN655139Nxzz+mJJ57QunXrtGjRIi1fvtxp6w4UBy0TP7jhvtuqDrl+JwAAAABOUayP/M+ZM0dpaWlq166dAgMDzdfChQslSW5ublqzZo06d+6sunXrasyYMerZs6f++9//mmO4urpq2bJlcnV1VVhYmB5//HENGDBAkyZNMvuEhIRo+fLlio2NVePGjTV9+nR9+OGHPOYPAAAAAGAJxfrIv2EY15wfHBysjRs3XnecatWq6dtvv71mn3bt2umHH34oUH0AAAAAAJQExfrIPwAAAAAAuHmEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOMI/AAAAAAAWV6zv9o+iUZBntwMAAAAASj6O/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQAAAACwOG74B8Ah7G4kub7i9d/QPrroigEAAABghyP/AAAAAABYHEf+AThc3K+nr9tn2+WD15w/qlNtR5UDAAAA3PY48g8AAAAAgMUR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4gj/AAAAAABYHOEfAAAAAACLI/wDAAAAAGBxpZxdAIDbU8vED67dYX3F//u5fXTRFgMAAABYHEf+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgcN/wDUCzF/Xra/Hnb5YMFfv+oTrUdWQ4AAABQonHkHwAAAAAAi+PIP4Bi77qPBfyLbVWHFFElAAAAQMnEkX8AAAAAACyO8A8AAAAAgMUR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4rjbPwDLaZn4gbS+4o2/oX100RUDAAAAFAMc+QcAAAAAwOI48g/AkuJ+PX3DfbddPphv+6hOtR1VDgAAAOBUHPkHAAAAAMDiOPIP4LbXMvGD/Gfkd98A7g8AAACAEogj/wAAAAAAWBzhHwAAAAAAiyP8AwAAAABgcVzzDwBXke8TA34dW6AxwiLfclA1AAAAQOFx5B8AAAAAAIvjyD8AFKX1k2+8L08SAAAAQBHhyD8AAAAAABbHkX8AKEL53jfgaq5yP4GwOyvm358zBQAAAHCDCP9/8e6772ratGlKTk5W48aN9c9//lMtWrRwdlkAkBeXFAAAAOAGEf6vsHDhQo0ePVpz585VaGioZs2apYiICB04cEB+fn7OLg/AbapAZw9cRZgK8EWBxJcFAAAAFmMzDMNwdhHFRWhoqO655x698847kqScnBwFBwfrmWee0fPPP3/N96anp8vHx0dpaWny9va+FeUWWtxHBXtUGQA4ylUvYbgavoQAAAC4qoLkUI78/3+ZmZnatWuXoqP/7380XVxcFB4erri4uDz9MzIylJGRYU6npaVJ+nPjF3fnL2ZcvxMAFIE1+04U7A37nimaQoqJFtUr/PlDmzEFe+Om6Tfet6BjAwCAEiM3f97IMX3C///3+++/Kzs7W/7+/nbt/v7+2r9/f57+kydP1quvvpqnPTg4uMhqBABY1aQSOjYAACgOzp49Kx8fn2v2IfwXUnR0tEaPHm1O5+Tk6MyZM6pYsaJsNpsTK7u29PR0BQcH63//+1+xvzwB+WMflnzsw5KPfVjysQ9LPvZhycc+LPnYh85nGIbOnj2roKCg6/Yl/P9/lSpVkqurq1JSUuzaU1JSFBAQkKe/u7u73N3d7dp8fX2LskSH8vb25gNawrEPSz72YcnHPiz52IclH/uw5GMflnzsQ+e63hH/XC5FXEeJ4ebmpubNm2vt2rVmW05OjtauXauwsDAnVgYAAAAAwM3hyP8VRo8erYEDB+ruu+9WixYtNGvWLJ0/f16DBw92dmkAAAAAABQa4f8KvXr10qlTpzRhwgQlJyerSZMmWrlyZZ6bAJZk7u7ueuWVV/JcsoCSg31Y8rEPSz72YcnHPiz52IclH/uw5GMfliw240aeCQAAAAAAAEosrvkHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGE/9vIu+++q+rVq8vDw0OhoaH6/vvvnV0S/r/JkyfrnnvuUbly5eTn56cePXrowIEDdn3atWsnm81m93r66aft+iQmJqpbt27y9PSUn5+fxo0bp8uXL9/KVbltTZw4Mc/+qVu3rjn/0qVLioqKUsWKFVW2bFn17NlTKSkpdmOw/5yrevXqefahzWZTVFSUJD6DxdGmTZvUvXt3BQUFyWazaenSpXbzDcPQhAkTFBgYqDJlyig8PFyHDh2y63PmzBn169dP3t7e8vX1VWRkpM6dO2fXZ+/evWrdurU8PDwUHBysqVOnFvWq3TautQ+zsrI0fvx4NWzYUF5eXgoKCtKAAQN04sQJuzHy++xOmTLFrg/7sOhc73M4aNCgPPunS5cudn34HDrX9fZhfv822mw2TZs2zezD57BkIPzfJhYuXKjRo0frlVde0e7du9W4cWNFRETo5MmTzi4NkjZu3KioqCht27ZNsbGxysrKUufOnXX+/Hm7fk899ZSSkpLM15V/NLOzs9WtWzdlZmZq69atmj9/vmJiYjRhwoRbvTq3rbvuustu/2zevNmcN2rUKP33v//V4sWLtXHjRp04cUKPPPKIOZ/953w7duyw23+xsbGSpL/97W9mHz6Dxcv58+fVuHFjvfvuu/nOnzp1qt5++23NnTtX27dvl5eXlyIiInTp0iWzT79+/bRv3z7FxsZq2bJl2rRpk4YMGWLOT09PV+fOnVWtWjXt2rVL06ZN08SJE/XBBx8U+frdDq61Dy9cuKDdu3fr5Zdf1u7du7VkyRIdOHBADz74YJ6+kyZNsvtsPvPMM+Y89mHRut7nUJK6dOlit3/+85//2M3nc+hc19uHV+67pKQkffzxx7LZbOrZs6ddPz6HJYCB20KLFi2MqKgoczo7O9sICgoyJk+e7MSqcDUnT540JBkbN24029q2bWs8++yzV33Pt99+a7i4uBjJyclm25w5cwxvb28jIyOjKMuFYRivvPKK0bhx43znpaamGqVLlzYWL15stv3yyy+GJCMuLs4wDPZfcfTss88aNWrUMHJycgzD4DNY3EkyvvrqK3M6JyfHCAgIMKZNm2a2paamGu7u7sZ//vMfwzAM4+effzYkGTt27DD7rFixwrDZbMZvv/1mGIZhvPfee0b58uXt9uH48eONOnXqFPEa3X7+ug/z8/333xuSjGPHjplt1apVM2bOnHnV97APb5389uHAgQONhx566Krv4XNYvNzI5/Chhx4yOnToYNfG57Bk4Mj/bSAzM1O7du1SeHi42ebi4qLw8HDFxcU5sTJcTVpamiSpQoUKdu2fffaZKlWqpAYNGig6OloXLlww58XFxalhw4by9/c32yIiIpSenq59+/bdmsJvc4cOHVJQUJDuvPNO9evXT4mJiZKkXbt2KSsry+4zWLduXVWtWtX8DLL/ipfMzEx9+umneuKJJ2Sz2cx2PoMlR0JCgpKTk+0+dz4+PgoNDbX73Pn6+uruu+82+4SHh8vFxUXbt283+7Rp00Zubm5mn4iICB04cEB//PHHLVob5EpLS5PNZpOvr69d+5QpU1SxYkU1bdpU06ZNs7vchn3ofBs2bJCfn5/q1KmjYcOG6fTp0+Y8PoclS0pKipYvX67IyMg88/gcFn+lnF0Ait7vv/+u7Oxsu/8hlSR/f3/t37/fSVXhanJycjRy5Ei1atVKDRo0MNv79u2ratWqKSgoSHv37tX48eN14MABLVmyRJKUnJyc7z7OnYeiFRoaqpiYGNWpU0dJSUl69dVX1bp1a/30009KTk6Wm5tbnv9Z9ff3N/cN+694Wbp0qVJTUzVo0CCzjc9gyZK7zfPbJ1d+7vz8/OzmlypVShUqVLDrExISkmeM3Hnly5cvkvqR16VLlzR+/Hj16dNH3t7eZvuIESPUrFkzVahQQVu3blV0dLSSkpI0Y8YMSexDZ+vSpYseeeQRhYSE6MiRI3rhhRfUtWtXxcXFydXVlc9hCTN//nyVK1fO7tJFic9hSUH4B4qZqKgo/fTTT3bXi0uyu/atYcOGCgwMVMeOHXXkyBHVqFHjVpeJv+jatav5c6NGjRQaGqpq1app0aJFKlOmjBMrQ2F89NFH6tq1q4KCgsw2PoOA82RlZemxxx6TYRiaM2eO3bzRo0ebPzdq1Ehubm4aOnSoJk+eLHd391tdKv6id+/e5s8NGzZUo0aNVKNGDW3YsEEdO3Z0YmUojI8//lj9+vWTh4eHXTufw5KB0/5vA5UqVZKrq2ueO4unpKQoICDASVUhP8OHD9eyZcu0fv16ValS5Zp9Q0NDJUmHDx+WJAUEBOS7j3Pn4dby9fVV7dq1dfjwYQUEBCgzM1Opqal2fa78DLL/io9jx45pzZo1evLJJ6/Zj89g8Za7za/1b19AQECeG99evnxZZ86c4bNZjOQG/2PHjik2NtbuqH9+QkNDdfnyZR09elQS+7C4ufPOO1WpUiW7v518DkuG7777TgcOHLjuv48Sn8PiivB/G3Bzc1Pz5s21du1asy0nJ0dr165VWFiYEytDLsMwNHz4cH311Vdat25dntOi8hMfHy9JCgwMlCSFhYXpxx9/tPsHNPd/kurXr18kdePqzp07pyNHjigwMFDNmzdX6dKl7T6DBw4cUGJiovkZZP8VH/PmzZOfn5+6det2zX58Bou3kJAQBQQE2H3u0tPTtX37drvPXWpqqnbt2mX2WbdunXJycswvd8LCwrRp0yZlZWWZfWJjY1WnTh1OU70FcoP/oUOHtGbNGlWsWPG674mPj5eLi4t5Kjn7sHg5fvy4Tp8+bfe3k89hyfDRRx+pefPmaty48XX78jksppx9x0HcGp9//rnh7u5uxMTEGD///LMxZMgQw9fX1+6u1HCeYcOGGT4+PsaGDRuMpKQk83XhwgXDMAzj8OHDxqRJk4ydO3caCQkJxtdff23ceeedRps2bcwxLl++bDRo0MDo3LmzER8fb6xcudKoXLmyER0d7azVuq2MGTPG2LBhg5GQkGBs2bLFCA8PNypVqmScPHnSMAzDePrpp42qVasa69atM3bu3GmEhYUZYWFh5vvZf8VDdna2UbVqVWP8+PF27XwGi6ezZ88aP/zwg/HDDz8YkowZM2YYP/zwg3kn+ClTphi+vr7G119/bezdu9d46KGHjJCQEOPixYvmGF26dDGaNm1qbN++3di8ebNRq1Yto0+fPub81NRUw9/f3+jfv7/x008/GZ9//rnh6elpvP/++7d8fa3oWvswMzPTePDBB40qVaoY8fHxdv8+5t4xfOvWrcbMmTON+Ph448iRI8ann35qVK5c2RgwYIC5DPZh0brWPjx79qwxduxYIy4uzkhISDDWrFljNGvWzKhVq5Zx6dIlcww+h851vb+lhmEYaWlphqenpzFnzpw87+dzWHIQ/m8j//znP42qVasabm5uRosWLYxt27Y5uyT8f5Lyfc2bN88wDMNITEw02rRpY1SoUMFwd3c3atasaYwbN85IS0uzG+fo0aNG165djTJlyhiVKlUyxowZY2RlZTlhjW4/vXr1MgIDAw03NzfjjjvuMHr16mUcPnzYnH/x4kXj73//u1G+fHnD09PTePjhh42kpCS7Mdh/zrdq1SpDknHgwAG7dj6DxdP69evz/ds5cOBAwzD+fNzfyy+/bPj7+xvu7u5Gx44d8+zb06dPG3369DHKli1reHt7G4MHDzbOnj1r12fPnj3GfffdZ7i7uxt33HGHMWXKlFu1ipZ3rX2YkJBw1X8f169fbxiGYezatcsIDQ01fHx8DA8PD6NevXrGG2+8YRcsDYN9WJSutQ8vXLhgdO7c2ahcubJRunRpo1q1asZTTz2V5+ATn0Pnut7fUsMwjPfff98oU6aMkZqamuf9fA5LDpthGEaRnloAAAAAAACcimv+AQAAAACwOMI/AAAAAAAWR/gHAAAAAMDiCP8AAAAAAFgc4R8AAAAAAIsj/AMAAAAAYHGEfwAAAAAALI7wDwAAAACAxRH+AQCAZcTExMjX17dA79mwYYNsNptSU1MLvVxHjAEAQFEi/AMAcA2nTp3SsGHDVLVqVbm7uysgIEARERHasmWLQ5fTrl07jRw50qFjFpXCBOyiUL16dc2aNeumx7n33nuVlJQkHx+fmy+qAPKrv7hsWwCA9ZRydgEAABRnPXv2VGZmpubPn68777xTKSkpWrt2rU6fPu3s0uAgbm5uCggIcHYZDpWdnS2bzSYXF47zAAD+xL8IAABcRWpqqr777ju9+eabat++vapVq6YWLVooOjpaDz74oF2/J598UpUrV5a3t7c6dOigPXv2mPMnTpyoJk2a6JNPPlH16tXl4+Oj3r176+zZs5KkQYMGaePGjZo9e7ZsNptsNpuOHj0qSfrpp5/UtWtXlS1bVv7+/urfv79+//13c+x27dppxIgReu6551ShQgUFBARo4sSJedZj6NCh8vf3l4eHhxo0aKBly5aZ8zdv3qzWrVurTJkyCg4O1ogRI3T+/Pmb2m43sz0k6ezZs+rXr5+8vLwUGBiomTNn2p0d0a5dOx07dkyjRo0yt9mVVq1apXr16qls2bLq0qWLkpKSrlrvX0/Zzz36XpAx8nOt7Zpf/Rs2bNDgwYOVlpZmtuXuy4yMDI0dO1Z33HGHvLy8FBoaqg0bNpjLyq35m2++Uf369eXu7q7ExERt2LBBLVq0kJeXl3x9fdWqVSsdO3asQOsBALAGwj8AAFdRtmxZlS1bVkuXLlVGRsZV+/3tb3/TyZMntWLFCu3atUvNmjVTx44ddebMGbPPkSNHtHTpUi1btkzLli3Txo0bNWXKFEnS7NmzFRYWpqeeekpJSUlKSkpScHCwUlNT1aFDBzVt2lQ7d+7UypUrlZKSoscee8xu+fPnz5eXl5e2b9+uqVOnatKkSYqNjZUk5eTkqGvXrtqyZYs+/fRT/fzzz5oyZYpcXV3Nurp06aKePXtq7969WrhwoTZv3qzhw4cXervd7PaQpNGjR2vLli365ptvFBsbq++++067d+825y9ZskRVqlTRpEmTzG2W68KFC3rrrbf0ySefaNOmTUpMTNTYsWMLtA43O8b1tmt+9d97772aNWuWvL29zbbcZQ4fPlxxcXH6/PPPtXfvXv3tb39Tly5ddOjQIbua33zzTX344Yfat2+fKlSooB49eqht27bau3ev4uLiNGTIkDxflAAAbhMGAAC4qi+++MIoX7684eHhYdx7771GdHS0sWfPHnP+d999Z3h7exuXLl2ye1+NGjWM999/3zAMw3jllVcMT09PIz093Zw/btw4IzQ01Jxu27at8eyzz9qN8dprrxmdO3e2a/vf//5nSDIOHDhgvu++++6z63PPPfcY48ePNwzDMFatWmW4uLiY/f8qMjLSGDJkiF3bd999Z7i4uBgXL17M9z3z5s0zfHx88p3niO2Rnp5ulC5d2li8eLE5PzU11fD09LTbRtWqVTNmzpyZpzZJxuHDh822d9991/D398+3XsMwjPXr1xuSjD/++MNhY9zIdr1a/X/dtseOHTNcXV2N3377za69Y8eORnR0tF3N8fHx5vzTp08bkowNGzZctW4AwO2Da/4BALiGnj17qlu3bvruu++0bds2rVixQlOnTtWHH36oQYMGac+ePTp37pwqVqxo976LFy/qyJEj5nT16tVVrlw5czowMFAnT5685rL37Nmj9evXq2zZsnnmHTlyRLVr15YkNWrUyG7elWPHx8erSpUqZt/8lrF371599tlnZpthGMrJyVFCQoLq1at3zRrzG+9mt8evv/6qrKwstWjRwpzv4+OjOnXq3FANnp6eqlGjRr5j36ibHcOR2/XHH39UdnZ2nn2YkZFht53d3NzsfhcqVKigQYMGKSIiQp06dVJ4eLgee+wxBQYG3vCyAQDWQfgHAOA6PDw81KlTJ3Xq1Ekvv/yynnzySb3yyisaNGiQzp07p8DAQLvrr3Ndedf20qVL282z2WzKycm55nLPnTun7t27680338wz78oAd62xy5Qpc91lDB06VCNGjMgzr2rVqtd879XGK6rtcaPyG9swjFs6hiO367lz5+Tq6qpdu3aZl2vkuvKLoTJlyuQ5pX/evHkaMWKEVq5cqYULF+qll15SbGysWrZsWaAaAAAlH+EfAIACql+/vpYuXSpJatasmZKTk1WqVClVr1690GO6ubkpOzvbrq1Zs2b68ssvVb16dZUqVbh/shs1aqTjx4/r4MGD+R79b9asmX7++WfVrFmzUOPnN97Nbo8777xTpUuX1o4dO8ygnJaWpoMHD6pNmzZmv/y2WXFxI9s1v/rza2vatKmys7N18uRJtW7dusC1NG3aVE2bNlV0dLTCwsK0YMECwj8A3Ia44R8AAFdx+vRpdejQQZ9++qn27t2rhIQELV68WFOnTtVDDz0kSQoPD1dYWJh69Oih1atX6+jRo9q6datefPFF7dy584aXVb16dW3fvl1Hjx7V77//rpycHEVFRenMmTPq06ePduzYoSNHjmjVqlUaPHjwDYfetm3bqk2bNurZs6diY2OVkJCgFStWaOXKlZKk8ePHa+vWrRo+fLji4+N16NAhff3119e94V92drbi4+PtXr/88otDtke5cuU0cOBAjRs3TuvXr9e+ffsUGRkpFxcXuyPb1atX16ZNm/Tbb7/ZPQGhOLiR7Zpf/dWrV9e5c+e0du1a/f7777pw4YJq166tfv36acCAAVqyZIkSEhL0/fffa/LkyVq+fPlVa0hISFB0dLTi4uJ07NgxrV69WocOHSrwpRwAAGsg/AMAcBVly5ZVaGioZs6cqTZt2qhBgwZ6+eWX9dRTT+mdd96R9Ofp4N9++63atGmjwYMHq3bt2urdu7eOHTsmf3//G17W2LFj5erqqvr166ty5cpKTExUUFCQtmzZouzsbHXu3FkNGzbUyJEj5evrW6Dnt3/55Ze655571KdPH9WvX1/PPfec+eVBo0aNtHHjRh08eFCtW7dW06ZNNWHCBAUFBV1zzHPnzplHlHNf3bt3d9j2mDFjhsLCwvTAAw8oPDxcrVq1Ur169eTh4WH2mTRpko4ePaoaNWqocuXKNzz2rXAj2zW/+u+99149/fTT6tWrlypXrqypU6dK+vP0/QEDBmjMmDGqU6eOevToYXdmRH48PT21f/9+9ezZU7Vr19aQIUMUFRWloUOHFu3KAwCKJZtR0IvgAAAAbrHz58/rjjvu0PTp0xUZGenscgAAKHG45h8AABQ7P/zwg/bv368WLVooLS1NkyZNkiTzcgsAAFAwhH8AAFAsvfXWWzpw4IDc3NzUvHlzfffdd6pUqZKzywIAoETitH8AAAAAACyOG/4BAAAAAGBxhH8AAAAAACyO8A8AAAAAgMUR/gEAAAAAsDjCPwAAAAAAFkf4BwAAAADA4gj/AAAAAABYHOEfAAAAAACL+3+rQr/xASSf7AAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in English:\n",
      "[('the', 194588), (',', 135412), ('.', 98053), ('of', 92322), ('to', 87243), ('and', 70213), ('in', 60717), ('is', 43861), ('a', 43345), ('that', 42997)]\n",
      "Most common words in French:\n",
      "[(',', 149532), ('de', 145160), ('.', 97245), ('la', 94726), ('et', 63824), ('le', 63380), ('à', 55690), ('les', 54687), ('des', 53036), ('que', 46173)]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:37:14.294111Z",
     "start_time": "2024-07-26T18:37:05.495850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!python3 -m spacy download en_core_web_sm\n",
    "!python3 -m spacy download fr_core_news_sm"
   ],
   "id": "a3a393423c02147d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\r\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\r\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\r\n",
      "Requirement already satisfied: setuptools in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (70.1.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\r\n",
      "Requirement already satisfied: wrapt in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n",
      "Collecting fr-core-news-sm==3.7.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.3/16.3 MB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from fr-core-news-sm==3.7.0) (3.7.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.4)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.8.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\r\n",
      "Requirement already satisfied: setuptools in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (70.1.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.6.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.7.1)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.18.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\r\n",
      "Requirement already satisfied: wrapt in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.16.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/niclasstoffregen/nlp_project/nlp_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\r\n",
      "Installing collected packages: fr-core-news-sm\r\n",
      "Successfully installed fr-core-news-sm-3.7.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\r\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "1858d811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:47:38.624337Z",
     "start_time": "2024-07-26T18:37:15.699783Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Load NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_fr = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Stopwords setup\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_fr = set(stopwords.words('french'))\n",
    "\n",
    "# Helper function to perform advanced analyses\n",
    "def analyze_text(data, language):\n",
    "    \n",
    "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    data = data.copy()\n",
    "    # Tokenization and removing punctuation\n",
    "    data['tokens'] = data['text'].apply(lambda x: [token.text.lower() for token in nlp_en(x) if token.is_alpha])\n",
    "    data['num_tokens'] = data['tokens'].apply(len)\n",
    "\n",
    "    # Stopword analysis\n",
    "    data['stopword_count'] = data['tokens'].apply(lambda x: sum(token in (stopwords_en if language == 'English' else stopwords_fr) for token in x))\n",
    "    data['stopword_ratio'] = data['stopword_count'] / data['num_tokens']\n",
    "\n",
    "    # Lexical diversity\n",
    "    data['lexical_diversity'] = data['tokens'].apply(lambda x: len(set(x)) / len(x) if x else 0)\n",
    "\n",
    "    # N-Gram analysis\n",
    "    data['bigrams'] = data['tokens'].apply(lambda x: list(bigrams(x)))\n",
    "    data['trigrams'] = data['tokens'].apply(lambda x: list(trigrams(x)))\n",
    "\n",
    "    # POS tagging\n",
    "    data['pos_tags'] = data['text'].apply(lambda x: [token.pos_ for token in nlp_en(x) if token.is_alpha])\n",
    "\n",
    "    # Sentence complexity (count of clauses per sentence)\n",
    "    data['sentence_complexity'] = data['text'].apply(lambda x: sum(1 for token in nlp_en(x) if token.dep_ == 'conj'))\n",
    "\n",
    "    return data\n",
    "\n",
    "english_data = analyze_text(english_data, 'English')\n",
    "french_data = analyze_text(french_data, 'French')\n",
    "\n",
    "# Example output of analysis\n",
    "print(\"English sample analysis:\")\n",
    "print(english_data[['lexical_diversity', 'stopword_ratio', 'sentence_complexity']].head())\n",
    "print(\"French sample analysis:\")\n",
    "print(french_data[['lexical_diversity', 'stopword_ratio', 'sentence_complexity']].head())\n",
    "\n",
    "# Visualization of sentence complexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(english_data['sentence_complexity'], bins=30, alpha=0.5, label='English', color='blue')\n",
    "plt.hist(french_data['sentence_complexity'], bins=30, alpha=0.5, label='French', color='red')\n",
    "plt.title('Distribution of Sentence Complexity')\n",
    "plt.xlabel('Number of Clauses per Sentence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/niclasstoffregen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/niclasstoffregen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 41\u001B[0m\n\u001B[1;32m     37\u001B[0m     data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence_complexity\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m nlp_en(x) \u001B[38;5;28;01mif\u001B[39;00m token\u001B[38;5;241m.\u001B[39mdep_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconj\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n\u001B[0;32m---> 41\u001B[0m english_data \u001B[38;5;241m=\u001B[39m \u001B[43manalyze_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43menglish_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mEnglish\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m french_data \u001B[38;5;241m=\u001B[39m analyze_text(french_data, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFrench\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Example output of analysis\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[26], line 34\u001B[0m, in \u001B[0;36manalyze_text\u001B[0;34m(data, language)\u001B[0m\n\u001B[1;32m     31\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrigrams\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mlist\u001B[39m(trigrams(x)))\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# POS tagging\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpos_tags\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnlp_en\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_alpha\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Sentence complexity (count of clauses per sentence)\u001B[39;00m\n\u001B[1;32m     37\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence_complexity\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m nlp_en(x) \u001B[38;5;28;01mif\u001B[39;00m token\u001B[38;5;241m.\u001B[39mdep_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconj\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[26], line 34\u001B[0m, in \u001B[0;36manalyze_text.<locals>.<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     31\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrigrams\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mlist\u001B[39m(trigrams(x)))\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# POS tagging\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpos_tags\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: [token\u001B[38;5;241m.\u001B[39mpos_ \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43mnlp_en\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m token\u001B[38;5;241m.\u001B[39mis_alpha])\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Sentence complexity (count of clauses per sentence)\u001B[39;00m\n\u001B[1;32m     37\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence_complexity\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m nlp_en(x) \u001B[38;5;28;01mif\u001B[39;00m token\u001B[38;5;241m.\u001B[39mdep_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconj\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/spacy/language.py:1049\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[0;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[1;32m   1047\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1049\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1051\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[1;32m   1052\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001B[0m, in \u001B[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/spacy/pipeline/transition_parser.pyx:264\u001B[0m, in \u001B[0;36mspacy.pipeline.transition_parser.Parser.predict\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/spacy/pipeline/transition_parser.pyx:285\u001B[0m, in \u001B[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/thinc/model.py:334\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OutT:\n\u001B[1;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/spacy/ml/tb_framework.py:34\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(model, X, is_train):\n\u001B[0;32m---> 34\u001B[0m     step_model \u001B[38;5;241m=\u001B[39m \u001B[43mParserStepModel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m        \u001B[49m\u001B[43munseen_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43munseen_classes\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_upper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhas_upper\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m step_model, step_model\u001B[38;5;241m.\u001B[39mfinish_steps\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/spacy/ml/parser_model.pyx:250\u001B[0m, in \u001B[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, X, is_train)\u001B[0m\n\u001B[1;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[1;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/thinc/model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, X, is_train)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[1;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/thinc/layers/list2array.py:30\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(model, Xs, is_train)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dY: OutT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m InT:\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39munflatten(dY, lengths)\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXs\u001B[49m\u001B[43m)\u001B[49m, backprop\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/thinc/backends/ops.py:341\u001B[0m, in \u001B[0;36mOps.flatten\u001B[0;34m(self, X, dtype, pad, ndim_if_empty)\u001B[0m\n\u001B[1;32m    339\u001B[0m     padded\u001B[38;5;241m.\u001B[39mappend(xp\u001B[38;5;241m.\u001B[39mzeros((pad,) \u001B[38;5;241m+\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m:], dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype))\n\u001B[1;32m    340\u001B[0m     X \u001B[38;5;241m=\u001B[39m padded\n\u001B[0;32m--> 341\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mxp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    343\u001B[0m     result \u001B[38;5;241m=\u001B[39m xp\u001B[38;5;241m.\u001B[39masarray(result, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "1dd67f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:47:40.422540Z",
     "start_time": "2024-07-26T18:47:40.420679Z"
    }
   },
   "source": [
    "########### TASK 2###############"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "50827cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:47:42.176737Z",
     "start_time": "2024-07-26T18:47:42.174783Z"
    }
   },
   "source": [
    "##################DATA PREPROCESSING#################################"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "b20fd660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:47:42.574530Z",
     "start_time": "2024-07-26T18:47:42.428681Z"
    }
   },
   "source": [
    "def preprocess_data(data_en, data_fr):\n",
    "    # Lowercase the text\n",
    "    \"\"\"\n",
    "    Normalizing case helps reduce the complexity of the language model by treating words like “The” and “the” as the same word, which can be particularly helpful in languages like English where capitalization is more stylistic than semantic.\n",
    "    \"\"\"\n",
    "    data_en['text'] = data_en['text'].str.lower()\n",
    "    data_fr['text'] = data_fr['text'].str.lower()\n",
    "\n",
    "    # Remove XML tags\n",
    "    \"\"\"\n",
    "    Lines containing XML-tags are likely not actual conversational or formal text but rather formatting or metadata which is irrelevant for translation purposes\n",
    "    \"\"\"\n",
    "    data_en['text'] = data_en['text'].apply(lambda x: '' if x.strip().startswith('<') else x)\n",
    "    data_fr['text'] = data_fr['text'].apply(lambda x: '' if x.strip().startswith('<') else x)\n",
    "\n",
    "    # Strip empty lines and remove their correspondences\n",
    "    \"\"\"\n",
    "     Empty lines or lines that do not contain any meaningful content should be removed because they do not provide valuable information for training the model. It is also important to remove the corresponding line in the other language to maintain alignment.\n",
    "    \"\"\"\n",
    "    mask = (data_en['text'].str.strip().astype(bool) & data_fr['text'].str.strip().astype(bool))\n",
    "    data_en = data_en[mask]\n",
    "    data_fr = data_fr[mask]\n",
    "\n",
    "    return data_en, data_fr\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Steps Not Chosen and Why:\n",
    "\n",
    "- Removing Numbers or Special Characters: Not chosen because numbers and certain punctuation can carry semantic weight in sentences, which can be important for translations, such as dates, quantities, or formatted text.\n",
    "- Stemming/Lemmatization: Not typically used in machine translation preprocessing because retaining the full form of words is important for accurate translation, especially between languages with different linguistic structures.\n",
    "- Removing Stopwords: Not recommended for translation tasks because stopwords (common words like “and”, “the”, etc.) are crucial for maintaining the grammatical structure of the sentence in both source and target languages.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "preprocessed_en, preprocessed_fr  = preprocess_data(english_data,french_data)\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "9b830fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:47:47.161281Z",
     "start_time": "2024-07-26T18:47:46.990205Z"
    }
   },
   "source": [
    "import re\n",
    "\"\"\"\n",
    "Remove all characters that are defined as noise from both translations\n",
    "\"\"\"\n",
    "def remove_noisy_characters(data):\n",
    "    # Define the characters to remove\n",
    "    noisy_characters = re.escape('@#$%^&*~<>|\\\\{}[]+=_/')\n",
    "    \n",
    "    # Regex to match any noisy character\n",
    "    regex_pattern = f'[{noisy_characters}]'\n",
    "    \n",
    "    # Remove noisy characters using regex substitution\n",
    "    data['text'] = data['text'].apply(lambda x: re.sub(regex_pattern, '', x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "preprocessed_en = remove_noisy_characters(english_data)\n",
    "preprocessed_fr = remove_noisy_characters(french_data)\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "30a2899f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:47:48.346301Z",
     "start_time": "2024-07-26T18:47:47.805107Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\"\"\"\n",
    "remove special characterers only if a special character appears in one translation but not in the other and vice versa\n",
    "\"\"\"\n",
    "\n",
    "def synchronize_special_characters(data_en, data_fr):\n",
    "    counter = 0\n",
    "    # Define the characters to synchronize\n",
    "    special_characters = re.escape('@#$%^&*~<>|\\\\{}[]+=_/')\n",
    "    \n",
    "    # Regex to match any special character\n",
    "    regex_pattern = f'[{special_characters}]'\n",
    "\n",
    "    # Process each sentence pair\n",
    "    for idx in range(len(data_en)):\n",
    "        if idx >= len(data_fr):  # Ensure index is within the bounds for both dataframes\n",
    "            break\n",
    "        \n",
    "        # Extract texts from both dataframes\n",
    "        text_en = data_en.loc[idx, 'text']\n",
    "        text_fr = data_fr.loc[idx, 'text']\n",
    "\n",
    "        # Find special characters in both texts\n",
    "        found_chars_en = set(re.findall(regex_pattern, text_en))\n",
    "        found_chars_fr = set(re.findall(regex_pattern, text_fr))\n",
    "\n",
    "        # Determine characters to remove (those not in both)\n",
    "        chars_to_remove = found_chars_en.symmetric_difference(found_chars_fr)\n",
    "\n",
    "        # Remove the special characters that do not appear in both translations\n",
    "        if chars_to_remove:\n",
    "            counter += 1\n",
    "            remove_regex = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "            data_en.loc[idx, 'text'] = re.sub(remove_regex, '', text_en)\n",
    "            data_fr.loc[idx, 'text'] = re.sub(remove_regex, '', text_fr)\n",
    "    print(counter)\n",
    "    return data_en, data_fr\n",
    "\n",
    "data_en_sync, data_fr_sync = synchronize_special_characters(data_en, data_fr)"
   ],
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Too many indexers",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexingError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28mprint\u001B[39m(counter)\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data_en, data_fr\n\u001B[0;32m---> 40\u001B[0m data_en_sync, data_fr_sync \u001B[38;5;241m=\u001B[39m \u001B[43msynchronize_special_characters\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_en\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_fr\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[31], line 21\u001B[0m, in \u001B[0;36msynchronize_special_characters\u001B[0;34m(data_en, data_fr)\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Extract texts from both dataframes\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m text_en \u001B[38;5;241m=\u001B[39m \u001B[43mdata_en\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     22\u001B[0m text_fr \u001B[38;5;241m=\u001B[39m data_fr\u001B[38;5;241m.\u001B[39mloc[idx, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Find special characters in both texts\u001B[39;00m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/indexing.py:1184\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_scalar_access(key):\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_value(\u001B[38;5;241m*\u001B[39mkey, takeable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_takeable)\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1186\u001B[0m     \u001B[38;5;66;03m# we by definition only have the 0th axis\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m     axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/indexing.py:1371\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_tuple\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m   1368\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_lowerdim(tup)\n\u001B[1;32m   1370\u001B[0m \u001B[38;5;66;03m# no multi-index, so validate all of the indexers\u001B[39;00m\n\u001B[0;32m-> 1371\u001B[0m tup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_tuple_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;66;03m# ugly hack for GH #836\u001B[39;00m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_multi_take_opportunity(tup):\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/indexing.py:962\u001B[0m, in \u001B[0;36m_LocationIndexer._validate_tuple_indexer\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    957\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m    958\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_tuple_indexer\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[1;32m    959\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    960\u001B[0m \u001B[38;5;124;03m    Check the key for valid keys across my indexer.\u001B[39;00m\n\u001B[1;32m    961\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 962\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_key_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    963\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_ellipsis(key)\n\u001B[1;32m    964\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(key):\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/pandas/core/indexing.py:1001\u001B[0m, in \u001B[0;36m_LocationIndexer._validate_key_length\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    999\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m IndexingError(_one_ellipsis_message)\n\u001B[1;32m   1000\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_key_length(key)\n\u001B[0;32m-> 1001\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m IndexingError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToo many indexers\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1002\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m key\n",
      "\u001B[0;31mIndexingError\u001B[0m: Too many indexers"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "dc80e109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:48:36.460152Z",
     "start_time": "2024-07-26T18:48:36.456078Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Futher Preprocessign steps:\n",
    "\n",
    "1. Tokenization\n",
    "\n",
    "\t•\tWhy: Tokenization involves splitting text into words, phrases, or other meaningful elements called tokens. Proper tokenization is crucial as it forms the basis of how input data is fed into the model.\n",
    "\t•\tHow: Use libraries like NLTK, spaCy, or specialized tokenizer libraries like SentencePiece, which are especially useful for languages without clear word boundaries.\n",
    "\n",
    "2. Normalization\n",
    "\n",
    "\t•\tWhy: This step involves standardizing text by correcting typos, expanding contractions (e.g., converting “can’t” to “cannot”), and converting numerals into words. Normalization helps reduce the many variations of the same word, which can dilute the model’s ability to learn.\n",
    "\t•\tHow: Apply custom rules or use language-specific libraries that handle linguistic variations.\n",
    "\n",
    "3. Removing/Replacing Non-Text Elements\n",
    "\n",
    "\t•\tWhy: Non-text elements like numbers, URLs, or user mentions (e.g., Twitter handles) may not be relevant and could distract the model from learning linguistic patterns.\n",
    "\t•\tHow: Detect and replace these elements with generic placeholders like <number>, <url>, or <user>.\n",
    "\n",
    "4. Punctuation Cleaning\n",
    "\n",
    "\t•\tWhy: While removing all punctuation can be harmful, normalizing punctuation and ensuring consistent use can be very beneficial. For example, converting fancy quotation marks to standard ones (’ or “) ensures consistency.\n",
    "\t•\tHow: Replace or remove specific punctuation characters that are not beneficial for the training process.\n",
    "\n",
    "5. Handling Rare Words and Out-of-Vocabulary (OOV) Issues\n",
    "\n",
    "\t•\tWhy: Rare words can lead to problems during training, as the model might overfit to these instances. Handling them can improve model robustness.\n",
    "\t•\tHow: Replace rare words with a <unk> token or use subword tokenization techniques (like BPE or SentencePiece) to decompose rare words into more common subwords.\n",
    "\n",
    "6. Stop Words Removal (Conditional)\n",
    "\n",
    "\t•\tWhy: For some NLP tasks like topic modeling or keyword extraction, removing common words (stop words) that carry minimal meaningful information can focus the model on more important terms.\n",
    "\t•\tHow: Use libraries like NLTK or spaCy to remove stop words, but this is generally not recommended for machine translation unless specifically required for a styled translation.\n",
    "\n",
    "7. Part-of-Speech Tagging and Syntactic Parsing (Advanced)\n",
    "\n",
    "\t•\tWhy: This can help in understanding sentence structure and can be used to improve alignment in parallel corpora.\n",
    "\t•\tHow: Use language models from libraries like spaCy to annotate words with their parts of speech or to parse sentence structure.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFuther Preprocessign steps:\\n\\n1. Tokenization\\n\\n\\t•\\tWhy: Tokenization involves splitting text into words, phrases, or other meaningful elements called tokens. Proper tokenization is crucial as it forms the basis of how input data is fed into the model.\\n\\t•\\tHow: Use libraries like NLTK, spaCy, or specialized tokenizer libraries like SentencePiece, which are especially useful for languages without clear word boundaries.\\n\\n2. Normalization\\n\\n\\t•\\tWhy: This step involves standardizing text by correcting typos, expanding contractions (e.g., converting “can’t” to “cannot”), and converting numerals into words. Normalization helps reduce the many variations of the same word, which can dilute the model’s ability to learn.\\n\\t•\\tHow: Apply custom rules or use language-specific libraries that handle linguistic variations.\\n\\n3. Removing/Replacing Non-Text Elements\\n\\n\\t•\\tWhy: Non-text elements like numbers, URLs, or user mentions (e.g., Twitter handles) may not be relevant and could distract the model from learning linguistic patterns.\\n\\t•\\tHow: Detect and replace these elements with generic placeholders like <number>, <url>, or <user>.\\n\\n4. Punctuation Cleaning\\n\\n\\t•\\tWhy: While removing all punctuation can be harmful, normalizing punctuation and ensuring consistent use can be very beneficial. For example, converting fancy quotation marks to standard ones (’ or “) ensures consistency.\\n\\t•\\tHow: Replace or remove specific punctuation characters that are not beneficial for the training process.\\n\\n5. Handling Rare Words and Out-of-Vocabulary (OOV) Issues\\n\\n\\t•\\tWhy: Rare words can lead to problems during training, as the model might overfit to these instances. Handling them can improve model robustness.\\n\\t•\\tHow: Replace rare words with a <unk> token or use subword tokenization techniques (like BPE or SentencePiece) to decompose rare words into more common subwords.\\n\\n6. Stop Words Removal (Conditional)\\n\\n\\t•\\tWhy: For some NLP tasks like topic modeling or keyword extraction, removing common words (stop words) that carry minimal meaningful information can focus the model on more important terms.\\n\\t•\\tHow: Use libraries like NLTK or spaCy to remove stop words, but this is generally not recommended for machine translation unless specifically required for a styled translation.\\n\\n7. Part-of-Speech Tagging and Syntactic Parsing (Advanced)\\n\\n\\t•\\tWhy: This can help in understanding sentence structure and can be used to improve alignment in parallel corpora.\\n\\t•\\tHow: Use language models from libraries like spaCy to annotate words with their parts of speech or to parse sentence structure.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "2beef055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:48:37.141334Z",
     "start_time": "2024-07-26T18:48:37.139745Z"
    }
   },
   "source": [
    "#######TRAINING#############"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "8113c4b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:48:37.584235Z",
     "start_time": "2024-07-26T18:48:37.572023Z"
    }
   },
   "source": [
    "#Split data into Train, Val, Test\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_en, data_fr, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "4112a9fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:48:38.095663Z",
     "start_time": "2024-07-26T18:48:38.092155Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Model Architecture:\n",
    "Encoder: An RNN (LSTM or GRU) that processes the input sentence and encodes it into a context vector.\n",
    "Decoder: Another RNN that takes the context vector and generates the output sentence in the target language.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = nn.functional.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "508d1e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:48:39.757771Z",
     "start_time": "2024-07-26T18:48:39.105527Z"
    }
   },
   "source": [
    "# init decoder and encoder and loss function \n",
    "\n",
    "input_size = vocab_size_en   # size of the English vocabulary was calculated in task 1\n",
    "output_size = vocab_size_fr  # size of the French vocabulary was calculated in task 1\n",
    "hidden_size = 256            # typically a power of 2 like 256 or 512\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, output_size)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.01)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "b8947913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:48:41.508827Z",
     "start_time": "2024-07-26T18:48:41.505369Z"
    }
   },
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    # Decoder initialization without an SOS token\n",
    "    # The first input to the decoder could be an assumed blank token, often just zeros\n",
    "    decoder_input = torch.zeros((1, 1), dtype=torch.long)  # Assuming your vocab is zero-indexed\n",
    "    decoder_hidden = encoder_hidden  # Use the last hidden state from the encoder to start the decoder\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # Use model's own prediction as next input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        # Check if decoder has generated the end of sequence, often by target length or a special condition\n",
    "        if di == target_length - 1:  # Simple condition assuming reaching the end of target tensor\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "99583761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:48:42.486322Z",
     "start_time": "2024-07-26T18:48:42.483449Z"
    }
   },
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f'Iteration: {iter} Average Loss: {print_loss_avg:.4f}')"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "7e12666f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:48:58.695464Z",
     "start_time": "2024-07-26T18:48:45.963812Z"
    }
   },
   "source": [
    "#Create Embeddings manually \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "data = {\n",
    "    'X_train': X_train.tolist(),\n",
    "    'y_train': y_train.tolist()\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(sentences):\n",
    "    return [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Building vocabulary\n",
    "def build_vocab(tokenized_texts):\n",
    "    vocab = defaultdict()\n",
    "    vocab.default_factory = lambda: len(vocab)\n",
    "    for tokens in tokenized_texts:\n",
    "        for token in tokens:\n",
    "            vocab[token]\n",
    "    return vocab\n",
    "\n",
    "# Convert texts to numerical indices\n",
    "def numericalize(texts, vocab):\n",
    "    return [[vocab[token] for token in tokens] for tokens in tokenize(texts)]\n",
    "\n",
    "# Tokenize and build vocabularies\n",
    "tokenized_x = tokenize(df['X_train'])\n",
    "tokenized_y = tokenize(df['y_train'])\n",
    "\n",
    "#{token(in this case a word):number} ex. 'watts': 4239\n",
    "vocab_x = build_vocab(tokenized_x)\n",
    "vocab_y = build_vocab(tokenized_y)\n",
    "\n",
    "# Numericalize the data\n",
    "numericalized_x = numericalize(df['X_train'], vocab_x)\n",
    "numericalized_y = numericalize(df['y_train'], vocab_y)\n",
    "\n",
    "# Convert lists to PyTorch tensors and pad sequences\n",
    "input_tensor = pad_sequence([torch.tensor(seq) for seq in numericalized_x], batch_first=True, padding_value=0)\n",
    "target_tensor = pad_sequence([torch.tensor(seq) for seq in numericalized_y], batch_first=True, padding_value=0)\n",
    "\n",
    "print(input_tensor)\n",
    "print(target_tensor)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    2,  ...,    0,    0,    0],\n",
      "        [  18,   19,   20,  ...,    0,    0,    0],\n",
      "        [  28,   14,   29,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  29,    6, 1091,  ...,    0,    0,    0],\n",
      "        [ 664,   14,    6,  ...,    0,    0,    0],\n",
      "        [ 460,   18,  183,  ...,    0,    0,    0]])\n",
      "tensor([[   0,    1,    2,  ...,    0,    0,    0],\n",
      "        [  29,   30,    2,  ...,    0,    0,    0],\n",
      "        [  24,   46,   47,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 324, 4860,  641,  ...,    0,    0,    0],\n",
      "        [ 120,  641, 1291,  ...,    0,    0,    0],\n",
      "        [ 264,    2,   29,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "486ed0c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:52:57.216840Z",
     "start_time": "2024-07-26T18:49:02.403032Z"
    }
   },
   "source": [
    "#TODO takes to much time..\n",
    "\n",
    "import random\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "num_iterations = 10000\n",
    "print_every = 1000\n",
    "\n",
    "for iter in range(1, num_iterations + 1):\n",
    "    # Randomly selecting an example each time (for simplicity in this example)\n",
    "    # Ideally, use a more systematic way to create batches and cycle through data\n",
    "    input_example = random.choice(input_tensor)\n",
    "    target_example = random.choice(target_tensor)\n",
    "    loss = train(input_example, target_example, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, 1000)\n",
    "    if iter % print_every == 0:\n",
    "        print(f'Iter: {iter}, Loss: {loss:.4f}')"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m input_example \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoice(input_tensor)\n\u001B[1;32m     13\u001B[0m target_example \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoice(target_tensor)\n\u001B[0;32m---> 14\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_example\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_example\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m%\u001B[39m print_every \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIter: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28miter\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[37], line 31\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m di \u001B[38;5;241m==\u001B[39m target_length \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:  \u001B[38;5;66;03m# Simple condition assuming reaching the end of target tensor\u001B[39;00m\n\u001B[1;32m     29\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m encoder_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     34\u001B[0m decoder_optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/nlp_project/nlp_venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "fa8ef9fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:52:59.139053Z",
     "start_time": "2024-07-26T18:52:59.137333Z"
    }
   },
   "source": [
    "####### TODO TRAIN MODEL WITH SOS AND EOS TOKEN #########"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "a0f76edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:53:00.194115Z",
     "start_time": "2024-07-26T18:53:00.191461Z"
    }
   },
   "source": [
    "#TODO add SOS and EOS tokens \n",
    "\n",
    "SOS_token = 0  # Assuming 0 is the index for SOS token in your vocabulary\n",
    "EOS_token = 1  # Assuming 1 is the index for EOS token in your vocabulary\n",
    "\n",
    "def add_special_tokens(sentences, sos_token, eos_token=None):\n",
    "    # Add an SOS token at the beginning of each sentence.\n",
    "    # Optionally, add an EOS token at the end.\n",
    "    updated_sentences = []\n",
    "    for sentence in sentences:\n",
    "        modified_sentence = [sos_token] + sentence\n",
    "        if eos_token is not None:\n",
    "            modified_sentence += [eos_token]\n",
    "        updated_sentences.append(modified_sentence)\n",
    "    return updated_sentences\n"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "300148ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:53:00.872092Z",
     "start_time": "2024-07-26T18:53:00.870736Z"
    }
   },
   "source": [
    "########################## USE DIFFERENT EMBEDDING MODELS #################\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "1f6bebfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:53:01.191960Z",
     "start_time": "2024-07-26T18:53:01.188741Z"
    }
   },
   "source": [
    "\"\"\"Here’s a breakdown of how these steps align with the use of pre-trained embeddings:\n",
    "\n",
    "1. Tokenization\n",
    "\n",
    "Tokenization remains a necessary first step because it transforms raw text into lists of words or subwords. This process is independent of whether you use pre-trained embeddings.\n",
    "\n",
    "2. Build Vocabulary\n",
    "\n",
    "When using pre-trained embeddings, you build a vocabulary that aligns closely with the vocabulary used by the embeddings. Often, this means adapting your dataset’s vocabulary to match the pre-trained set:\n",
    "\n",
    "\t•\tInclude only words found in the pre-trained embeddings: Words not present in the embeddings can be handled by a special token like <unk> (unknown).\n",
    "\t•\tReuse pre-trained special tokens if available: Some embeddings include tokens for unknown words, start of sentences, or padding, which you should utilize if consistent with your model architecture.\n",
    "\n",
    "3. Numericalization\n",
    "\n",
    "This step converts the tokenized sentences into sequences of indices based on the vocabulary. For models using pre-trained embeddings, these indices correspond to the rows in the embedding matrix where each word’s vector is stored.\n",
    "\n",
    "4. Padding\n",
    "\n",
    "To handle variable-length sequences in batch processing efficiently, padding is necessary. This step standardizes the length of all sequences in a batch by appending a special padding token (often mapped to a zero vector in the embedding matrix).\n",
    "\n",
    "5. Convert to Tensors\n",
    "\n",
    "Finally, the lists of indices are converted to PyTorch tensors, which can be efficiently processed by your model. These tensors are what you will feed into your neural network.\n",
    "\n",
    "Incorporating Pre-trained Embeddings\n",
    "\n",
    "After preparing your tensors:\n",
    "\n",
    "\t•\tLoad Embedding Matrix: Load your pre-trained embedding matrix, ensuring it corresponds to your vocabulary.\n",
    "\t•\tSet Model’s Embedding Layer: Initialize the embedding layer of your model with the pre-trained embeddings. You can choose to freeze these weights (to prevent further training) or allow them to fine-tune during training based on your specific needs.\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s a breakdown of how these steps align with the use of pre-trained embeddings:\\n\\n1. Tokenization\\n\\nTokenization remains a necessary first step because it transforms raw text into lists of words or subwords. This process is independent of whether you use pre-trained embeddings.\\n\\n2. Build Vocabulary\\n\\nWhen using pre-trained embeddings, you build a vocabulary that aligns closely with the vocabulary used by the embeddings. Often, this means adapting your dataset’s vocabulary to match the pre-trained set:\\n\\n\\t•\\tInclude only words found in the pre-trained embeddings: Words not present in the embeddings can be handled by a special token like <unk> (unknown).\\n\\t•\\tReuse pre-trained special tokens if available: Some embeddings include tokens for unknown words, start of sentences, or padding, which you should utilize if consistent with your model architecture.\\n\\n3. Numericalization\\n\\nThis step converts the tokenized sentences into sequences of indices based on the vocabulary. For models using pre-trained embeddings, these indices correspond to the rows in the embedding matrix where each word’s vector is stored.\\n\\n4. Padding\\n\\nTo handle variable-length sequences in batch processing efficiently, padding is necessary. This step standardizes the length of all sequences in a batch by appending a special padding token (often mapped to a zero vector in the embedding matrix).\\n\\n5. Convert to Tensors\\n\\nFinally, the lists of indices are converted to PyTorch tensors, which can be efficiently processed by your model. These tensors are what you will feed into your neural network.\\n\\nIncorporating Pre-trained Embeddings\\n\\nAfter preparing your tensors:\\n\\n\\t•\\tLoad Embedding Matrix: Load your pre-trained embedding matrix, ensuring it corresponds to your vocabulary.\\n\\t•\\tSet Model’s Embedding Layer: Initialize the embedding layer of your model with the pre-trained embeddings. You can choose to freeze these weights (to prevent further training) or allow them to fine-tune during training based on your specific needs.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:55:14.348400Z",
     "start_time": "2024-07-26T18:55:11.635136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example GloVe embedding file path and embedding dimension\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "def load_glove_embeddings(path: str, word2idx: Dict[str, int], embedding_dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a specified file and align them with the given word index dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The file path to the GloVe embeddings file.\n",
    "    - word2idx (Dict[str, int]): A dictionary mapping words to their corresponding indices. This dictionary defines\n",
    "      the position each word’s vector should occupy in the resulting embedding matrix.\n",
    "    - embedding_dim (int): The dimensionality of the GloVe vectors (e.g., 50, 100, 200, 300).\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor of shape (len(word2idx), embedding_dim) containing the GloVe vectors aligned according to word2idx.\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # Initialize the embedding matrix with zeros\n",
    "        embeddings = np.zeros((len(word2idx), embedding_dim))\n",
    "        \n",
    "        # Process each line in the GloVe file\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            \n",
    "            # If the word is in the provided dictionary, update the corresponding row in embeddings\n",
    "            if word in word2idx:\n",
    "                # Convert embedding values from strings to float32\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                # Place the vector in the correct index as per word2idx\n",
    "                embeddings[word2idx[word]] = vector\n",
    "    \n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    return torch.from_numpy(embeddings)\n",
    "\n",
    "\n",
    "def load_embeddings_and_create_index(path):\n",
    "    word_to_idx = {}\n",
    "    idx = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1\n",
    "    return word_to_idx\n",
    "\n",
    "# Load the word to index mappings for both languages\n",
    "word_to_idx_en = load_embeddings_and_create_index('glove.6B/glove.6B.100d.txt')\n",
    "word_to_idx_fr = load_embeddings_and_create_index('glove.6B/glove.6B.100d.txt')\n"
   ],
   "id": "eec1da68cb84c0b6",
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "c2bae849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:55:16.109538Z",
     "start_time": "2024-07-26T18:55:15.660034Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Load data\n",
    "data_en = pd.DataFrame({'sentence': X_train.tolist()}) \n",
    "data_fr = pd.DataFrame({'sentence': y_train.tolist()})  \n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Vocabulary mapping\n",
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.token_to_idx:\n",
    "            idx = len(self.token_to_idx)\n",
    "            self.token_to_idx[word] = idx\n",
    "            self.idx_to_token[idx] = word\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        return self.token_to_idx.get(word, self.token_to_idx['<unk>'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "# Build vocabularies for both languages\n",
    "def build_vocab(sentences, existing_embeddings_word2idx):\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')  # Assuming you handle padding explicitly\n",
    "    vocab.add_word('<unk>')  # Handle unknown words\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token in existing_embeddings_word2idx:\n",
    "                vocab.add_word(token)\n",
    "    return vocab\n",
    "\n",
    "vocab_en = build_vocab(data_en['sentence'], word_to_idx_en)\n",
    "vocab_fr = build_vocab(data_fr['sentence'], word_to_idx_fr)\n",
    "\n",
    "# Dataset preparation\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences, source_vocab, target_vocab):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source_sentence = [self.source_vocab(token) if token in self.source_vocab.token_to_idx else self.source_vocab('<unk>') for token in tokenize(self.source_sentences.iloc[index])]\n",
    "        target_sentence = [self.target_vocab(token) if token in self.target_vocab.token_to_idx else self.target_vocab('<unk>') for token in tokenize(self.target_sentences.iloc[index])]\n",
    "        return torch.tensor(source_sentence, dtype=torch.long), torch.tensor(target_sentence, dtype=torch.long)\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    source_batch, target_batch = zip(*batch)\n",
    "    source_batch_padded = pad_sequence(source_batch, padding_value=vocab_en('<pad>'), batch_first=True)\n",
    "    target_batch_padded = pad_sequence(target_batch, padding_value=vocab_fr('<pad>'), batch_first=True)\n",
    "    return source_batch_padded, target_batch_padded\n"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "f5ce67a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:56:19.774132Z",
     "start_time": "2024-07-26T18:56:19.768798Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the Encoder with pre-trained embeddings and a GRU layer.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "            pretrained_embeddings (torch.Tensor): A tensor containing the pre-trained word embeddings.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # Ensure that the pretrained embeddings are of type float32\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        embed_size = pretrained_embeddings.shape[1]  # Embedding size is the second dimension of the embeddings tensor\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True).float()  # Ensure GRU is initialized as float32\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder which processes the input sequence.\n",
    "    \n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input sequence tensor, which should be indexed by batch.\n",
    "    \n",
    "        Returns:\n",
    "            hidden (torch.Tensor): The hidden state of the GRU, representing the encoded information of the input.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x).float()  # Ensure embedding outputs float32\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, pretrained_embeddings):\n",
    "        \"\"\"\n",
    "        Initialize the Decoder with pre-trained embeddings, a GRU layer, and a linear output layer.\n",
    "\n",
    "        Parameters:\n",
    "            embed_size (int): The size of each embedding vector.\n",
    "            hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            pretrained_embeddings (torch.Tensor): A tensor containing the pre-trained word embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        # Ensure that the pretrained embeddings are of type float32\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True).float()  # Ensure GRU is initialized as float32\n",
    "        self.fc = nn.Linear(hidden_size, output_size).float()  # Ensure Linear is initialized as float32\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder that processes one timestep of the sequence.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input tensor for the current timestep.\n",
    "            hidden (torch.Tensor): The hidden state from the last timestep.\n",
    "\n",
    "        Returns:\n",
    "            predicted (torch.Tensor): The output logits for the next word in the sequence.\n",
    "            hidden (torch.Tensor): The updated hidden state.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x).float()  # Ensure embedding outputs float32\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        predicted = self.fc(output)\n",
    "        return predicted, hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "        \n",
    "    def __init__(self, encoder, decoder):\n",
    "        \"\"\"\n",
    "        Initialize the sequence-to-sequence model which contains an encoder and a decoder.\n",
    "    \n",
    "        Parameters:\n",
    "            encoder (Encoder): The encoder part of the Seq2Seq model.\n",
    "            decoder (Decoder): The decoder part of the Seq2Seq model.\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Forward pass of the Seq2Seq model which processes the entire input and target sequence.\n",
    "\n",
    "        Parameters:\n",
    "            src (torch.Tensor): The input sequence tensor.\n",
    "            trg (torch.Tensor): The target sequence tensor used during training.\n",
    "\n",
    "        Returns:\n",
    "            outputs (torch.Tensor): The output from the decoder for each step in the sequence.\n",
    "        \"\"\"\n",
    "        hidden = self.encoder(src)\n",
    "        outputs, _ = self.decoder(trg, hidden)\n",
    "        return outputs\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "6610a422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T19:02:58.993335Z",
     "start_time": "2024-07-26T19:02:58.990251Z"
    }
   },
   "source": [
    "\n",
    "def train(model, loader, optimizer, criterion, epochs=10, device=\"cpu\"):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for src, trg in loader:\n",
    "            # Move tensors to the correct device and ensure they are long type for indexing operations\n",
    "            src = src.to(device).long()  # Correct type for embedding layer\n",
    "            trg = trg.to(device).long()  # Correct type for embedding layer\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: The decoder's input is all except the last word\n",
    "            output = model(src, trg[:, :-1])  \n",
    "            \n",
    "            # Since output will be in float (from linear layers, and GRU output), ensure it's float32 if not already\n",
    "            output = output.float()\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)  # Target doesn't include the first <sos> token\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}')\n",
    "        \n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "3e51a741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T19:02:59.635475Z",
     "start_time": "2024-07-26T19:02:59.620100Z"
    }
   },
   "source": [
    "# Checking if GPU is available and setting the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Init and run\n",
    "\n",
    "# Load embeddings\n",
    "glove_embeddings_en = load_glove_embeddings('glove.6B/glove.6B.100d.txt', vocab_x, embedding_dim)\n",
    "glove_embeddings_fr = load_glove_embeddings('glove.6B/glove.6B.100d.txt', vocab_x, embedding_dim)\n",
    "\n",
    "# Model instantiation\n",
    "encoder = Encoder(hidden_size=hidden_size, pretrained_embeddings=glove_embeddings_en)\n",
    "decoder = Decoder(embed_size=embedding_dim, hidden_size=hidden_size, output_size=len(vocab_fr), pretrained_embeddings=glove_embeddings_fr)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "# Move the model to the configured device\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss(ignore_index=vocab_fr.token_to_idx['<pad>']).to(device)  # Move the loss function to the device\n",
    "\n",
    "\n",
    "dataset = TranslationDataset(data_en['sentence'], data_fr['sentence'], vocab_en, vocab_fr)\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "train(model, loader, optimizer, criterion, epochs=10, device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedding_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[60], line 8\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing device:\u001B[39m\u001B[38;5;124m\"\u001B[39m, device)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Init and run\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Load embeddings\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m glove_embeddings_en \u001B[38;5;241m=\u001B[39m load_glove_embeddings(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mglove.6B/glove.6B.100d.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, vocab_x, \u001B[43membedding_dim\u001B[49m)\n\u001B[1;32m      9\u001B[0m glove_embeddings_fr \u001B[38;5;241m=\u001B[39m load_glove_embeddings(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mglove.6B/glove.6B.100d.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, vocab_x, embedding_dim)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Model instantiation\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'embedding_dim' is not defined"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96aa90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
