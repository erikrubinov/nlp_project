{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:24:18.680175Z",
     "start_time": "2024-07-27T14:24:06.436491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from preprocessing_task_2 import prepare_data\n",
    "import spacy\n",
    "from collections import Counter"
   ],
   "id": "3b5d9883f748e844",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stoffregen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/stoffregen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m26.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\r\n",
      "Requirement already satisfied: jinja2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\r\n",
      "Requirement already satisfied: setuptools in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (70.1.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\r\n",
      "Requirement already satisfied: wrapt in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n",
      "Collecting fr-core-news-sm==3.7.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.3/16.3 MB\u001B[0m \u001B[31m22.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from fr-core-news-sm==3.7.0) (3.7.5)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\r\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.4)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.8.2)\r\n",
      "Requirement already satisfied: jinja2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\r\n",
      "Requirement already satisfied: setuptools in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (70.1.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.6.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.7.1)\r\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.18.1)\r\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\r\n",
      "Requirement already satisfied: wrapt in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.16.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/nlp_venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!python3 -m spacy download fr_core_news_sm"
   ],
   "id": "7c99c4a6b38254c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:24:48.911792Z",
     "start_time": "2024-07-27T14:24:36.316006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "####### LOADING AND PREPROCESSING #############\n",
    "english_data, french_data = prepare_data()"
   ],
   "id": "222dc7acd4ceed5b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/ass2/preprocessing_task_2.py:25: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  data_en = data_en[mask]\n",
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/ass2/preprocessing_task_2.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  data_fr = data_fr[mask]\n",
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/ass2/preprocessing_task_2.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_en['text'] = data_en['text'].apply(lambda x: re.sub(regex_pattern, '', x))\n",
      "/home/stoffregen/Documents/tub/summer24/natural_language_processing/nlp_project/ass2/preprocessing_task_2.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_fr['text'] = data_fr['text'].apply(lambda x: re.sub(regex_pattern, '', x))\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T14:32:28.709869Z",
     "start_time": "2024-07-27T14:32:27.410391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Functions to tokenize data and build vocab\n",
    "spacy_fr = spacy.load('fr_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def build_vocab(sentences, vocab_size, word_tokenize):\n",
    "    all_words = [word for sentence in sentences for word in word_tokenize(sentence)]\n",
    "    word_counts = Counter(all_words)\n",
    "    vocab = [word for word, _ in word_counts.most_common(vocab_size)]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab, start=4)}\n",
    "    word2idx['<pad>'] = 0\n",
    "    word2idx['<unk>'] = 1\n",
    "    word2idx['<sos>'] = 2\n",
    "    word2idx['<eos>'] = 3\n",
    "    return word2idx"
   ],
   "id": "61a7669965faa00",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "f30c0ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:03.861156Z",
     "start_time": "2024-07-27T08:35:03.858345Z"
    }
   },
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    # Decoder initialization without an SOS token\n",
    "    # The first input to the decoder could be an assumed blank token, often just zeros\n",
    "    decoder_input = torch.zeros((1, 1), dtype=torch.long)  # Assuming your vocab is zero-indexed\n",
    "    decoder_hidden = encoder_hidden  # Use the last hidden state from the encoder to start the decoder\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # Use model's own prediction as next input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        # Check if decoder has generated the end of sequence, often by target length or a special condition\n",
    "        if di == target_length - 1:  # Simple condition assuming reaching the end of target tensor\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "4fe5e804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:21.960602Z",
     "start_time": "2024-07-27T08:35:21.958254Z"
    }
   },
   "source": [
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    source_batch, target_batch = zip(*batch)\n",
    "    source_batch_padded = pad_sequence(source_batch, padding_value=vocab_en('<pad>'), batch_first=True)\n",
    "    target_batch_padded = pad_sequence(target_batch, padding_value=vocab_fr('<pad>'), batch_first=True)\n",
    "    return source_batch_padded, target_batch_padded"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2a14a700f37aee5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:05:02.349458Z",
     "start_time": "2024-07-27T15:05:02.347598Z"
    }
   },
   "cell_type": "code",
   "source": "########################## TODO: USE DIFFERENT EMBEDDING MODELS #################\n",
   "id": "a8faf5e32ad685b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "1bf21853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:05:02.818721Z",
     "start_time": "2024-07-27T15:05:02.815347Z"
    }
   },
   "source": [
    "\n",
    "def load_embeddings_and_create_index(path):\n",
    "    word_to_idx = {}\n",
    "    idx = 0\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1\n",
    "    return word_to_idx\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "929a107a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T08:35:23.842044Z",
     "start_time": "2024-07-27T08:35:23.835782Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Dataset preparation\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences, source_vocab, target_vocab):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        source_sentence = [self.source_vocab(token) if token in self.source_vocab.token_to_idx else self.source_vocab('<unk>') for token in tokenize(self.source_sentences.iloc[index])]\n",
    "        target_sentence = [self.target_vocab(token) if token in self.target_vocab.token_to_idx else self.target_vocab('<unk>') for token in tokenize(self.target_sentences.iloc[index])]\n",
    "        return torch.tensor(source_sentence, dtype=torch.long), torch.tensor(target_sentence, dtype=torch.long)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "cafce509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T15:04:38.294118Z",
     "start_time": "2024-07-27T15:04:38.289734Z"
    }
   },
   "source": [
    "# Example GloVe embedding file path and embedding dimension\n",
    "\n",
    "def load_glove_embeddings(path: str, word2idx: Dict[str, int], embedding_dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a specified file and align them with the given word index dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The file path to the GloVe embeddings file.\n",
    "    - word2idx (Dict[str, int]): A dictionary mapping words to their corresponding indices. This dictionary defines\n",
    "      the position each word’s vector should occupy in the resulting embedding matrix.\n",
    "    - embedding_dim (int): The dimensionality of the GloVe vectors (e.g., 50, 100, 200, 300).\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor of shape (len(word2idx), embedding_dim) containing the GloVe vectors aligned according to word2idx.\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # Initialize the embedding matrix with zeros\n",
    "        embeddings = np.zeros((len(word2idx), embedding_dim))\n",
    "        \n",
    "        # Process each line in the GloVe file\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            \n",
    "            # If the word is in the provided dictionary, update the corresponding row in embeddings\n",
    "            if word in word2idx.keys():\n",
    "                # Convert embedding values from strings to float32\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                # Place the vector in the correct index as per word2idx\n",
    "                embeddings[word2idx[word]] = vector\n",
    "    \n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    return torch.from_numpy(embeddings)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "c09042f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:48:16.414404Z",
     "start_time": "2024-07-27T16:48:16.405884Z"
    }
   },
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, pretrained_embeddings):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the Encoder with pre-trained embeddings and a GRU layer.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "            pretrained_embeddings (torch.Tensor): A tensor containing the pre-trained word embeddings.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # Ensure that the pretrained embeddings are of type float32\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        embed_size = pretrained_embeddings.shape[1]  # Embedding size is the second dimension of the embeddings tensor\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True).float()  # Ensure GRU is initialized as float32\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder which processes the input sequence.\n",
    "\n",
    "        Parameters:\n",
    "            input (torch.Tensor): The input sequence tensor, which should be indexed by batch.\n",
    "\n",
    "        Returns:\n",
    "            hidden (torch.Tensor): The hidden state of the GRU, representing the encoded information of the input.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input).float()  # Ensure embedding outputs float32\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, pretrained_embeddings):\n",
    "        \"\"\"\n",
    "        Initialize the Decoder with pre-trained embeddings, a GRU layer, and a linear output layer.\n",
    "\n",
    "        Parameters:\n",
    "            embed_size (int): The size of each embedding vector.\n",
    "            hidden_size (int): The number of features in the hidden state of the GRU.\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            pretrained_embeddings (torch.Tensor): A tensor containing the pre-trained word embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        # Ensure that the pretrained embeddings are of type float32\n",
    "        if pretrained_embeddings.dtype != torch.float32:\n",
    "            pretrained_embeddings = pretrained_embeddings.to(dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True).float()  # Ensure GRU is initialized as float32\n",
    "        self.fc = nn.Linear(hidden_size, output_size).float()  # Ensure Linear is initialized as float32\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder that processes one timestep of the sequence.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input tensor for the current timestep.\n",
    "            hidden (torch.Tensor): The hidden state from the last timestep.\n",
    "\n",
    "        Returns:\n",
    "            predicted (torch.Tensor): The output logits for the next word in the sequence.\n",
    "            hidden (torch.Tensor): The updated hidden state.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x).float()  # Ensure embedding outputs float32\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        predicted = self.fc(output)\n",
    "        return predicted, hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "\n",
    "        \"\"\"\n",
    "        Forward pass of the Seq2Seq model which processes the entire input and target sequence.\n",
    "\n",
    "        Parameters:\n",
    "            source (torch.Tensor): The input sequence tensor.\n",
    "            target (torch.Tensor): The target sequence tensor used during training.\n",
    "\n",
    "        Returns:\n",
    "            outputs (torch.Tensor): The output from the decoder for each step in the sequence.\n",
    "        \"\"\"\n",
    "        hidden = self.encoder(source)\n",
    "        outputs, _ = self.decoder(target, hidden)\n",
    "        return outputs\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "32199614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:43:55.458851Z",
     "start_time": "2024-07-27T16:43:55.455281Z"
    }
   },
   "source": [
    "\n",
    "def train(model, loader, optimizer, criterion, epochs=10, device=\"cpu\"):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for src, trg in loader:\n",
    "            # Move tensors to the correct device and ensure they are long type for indexing operations\n",
    "            src = src.to(device).long()  # Correct type for embedding layer\n",
    "            trg = trg.to(device).long()  # Correct type for embedding layer\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: The decoder's input is all except the last word\n",
    "            output = model(src, trg[:, :-1])  \n",
    "            \n",
    "            # Since output will be in float (from linear layers, and GRU output), ensure it's float32 if not already\n",
    "            output = output.float()\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)  # Target doesn't include the first <sos> token\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}')\n",
    "        \n",
    "        \n"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "40c6f773",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-27T16:48:18.456457Z"
    }
   },
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "vocab_size = 10000\n",
    "# TODO: Do we build vocab on the entire dataset or just the training set?\n",
    "#word2idx_pre_embeddings = load_embeddings_and_create_index('glove.6B/glove.6B.100d.txt')\n",
    "vocab_en = build_vocab(english_data[\"text\"], vocab_size, tokenize_en)\n",
    "vocab_fr = build_vocab(french_data[\"text\"], vocab_size, tokenize_fr)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(english_data, french_data, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Load embeddings\n",
    "vocab_embeddings_en = load_glove_embeddings('glove.6B/glove.6B.100d.txt', vocab_en, embedding_dim)\n",
    "vocab_embeddings_fr = load_glove_embeddings('glove.6B/glove.6B.100d.txt', vocab_fr, embedding_dim)\n",
    "\n",
    "# Model instantiation\n",
    "hidden_size = len(vocab_en)\n",
    "encoder = Encoder(hidden_size=hidden_size, pretrained_embeddings=vocab_embeddings_en)\n",
    "decoder = Decoder(embed_size=embedding_dim, hidden_size=hidden_size, output_size=len(vocab_fr), pretrained_embeddings=vocab_embeddings_fr)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss(ignore_index=vocab_fr.token_to_idx['<pad>']).to(device)  # Move the loss function to the device\n",
    "\n",
    "dataset = TranslationDataset(english_data['text'], french_data['text'], vocab_en, vocab_fr)\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "train(model, loader, optimizer, criterion, epochs=10, device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b13af837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocabulary at 0x350c2a910>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936c567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
